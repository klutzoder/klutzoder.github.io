<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[RocketMQ 03 - 源码分析]]></title>
    <url>%2FRocketMQ%2Fmiddleware%2Frocketmq-03%2F</url>
    <content type="text"><![CDATA[说明：本文档由《黑马程序员》整理，本人只是为了方便才整理到自己博客！！相应视频资料：RocketMQ系统精讲 高级功能消息存储分布式队列因为有高可靠性的要求，所以数据要进行持久化存储。 消息生成者发送消息 MQ收到消息，将消息进行持久化，在存储中新增一条记录 返回ACK给生产者 MQ push 消息给对应的消费者，然后等待消费者返回ACK 如果消息消费者在指定时间内成功返回ack，那么MQ认为消息消费成功，在存储中删除消息，即执行第6步；如果MQ在指定时间内没有收到ACK，则认为消息消费失败，会尝试重新push消息,重复执行4、5、6步骤 MQ删除消息 存储介质 关系型数据库DB Apache下开源的另外一款MQ—ActiveMQ（默认采用的KahaDB做消息存储）可选用JDBC的方式来做消息持久化，通过简单的xml配置信息即可实现JDBC消息存储。由于，普通关系型数据库（如Mysql）在单表数据量达到千万级别的情况下，其IO读写性能往往会出现瓶颈。在可靠性方面，该种方案非常依赖DB，如果一旦DB出现故障，则MQ的消息就无法落盘存储会导致线上故障 文件系统 目前业界较为常用的几款产品（RocketMQ/Kafka/RabbitMQ）均采用的是消息刷盘至所部署虚拟机/物理机的文件系统来做持久化（刷盘一般可以分为异步刷盘和同步刷盘两种模式）。消息刷盘为消息存储提供了一种高效率、高可靠性和高性能的数据持久化方式。除非部署MQ机器本身或是本地磁盘挂了，否则一般是不会出现无法持久化的故障问题。 性能对比文件系统&gt;关系型数据库DB 消息的存储和发送消息存储磁盘如果使用得当，磁盘的速度完全可以匹配上网络 的数据传输速度。目前的高性能磁盘，顺序写速度可以达到600MB/s， 超过了一般网卡的传输速度。但是磁盘随机写的速度只有大概100KB/s，和顺序写的性能相差6000倍！因为有如此巨大的速度差别，好的消息队列系统会比普通的消息队列系统速度快多个数量级。RocketMQ的消息用顺序写,保证了消息存储的速度。 消息发送Linux操作系统分为【用户态】和【内核态】，文件操作、网络操作需要涉及这两种形态的切换，免不了进行数据复制。 一台服务器 把本机磁盘文件的内容发送到客户端，一般分为两个步骤： 1）read；读取本地文件内容； 2）write；将读取的内容通过网络发送出去。 这两个看似简单的操作，实际进行了4 次数据复制，分别是： 从磁盘复制数据到内核态内存； 从内核态内存复 制到用户态内存； 然后从用户态 内存复制到网络驱动的内核态内存； 最后是从网络驱动的内核态内存复 制到网卡中进行传输。 通过使用mmap的方式，可以省去向用户态的内存复制，提高速度。这种机制在Java中是通过MappedByteBuffer实现的 RocketMQ充分利用了上述特性，也就是所谓的“零拷贝”技术，提高消息存盘和网络发送的速度。 这里需要注意的是，采用MappedByteBuffer这种内存映射的方式有几个限制，其中之一是一次只能映射1.5~2G 的文件至用户态的虚拟内存，这也是为何RocketMQ默认设置单个CommitLog日志数据文件为1G的原因了 消息存储结构RocketMQ消息的存储是由ConsumeQueue和CommitLog配合完成 的，消息真正的物理存储文件是CommitLog，ConsumeQueue是消息的逻辑队列，类似数据库的索引文件，存储的是指向物理存储的地址。每 个Topic下的每个Message Queue都有一个对应的ConsumeQueue文件。 CommitLog：存储消息的元数据 ConsumerQueue：存储消息在CommitLog的索引 IndexFile：为了消息查询提供了一种通过key或时间区间来查询消息的方法，这种通过IndexFile来查找消息的方法不影响发送与消费消息的主流程 刷盘机制RocketMQ的消息是存储到磁盘上的，这样既能保证断电后恢复， 又可以让存储的消息量超出内存的限制。RocketMQ为了提高性能，会尽可能地保证磁盘的顺序写。消息在通过Producer写入RocketMQ的时 候，有两种写磁盘方式，分布式同步刷盘和异步刷盘。 同步刷盘在返回写成功状态时，消息已经被写入磁盘。具体流程是，消息写入内存的PAGECACHE后，立刻通知刷盘线程刷盘， 然后等待刷盘完成，刷盘线程执行完成后唤醒等待的线程，返回消息写 成功的状态。 异步刷盘在返回写成功状态时，消息可能只是被写入了内存的PAGECACHE，写操作的返回快，吞吐量大；当内存里的消息量积累到一定程度时，统一触发写磁盘动作，快速写入。 配置同步刷盘还是异步刷盘，都是通过Broker配置文件里的flushDiskType 参数设置的，这个参数被配置成SYNC_FLUSH、ASYNC_FLUSH中的 一个。 高可用性机制 RocketMQ分布式集群是通过Master和Slave的配合达到高可用性的。 Master和Slave的区别：在Broker的配置文件中，参数 brokerId的值为0表明这个Broker是Master，大于0表明这个Broker是 Slave，同时brokerRole参数也会说明这个Broker是Master还是Slave。 Master角色的Broker支持读和写，Slave角色的Broker仅支持读，也就是 Producer只能和Master角色的Broker连接写入消息；Consumer可以连接 Master角色的Broker，也可以连接Slave角色的Broker来读取消息。 消息消费高可用在Consumer的配置文件中，并不需要设置是从Master读还是从Slave 读，当Master不可用或者繁忙的时候，Consumer会被自动切换到从Slave 读。有了自动切换Consumer这种机制，当一个Master角色的机器出现故障后，Consumer仍然可以从Slave读取消息，不影响Consumer程序。这就达到了消费端的高可用性。 消息发送高可用在创建Topic的时候，把Topic的多个Message Queue创建在多个Broker组上（相同Broker名称，不同 brokerId的机器组成一个Broker组），这样当一个Broker组的Master不可 用后，其他组的Master仍然可用，Producer仍然可以发送消息。 RocketMQ目前还不支持把Slave自动转成Master，如果机器资源不足， 需要把Slave转成Master，则要手动停止Slave角色的Broker，更改配置文 件，用新的配置文件启动Broker。 消息主从复制如果一个Broker组有Master和Slave，消息需要从Master复制到Slave 上，有同步和异步两种复制方式。 同步复制同步复制方式是等Master和Slave均写 成功后才反馈给客户端写成功状态； 在同步复制方式下，如果Master出故障， Slave上有全部的备份数据，容易恢复，但是同步复制会增大数据写入 延迟，降低系统吞吐量。 异步复制异步复制方式是只要Master写成功 即可反馈给客户端写成功状态。 在异步复制方式下，系统拥有较低的延迟和较高的吞吐量，但是如果Master出了故障，有些数据因为没有被写 入Slave，有可能会丢失； 配置同步复制和异步复制是通过Broker配置文件里的brokerRole参数进行设置的，这个参数可以被设置成ASYNC_MASTER、 SYNC_MASTER、SLAVE三个值中的一个。 总结 实际应用中要结合业务场景，合理设置刷盘方式和主从复制方式， 尤其是SYNC_FLUSH方式，由于频繁地触发磁盘写动作，会明显降低 性能。通常情况下，应该把Master和Save配置成ASYNC_FLUSH的刷盘 方式，主从之间配置成SYNC_MASTER的复制方式，这样即使有一台 机器出故障，仍然能保证数据不丢，是个不错的选择。 负载均衡Producer负载均衡Producer端，每个实例在发消息的时候，默认会轮询所有的message queue发送，以达到让消息平均落在不同的queue上。而由于queue可以散落在不同的broker，所以消息就发送到不同的broker下，如下图： 图中箭头线条上的标号代表顺序，发布方会把第一条消息发送至 Queue 0，然后第二条消息发送至 Queue 1，以此类推。 Consumer负载均衡集群模式在集群消费模式下，每条消息只需要投递到订阅这个topic的Consumer Group下的一个实例即可。RocketMQ采用主动拉取的方式拉取并消费消息，在拉取的时候需要明确指定拉取哪一条message queue。 而每当实例的数量有变更，都会触发一次所有实例的负载均衡，这时候会按照queue的数量和实例的数量平均分配queue给每个实例。 默认的分配算法是AllocateMessageQueueAveragely，如下图： 还有另外一种平均的算法是AllocateMessageQueueAveragelyByCircle，也是平均分摊每一条queue，只是以环状轮流分queue的形式，如下图： 需要注意的是，集群模式下，queue都是只允许分配只一个实例，这是由于如果多个实例同时消费一个queue的消息，由于拉取哪些消息是consumer主动控制的，那样会导致同一个消息在不同的实例下被消费多次，所以算法上都是一个queue只分给一个consumer实例，一个consumer实例可以允许同时分到不同的queue。 通过增加consumer实例去分摊queue的消费，可以起到水平扩展的消费能力的作用。而有实例下线的时候，会重新触发负载均衡，这时候原来分配到的queue将分配到其他实例上继续消费。 但是如果consumer实例的数量比message queue的总数量还多的话，多出来的consumer实例将无法分到queue，也就无法消费到消息，也就无法起到分摊负载的作用了。所以需要控制让queue的总数量大于等于consumer的数量。 广播模式由于广播模式下要求一条消息需要投递到一个消费组下面所有的消费者实例，所以也就没有消息被分摊消费的说法。 在实现上，其中一个不同就是在consumer分配queue的时候，所有consumer都分到所有的queue。 消息重试顺序消息的重试对于顺序消息，当消费者消费消息失败后，消息队列 RocketMQ 会自动不断进行消息重试（每次间隔时间为 1 秒），这时，应用会出现消息消费被阻塞的情况。因此，在使用顺序消息时，务必保证应用能够及时监控并处理消费失败的情况，避免阻塞现象的发生。 无序消息的重试对于无序消息（普通、定时、延时、事务消息），当消费者消费消息失败时，您可以通过设置返回状态达到消息重试的结果。 无序消息的重试只针对集群消费方式生效；广播方式不提供失败重试特性，即消费失败后，失败消息不再重试，继续消费新的消息。 重试次数消息队列 RocketMQ 默认允许每条消息最多重试 16 次，每次重试的间隔时间如下： 第几次重试 与上次重试的间隔时间 第几次重试 与上次重试的间隔时间 1 10 秒 9 7 分钟 2 30 秒 10 8 分钟 3 1 分钟 11 9 分钟 4 2 分钟 12 10 分钟 5 3 分钟 13 20 分钟 6 4 分钟 14 30 分钟 7 5 分钟 15 1 小时 8 6 分钟 16 2 小时 如果消息重试 16 次后仍然失败，消息将不再投递。如果严格按照上述重试时间间隔计算，某条消息在一直消费失败的前提下，将会在接下来的 4 小时 46 分钟之内进行 16 次重试，超过这个时间范围消息将不再重试投递。 注意： 一条消息无论重试多少次，这些重试消息的 Message ID 不会改变。 配置方式消费失败后，重试配置方式 集群消费方式下，消息消费失败后期望消息重试，需要在消息监听器接口的实现中明确进行配置（三种方式任选一种）： 返回 Action.ReconsumeLater （推荐） 返回 Null 抛出异常 12345678910111213public class MessageListenerImpl implements MessageListener &#123; @Override public Action consume(Message message, ConsumeContext context) &#123; //处理消息 doConsumeMessage(message); //方式1：返回 Action.ReconsumeLater，消息将重试 return Action.ReconsumeLater; //方式2：返回 null，消息将重试 return null; //方式3：直接抛出异常， 消息将重试 throw new RuntimeException("Consumer Message exceotion"); &#125;&#125; 消费失败后，不重试配置方式 集群消费方式下，消息失败后期望消息不重试，需要捕获消费逻辑中可能抛出的异常，最终返回 Action.CommitMessage，此后这条消息将不会再重试。 12345678910111213public class MessageListenerImpl implements MessageListener &#123; @Override public Action consume(Message message, ConsumeContext context) &#123; try &#123; doConsumeMessage(message); &#125; catch (Throwable e) &#123; //捕获消费逻辑中的所有异常，并返回 Action.CommitMessage; return Action.CommitMessage; &#125; //消息处理正常，直接返回 Action.CommitMessage; return Action.CommitMessage; &#125;&#125; 自定义消息最大重试次数 消息队列 RocketMQ 允许 Consumer 启动的时候设置最大重试次数，重试时间间隔将按照如下策略： 最大重试次数小于等于 16 次，则重试时间间隔同上表描述。 最大重试次数大于 16 次，超过 16 次的重试时间间隔均为每次 2 小时。 1234Properties properties = new Properties();//配置对应 Group ID 的最大消息重试次数为 20 次properties.put(PropertyKeyConst.MaxReconsumeTimes,"20");Consumer consumer =ONSFactory.createConsumer(properties); 注意： 消息最大重试次数的设置对相同 Group ID 下的所有 Consumer 实例有效。 如果只对相同 Group ID 下两个 Consumer 实例中的其中一个设置了 MaxReconsumeTimes，那么该配置对两个 Consumer 实例均生效。 配置采用覆盖的方式生效，即最后启动的 Consumer 实例会覆盖之前的启动实例的配置 获取消息重试次数 消费者收到消息后，可按照如下方式获取消息的重试次数： 12345678public class MessageListenerImpl implements MessageListener &#123; @Override public Action consume(Message message, ConsumeContext context) &#123; //获取消息的重试次数 System.out.println(message.getReconsumeTimes()); return Action.CommitMessage; &#125;&#125; 死信队列当一条消息初次消费失败，消息队列 RocketMQ 会自动进行消息重试；达到最大重试次数后，若消费依然失败，则表明消费者在正常情况下无法正确地消费该消息，此时，消息队列 RocketMQ 不会立刻将消息丢弃，而是将其发送到该消费者对应的特殊队列中。 在消息队列 RocketMQ 中，这种正常情况下无法被消费的消息称为死信消息（Dead-Letter Message），存储死信消息的特殊队列称为死信队列（Dead-Letter Queue）。 死信特性死信消息具有以下特性 不会再被消费者正常消费。 有效期与正常消息相同，均为 3 天，3 天后会被自动删除。因此，请在死信消息产生后的 3 天内及时处理。 死信队列具有以下特性： 一个死信队列对应一个 Group ID， 而不是对应单个消费者实例。 如果一个 Group ID 未产生死信消息，消息队列 RocketMQ 不会为其创建相应的死信队列。 一个死信队列包含了对应 Group ID 产生的所有死信消息，不论该消息属于哪个 Topic。 查看死信信息 在控制台查询出现死信队列的主题信息 在消息界面根据主题查询死信消息 选择重新发送消息 一条消息进入死信队列，意味着某些因素导致消费者无法正常消费该消息，因此，通常需要您对其进行特殊处理。排查可疑因素并解决问题后，可以在消息队列 RocketMQ 控制台重新发送该消息，让消费者重新消费一次。 消费幂等消息队列 RocketMQ 消费者在接收到消息以后，有必要根据业务上的唯一 Key 对消息做幂等处理的必要性。 消费幂等的必要性在互联网应用中，尤其在网络不稳定的情况下，消息队列 RocketMQ 的消息有可能会出现重复，这个重复简单可以概括为以下情况： 发送时消息重复 当一条消息已被成功发送到服务端并完成持久化，此时出现了网络闪断或者客户端宕机，导致服务端对客户端应答失败。 如果此时生产者意识到消息发送失败并尝试再次发送消息，消费者后续会收到两条内容相同并且 Message ID 也相同的消息。 投递时消息重复 消息消费的场景下，消息已投递到消费者并完成业务处理，当客户端给服务端反馈应答的时候网络闪断。 为了保证消息至少被消费一次，消息队列 RocketMQ 的服务端将在网络恢复后再次尝试投递之前已被处理过的消息，消费者后续会收到两条内容相同并且 Message ID 也相同的消息。 负载均衡时消息重复（包括但不限于网络抖动、Broker 重启以及订阅方应用重启） 当消息队列 RocketMQ 的 Broker 或客户端重启、扩容或缩容时，会触发 Rebalance，此时消费者可能会收到重复消息。 处理方式因为 Message ID 有可能出现冲突（重复）的情况，所以真正安全的幂等处理，不建议以 Message ID 作为处理依据。 最好的方式是以业务唯一标识作为幂等处理的关键依据，而业务的唯一标识可以通过消息 Key 进行设置： 123Message message = new Message();message.setKey("ORDERID_100");SendResult sendResult = producer.send(message); 订阅方收到消息时可以根据消息的 Key 进行幂等处理： 123456consumer.subscribe("ons_test", "*", new MessageListener() &#123; public Action consume(Message message, ConsumeContext context) &#123; String key = message.getKey() // 根据业务唯一标识的 key 做幂等处理 &#125;&#125;); 源码分析环境搭建依赖工具 JDK ：1.8+ Maven IntelliJ IDEA 源码拉取从官方仓库 https://github.com/apache/rocketmq clone或者download源码。 源码目录结构： broker: broker 模块（broke 启动进程） client ：消息客户端，包含消息生产者、消息消费者相关类 common ：公共包 dev ：开发者信息（非源代码） distribution ：部署实例文件夹（非源代码） example: RocketMQ 例代码 filter ：消息过滤相关基础类 filtersrv：消息过滤服务器实现相关类（Filter启动进程） logappender：日志实现相关类 namesrv：NameServer实现相关类（NameServer启动进程） openmessageing：消息开放标准 remoting：远程通信模块，给予Netty srcutil：服务工具类 store：消息存储实现相关类 style：checkstyle相关实现 test：测试相关类 tools：工具类，监控命令相关实现类 导入IDEA 执行安装 1clean install -Dmaven.test.skip=true 调试创建conf配置文件夹,从distribution拷贝broker.conf和logback_broker.xml和logback_namesrv.xml 启动NameServer 展开namesrv模块，右键NamesrvStartup.java 配置ROCKETMQ_HOME 重新启动 控制台打印结果 1The Name Server boot success. serializeType=JSON 启动Broker broker.conf配置文件内容 1234567891011121314151617181920212223brokerClusterName = DefaultClusterbrokerName = broker-abrokerId = 0# namesrvAddr地址namesrvAddr=127.0.0.1:9876deleteWhen = 04fileReservedTime = 48brokerRole = ASYNC_MASTERflushDiskType = ASYNC_FLUSHautoCreateTopicEnable=true# 存储路径storePathRootDir=E:\\RocketMQ\\data\\rocketmq\\dataDir# commitLog路径storePathCommitLog=E:\\RocketMQ\\data\\rocketmq\\dataDir\\commitlog# 消息队列存储路径storePathConsumeQueue=E:\\RocketMQ\\data\\rocketmq\\dataDir\\consumequeue# 消息索引存储路径storePathIndex=E:\\RocketMQ\\data\\rocketmq\\dataDir\\index# checkpoint文件路径storeCheckpoint=E:\\RocketMQ\\data\\rocketmq\\dataDir\\checkpoint# abort文件存储路径abortFile=E:\\RocketMQ\\data\\rocketmq\\dataDir\\abort 创建数据文件夹dataDir 启动BrokerStartup,配置broker.conf和ROCKETMQ_HOME 发送消息 进入example模块的org.apache.rocketmq.example.quickstart 指定Namesrv地址 12DefaultMQProducer producer = new DefaultMQProducer("please_rename_unique_group_name");producer.setNamesrvAddr("127.0.0.1:9876"); 运行main方法，发送消息 消费消息 进入example模块的org.apache.rocketmq.example.quickstart 指定Namesrv地址 12DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("please_rename_unique_group_name_4");consumer.setNamesrvAddr("127.0.0.1:9876"); 运行main方法，消费消息 NameServer架构设计消息中间件的设计思路一般是基于主题订阅发布的机制，消息生产者（Producer）发送某一个主题到消息服务器，消息服务器负责将消息持久化存储，消息消费者（Consumer）订阅该兴趣的主题，消息服务器根据订阅信息（路由信息）将消息推送到消费者（Push模式）或者消费者主动向消息服务器拉去（Pull模式），从而实现消息生产者与消息消费者解耦。为了避免消息服务器的单点故障导致的整个系统瘫痪，通常会部署多台消息服务器共同承担消息的存储。那消息生产者如何知道消息要发送到哪台消息服务器呢？如果某一台消息服务器宕机了，那么消息生产者如何在不重启服务情况下感知呢？ NameServer就是为了解决以上问题设计的。 Broker消息服务器在启动的时向所有NameServer注册，消息生产者（Producer）在发送消息时之前先从NameServer获取Broker服务器地址列表，然后根据负载均衡算法从列表中选择一台服务器进行发送。NameServer与每台Broker保持长连接，并间隔30S检测Broker是否存活，如果检测到Broker宕机，则从路由注册表中删除。但是路由变化不会马上通知消息生产者。这样设计的目的是为了降低NameServer实现的复杂度，在消息发送端提供容错机制保证消息发送的可用性。 NameServer本身的高可用是通过部署多台NameServer来实现，但彼此之间不通讯，也就是NameServer服务器之间在某一个时刻的数据并不完全相同，但这对消息发送并不会造成任何影响，这也是NameServer设计的一个亮点，总之，RocketMQ设计追求简单高效。 启动流程 启动类：org.apache.rocketmq.namesrv.NamesrvStartup 步骤一解析配置文件，填充NameServerConfig、NettyServerConfig属性值，并创建NamesrvController 代码：NamesrvController#createNamesrvController 12345678910111213141516171819202122232425262728293031323334//创建NamesrvConfigfinal NamesrvConfig namesrvConfig = new NamesrvConfig();//创建NettyServerConfigfinal NettyServerConfig nettyServerConfig = new NettyServerConfig();//设置启动端口号nettyServerConfig.setListenPort(9876);//解析启动-c参数if (commandLine.hasOption('c')) &#123; String file = commandLine.getOptionValue('c'); if (file != null) &#123; InputStream in = new BufferedInputStream(new FileInputStream(file)); properties = new Properties(); properties.load(in); MixAll.properties2Object(properties, namesrvConfig); MixAll.properties2Object(properties, nettyServerConfig); namesrvConfig.setConfigStorePath(file); System.out.printf("load config properties file OK, %s%n", file); in.close(); &#125;&#125;//解析启动-p参数if (commandLine.hasOption('p')) &#123; InternalLogger console = InternalLoggerFactory.getLogger(LoggerName.NAMESRV_CONSOLE_NAME); MixAll.printObjectProperties(console, namesrvConfig); MixAll.printObjectProperties(console, nettyServerConfig); System.exit(0);&#125;//将启动参数填充到namesrvConfig,nettyServerConfigMixAll.properties2Object(ServerUtil.commandLine2Properties(commandLine), namesrvConfig);//创建NameServerControllerfinal NamesrvController controller = new NamesrvController(namesrvConfig, nettyServerConfig); NamesrvConfig属性 123456private String rocketmqHome = System.getProperty(MixAll.ROCKETMQ_HOME_PROPERTY, System.getenv(MixAll.ROCKETMQ_HOME_ENV));private String kvConfigPath = System.getProperty("user.home") + File.separator + "namesrv" + File.separator + "kvConfig.json";private String configStorePath = System.getProperty("user.home") + File.separator + "namesrv" + File.separator + "namesrv.properties";private String productEnvName = "center";private boolean clusterTest = false;private boolean orderMessageEnable = false; rocketmqHome：rocketmq主目录 kvConfig：NameServer存储KV配置属性的持久化路径 configStorePath：nameServer默认配置文件路径 orderMessageEnable：是否支持顺序消息 NettyServerConfig属性 1234567891011private int listenPort = 8888;private int serverWorkerThreads = 8;private int serverCallbackExecutorThreads = 0;private int serverSelectorThreads = 3;private int serverOnewaySemaphoreValue = 256;private int serverAsyncSemaphoreValue = 64;private int serverChannelMaxIdleTimeSeconds = 120;private int serverSocketSndBufSize = NettySystemConfig.socketSndbufSize;private int serverSocketRcvBufSize = NettySystemConfig.socketRcvbufSize;private boolean serverPooledByteBufAllocatorEnable = true;private boolean useEpollNativeSelector = false; listenPort：NameServer监听端口，该值默认会被初始化为9876serverWorkerThreads：Netty业务线程池线程个数serverCallbackExecutorThreads：Netty public任务线程池线程个数，Netty网络设计，根据业务类型会创建不同的线程池，比如处理消息发送、消息消费、心跳检测等。如果该业务类型未注册线程池，则由public线程池执行。serverSelectorThreads：IO线程池个数，主要是NameServer、Broker端解析请求、返回相应的线程个数，这类线程主要是处理网路请求的，解析请求包，然后转发到各个业务线程池完成具体的操作，然后将结果返回给调用方;serverOnewaySemaphoreValue：send oneway消息请求并发读（Broker端参数）;serverAsyncSemaphoreValue：异步消息发送最大并发度;serverChannelMaxIdleTimeSeconds ：网络连接最大的空闲时间，默认120s。serverSocketSndBufSize：网络socket发送缓冲区大小。serverSocketRcvBufSize： 网络接收端缓存区大小。serverPooledByteBufAllocatorEnable：ByteBuffer是否开启缓存;useEpollNativeSelector：是否启用Epoll IO模型。 步骤二根据启动属性创建NamesrvController实例，并初始化该实例。NameServerController实例为NameServer核心控制器 代码：NamesrvController#initialize 12345678910111213141516171819202122232425public boolean initialize() &#123; //加载KV配置 this.kvConfigManager.load(); //创建NettyServer网络处理对象 this.remotingServer = new NettyRemotingServer(this.nettyServerConfig, this.brokerHousekeepingService); //开启定时任务:每隔10s扫描一次Broker,移除不活跃的Broker this.remotingExecutor = Executors.newFixedThreadPool(nettyServerConfig.getServerWorkerThreads(), new ThreadFactoryImpl("RemotingExecutorThread_")); this.registerProcessor(); this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; NamesrvController.this.routeInfoManager.scanNotActiveBroker(); &#125; &#125;, 5, 10, TimeUnit.SECONDS); //开启定时任务:每隔10min打印一次KV配置 this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; NamesrvController.this.kvConfigManager.printAllPeriodically(); &#125; &#125;, 1, 10, TimeUnit.MINUTES); return true;&#125; 步骤三在JVM进程关闭之前，先将线程池关闭，及时释放资源 代码：NamesrvStartup#start 123456789//注册JVM钩子函数代码Runtime.getRuntime().addShutdownHook(new ShutdownHookThread(log, new Callable&lt;Void&gt;() &#123; @Override public Void call() throws Exception &#123; //释放资源 controller.shutdown(); return null; &#125;&#125;)); 路由管理NameServer的主要作用是为消息的生产者和消息消费者提供关于主题Topic的路由信息，那么NameServer需要存储路由的基础信息，还要管理Broker节点，包括路由注册、路由删除等。 路由元信息代码：RouteInfoManager 12345private final HashMap&lt;String/* topic */, List&lt;QueueData&gt;&gt; topicQueueTable;private final HashMap&lt;String/* brokerName */, BrokerData&gt; brokerAddrTable;private final HashMap&lt;String/* clusterName */, Set&lt;String/* brokerName */&gt;&gt; clusterAddrTable;private final HashMap&lt;String/* brokerAddr */, BrokerLiveInfo&gt; brokerLiveTable;private final HashMap&lt;String/* brokerAddr */, List&lt;String&gt;/* Filter Server */&gt; filterServerTable; topicQueueTable：Topic消息队列路由信息，消息发送时根据路由表进行负载均衡 brokerAddrTable：Broker基础信息，包括brokerName、所属集群名称、主备Broker地址 clusterAddrTable：Broker集群信息，存储集群中所有Broker名称 brokerLiveTable：Broker状态信息，NameServer每次收到心跳包是会替换该信息 filterServerTable：Broker上的FilterServer列表，用于类模式消息过滤。 RocketMQ基于定于发布机制，一个Topic拥有多个消息队列，一个Broker为每一个主题创建4个读队列和4个写队列。多个Broker组成一个集群，集群由相同的多台Broker组成Master-Slave架构，brokerId为0代表Master，大于0为Slave。BrokerLiveInfo中的lastUpdateTimestamp存储上次收到Broker心跳包的时间。 路由注册发送心跳包 RocketMQ路由注册是通过Broker与NameServer的心跳功能实现的。Broker启动时向集群中所有的NameServer发送心跳信息，每隔30s向集群中所有NameServer发送心跳包，NameServer收到心跳包时会更新brokerLiveTable缓存中BrokerLiveInfo的lastUpdataTimeStamp信息，然后NameServer每隔10s扫描brokerLiveTable，如果连续120S没有收到心跳包，NameServer将移除Broker的路由信息同时关闭Socket连接。 代码：BrokerController#start 123456789101112131415//注册Broker信息this.registerBrokerAll(true, false, true);//每隔30s上报Broker信息到NameServerthis.scheduledExecutorService.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; try &#123; BrokerController.this.registerBrokerAll(true, false, brokerConfig.isForceRegister()); &#125; catch (Throwable e) &#123; log.error("registerBrokerAll Exception", e); &#125; &#125;&#125;, 1000 * 10, Math.max(10000, Math.min(brokerConfig.getRegisterNameServerPeriod(), 60000)), TimeUnit.MILLISECONDS); 代码：BrokerOuterAPI#registerBrokerAll 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647//获得nameServer地址信息List&lt;String&gt; nameServerAddressList = this.remotingClient.getNameServerAddressList();//遍历所有nameserver列表if (nameServerAddressList != null &amp;&amp; nameServerAddressList.size() &gt; 0) &#123; //封装请求头 final RegisterBrokerRequestHeader requestHeader = new RegisterBrokerRequestHeader(); requestHeader.setBrokerAddr(brokerAddr); requestHeader.setBrokerId(brokerId); requestHeader.setBrokerName(brokerName); requestHeader.setClusterName(clusterName); requestHeader.setHaServerAddr(haServerAddr); requestHeader.setCompressed(compressed); //封装请求体 RegisterBrokerBody requestBody = new RegisterBrokerBody(); requestBody.setTopicConfigSerializeWrapper(topicConfigWrapper); requestBody.setFilterServerList(filterServerList); final byte[] body = requestBody.encode(compressed); final int bodyCrc32 = UtilAll.crc32(body); requestHeader.setBodyCrc32(bodyCrc32); final CountDownLatch countDownLatch = new CountDownLatch(nameServerAddressList.size()); for (final String namesrvAddr : nameServerAddressList) &#123; brokerOuterExecutor.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; //分别向NameServer注册 RegisterBrokerResult result = registerBroker(namesrvAddr,oneway, timeoutMills,requestHeader,body); if (result != null) &#123; registerBrokerResultList.add(result); &#125; log.info("register broker[&#123;&#125;]to name server &#123;&#125; OK", brokerId, namesrvAddr); &#125; catch (Exception e) &#123; log.warn("registerBroker Exception, &#123;&#125;", namesrvAddr, e); &#125; finally &#123; countDownLatch.countDown(); &#125; &#125; &#125;); &#125; try &#123; countDownLatch.await(timeoutMills, TimeUnit.MILLISECONDS); &#125; catch (InterruptedException e) &#123; &#125;&#125; 代码：BrokerOutAPI#registerBroker 123456789if (oneway) &#123; try &#123; this.remotingClient.invokeOneway(namesrvAddr, request, timeoutMills); &#125; catch (RemotingTooMuchRequestException e) &#123; // Ignore &#125; return null;&#125;RemotingCommand response = this.remotingClient.invokeSync(namesrvAddr, request, timeoutMills); 处理心跳包 org.apache.rocketmq.namesrv.processor.DefaultRequestProcessor网路处理类解析请求类型，如果请求类型是为REGISTER_BROKER，则将请求转发到RouteInfoManager#regiesterBroker 代码：DefaultRequestProcessor#processRequest 123456789//判断是注册Broker信息case RequestCode.REGISTER_BROKER: Version brokerVersion = MQVersion.value2Version(request.getVersion()); if (brokerVersion.ordinal() &gt;= MQVersion.Version.V3_0_11.ordinal()) &#123; return this.registerBrokerWithFilterServer(ctx, request); &#125; else &#123; //注册Broker信息 return this.registerBroker(ctx, request); &#125; 代码：DefaultRequestProcessor#registerBroker 12345678910RegisterBrokerResult result = this.namesrvController.getRouteInfoManager().registerBroker( requestHeader.getClusterName(), requestHeader.getBrokerAddr(), requestHeader.getBrokerName(), requestHeader.getBrokerId(), requestHeader.getHaServerAddr(), topicConfigWrapper, null, ctx.channel()); 代码：RouteInfoManager#registerBroker 维护路由信息 123456789//加锁this.lock.writeLock().lockInterruptibly();//维护clusterAddrTableSet&lt;String&gt; brokerNames = this.clusterAddrTable.get(clusterName);if (null == brokerNames) &#123; brokerNames = new HashSet&lt;String&gt;(); this.clusterAddrTable.put(clusterName, brokerNames);&#125;brokerNames.add(brokerName); 12345678910111213141516171819//维护brokerAddrTableBrokerData brokerData = this.brokerAddrTable.get(brokerName);//第一次注册,则创建brokerDataif (null == brokerData) &#123; registerFirst = true; brokerData = new BrokerData(clusterName, brokerName, new HashMap&lt;Long, String&gt;()); this.brokerAddrTable.put(brokerName, brokerData);&#125;//非第一次注册,更新BrokerMap&lt;Long, String&gt; brokerAddrsMap = brokerData.getBrokerAddrs();Iterator&lt;Entry&lt;Long, String&gt;&gt; it = brokerAddrsMap.entrySet().iterator();while (it.hasNext()) &#123; Entry&lt;Long, String&gt; item = it.next(); if (null != brokerAddr &amp;&amp; brokerAddr.equals(item.getValue()) &amp;&amp; brokerId != item.getKey()) &#123; it.remove(); &#125;&#125;String oldAddr = brokerData.getBrokerAddrs().put(brokerId, brokerAddr);registerFirst = registerFirst || (null == oldAddr); 123456789101112//维护topicQueueTableif (null != topicConfigWrapper &amp;&amp; MixAll.MASTER_ID == brokerId) &#123; if (this.isBrokerTopicConfigChanged(brokerAddr, topicConfigWrapper.getDataVersion()) || registerFirst) &#123; ConcurrentMap&lt;String, TopicConfig&gt; tcTable = topicConfigWrapper.getTopicConfigTable(); if (tcTable != null) &#123; for (Map.Entry&lt;String, TopicConfig&gt; entry : tcTable.entrySet()) &#123; this.createAndUpdateQueueData(brokerName, entry.getValue()); &#125; &#125; &#125;&#125; 代码：RouteInfoManager#createAndUpdateQueueData 123456789101112131415161718192021222324252627282930313233343536373839private void createAndUpdateQueueData(final String brokerName, final TopicConfig topicConfig) &#123; //创建QueueData QueueData queueData = new QueueData(); queueData.setBrokerName(brokerName); queueData.setWriteQueueNums(topicConfig.getWriteQueueNums()); queueData.setReadQueueNums(topicConfig.getReadQueueNums()); queueData.setPerm(topicConfig.getPerm()); queueData.setTopicSynFlag(topicConfig.getTopicSysFlag()); //获得topicQueueTable中队列集合 List&lt;QueueData&gt; queueDataList = this.topicQueueTable.get(topicConfig.getTopicName()); //topicQueueTable为空,则直接添加queueData到队列集合 if (null == queueDataList) &#123; queueDataList = new LinkedList&lt;QueueData&gt;(); queueDataList.add(queueData); this.topicQueueTable.put(topicConfig.getTopicName(), queueDataList); log.info("new topic registered, &#123;&#125; &#123;&#125;", topicConfig.getTopicName(), queueData); &#125; else &#123; //判断是否是新的队列 boolean addNewOne = true; Iterator&lt;QueueData&gt; it = queueDataList.iterator(); while (it.hasNext()) &#123; QueueData qd = it.next(); //如果brokerName相同,代表不是新的队列 if (qd.getBrokerName().equals(brokerName)) &#123; if (qd.equals(queueData)) &#123; addNewOne = false; &#125; else &#123; log.info("topic changed, &#123;&#125; OLD: &#123;&#125; NEW: &#123;&#125;", topicConfig.getTopicName(), qd, queueData); it.remove(); &#125; &#125; &#125; //如果是新的队列,则添加队列到queueDataList if (addNewOne) &#123; queueDataList.add(queueData); &#125; &#125;&#125; 123456//维护brokerLiveTableBrokerLiveInfo prevBrokerLiveInfo = this.brokerLiveTable.put(brokerAddr,new BrokerLiveInfo( System.currentTimeMillis(), topicConfigWrapper.getDataVersion(), channel, haServerAddr)); 12345678910111213141516171819//维护filterServerListif (filterServerList != null) &#123; if (filterServerList.isEmpty()) &#123; this.filterServerTable.remove(brokerAddr); &#125; else &#123; this.filterServerTable.put(brokerAddr, filterServerList); &#125;&#125;if (MixAll.MASTER_ID != brokerId) &#123; String masterAddr = brokerData.getBrokerAddrs().get(MixAll.MASTER_ID); if (masterAddr != null) &#123; BrokerLiveInfo brokerLiveInfo = this.brokerLiveTable.get(masterAddr); if (brokerLiveInfo != null) &#123; result.setHaServerAddr(brokerLiveInfo.getHaServerAddr()); result.setMasterAddr(masterAddr); &#125; &#125;&#125; 路由删除123456789101112131415161718192021**RocketMQ有两个触发点来删除路由信息**：* NameServer定期扫描brokerLiveTable检测上次心跳包与当前系统的时间差，如果时间超过120s，则需要移除broker。* Broker在正常关闭的情况下，会执行unregisterBroker指令这两种方式路由删除的方法都是一样的，就是从相关路由表中删除与该broker相关的信息。&lt;img src=&quot;https://klutzoder-blog.oss-cn-beijing.aliyuncs.com/2020/03/lu-you-shan-chu.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_a2x1dHpvZGVy,color_0c0c0c,size_20,g_se,x_10,y_10&quot; width=&quot;600&quot; alt=&quot;&quot; title=&quot;&quot;&gt;***代码：NamesrvController#initialize***```java//每隔10s扫描一次为活跃Brokerthis.scheduledExecutorService.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; NamesrvController.this.routeInfoManager.scanNotActiveBroker(); &#125;&#125;, 5, 10, TimeUnit.SECONDS); 代码：RouteInfoManager#scanNotActiveBroker 123456789101112131415161718public void scanNotActiveBroker() &#123; //获得brokerLiveTable Iterator&lt;Entry&lt;String, BrokerLiveInfo&gt;&gt; it = this.brokerLiveTable.entrySet().iterator(); //遍历brokerLiveTable while (it.hasNext()) &#123; Entry&lt;String, BrokerLiveInfo&gt; next = it.next(); long last = next.getValue().getLastUpdateTimestamp(); //如果收到心跳包的时间距当时时间是否超过120s if ((last + BROKER_CHANNEL_EXPIRED_TIME) &lt; System.currentTimeMillis()) &#123; //关闭连接 RemotingUtil.closeChannel(next.getValue().getChannel()); //移除broker it.remove(); //维护路由表 this.onChannelDestroy(next.getKey(), next.getValue().getChannel()); &#125; &#125;&#125; 代码：RouteInfoManager#onChannelDestroy 1234//申请写锁,根据brokerAddress从brokerLiveTable和filterServerTable移除this.lock.writeLock().lockInterruptibly();this.brokerLiveTable.remove(brokerAddrFound);this.filterServerTable.remove(brokerAddrFound); 123456789101112131415161718192021222324252627282930//维护brokerAddrTableString brokerNameFound = null;boolean removeBrokerName = false;Iterator&lt;Entry&lt;String, BrokerData&gt;&gt; itBrokerAddrTable =this.brokerAddrTable.entrySet().iterator();//遍历brokerAddrTablewhile (itBrokerAddrTable.hasNext() &amp;&amp; (null == brokerNameFound)) &#123; BrokerData brokerData = itBrokerAddrTable.next().getValue(); //遍历broker地址 Iterator&lt;Entry&lt;Long, String&gt;&gt; it = brokerData.getBrokerAddrs().entrySet().iterator(); while (it.hasNext()) &#123; Entry&lt;Long, String&gt; entry = it.next(); Long brokerId = entry.getKey(); String brokerAddr = entry.getValue(); //根据broker地址移除brokerAddr if (brokerAddr.equals(brokerAddrFound)) &#123; brokerNameFound = brokerData.getBrokerName(); it.remove(); log.info("remove brokerAddr[&#123;&#125;, &#123;&#125;] from brokerAddrTable, because channel destroyed", brokerId, brokerAddr); break; &#125; &#125; //如果当前主题只包含待移除的broker,则移除该topic if (brokerData.getBrokerAddrs().isEmpty()) &#123; removeBrokerName = true; itBrokerAddrTable.remove(); log.info("remove brokerName[&#123;&#125;] from brokerAddrTable, because channel destroyed", brokerData.getBrokerName()); &#125;&#125; 123456789101112131415161718192021222324252627//维护clusterAddrTableif (brokerNameFound != null &amp;&amp; removeBrokerName) &#123; Iterator&lt;Entry&lt;String, Set&lt;String&gt;&gt;&gt; it = this.clusterAddrTable.entrySet().iterator(); //遍历clusterAddrTable while (it.hasNext()) &#123; Entry&lt;String, Set&lt;String&gt;&gt; entry = it.next(); //获得集群名称 String clusterName = entry.getKey(); //获得集群中brokerName集合 Set&lt;String&gt; brokerNames = entry.getValue(); //从brokerNames中移除brokerNameFound boolean removed = brokerNames.remove(brokerNameFound); if (removed) &#123; log.info("remove brokerName[&#123;&#125;], clusterName[&#123;&#125;] from clusterAddrTable, because channel destroyed", brokerNameFound, clusterName); if (brokerNames.isEmpty()) &#123; log.info("remove the clusterName[&#123;&#125;] from clusterAddrTable, because channel destroyed and no broker in this cluster", clusterName); //如果集群中不包含任何broker,则移除该集群 it.remove(); &#125; break; &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930//维护topicQueueTable队列if (removeBrokerName) &#123; //遍历topicQueueTable Iterator&lt;Entry&lt;String, List&lt;QueueData&gt;&gt;&gt; itTopicQueueTable = this.topicQueueTable.entrySet().iterator(); while (itTopicQueueTable.hasNext()) &#123; Entry&lt;String, List&lt;QueueData&gt;&gt; entry = itTopicQueueTable.next(); //主题名称 String topic = entry.getKey(); //队列集合 List&lt;QueueData&gt; queueDataList = entry.getValue(); //遍历该主题队列 Iterator&lt;QueueData&gt; itQueueData = queueDataList.iterator(); while (itQueueData.hasNext()) &#123; //从队列中移除为活跃broker信息 QueueData queueData = itQueueData.next(); if (queueData.getBrokerName().equals(brokerNameFound)) &#123; itQueueData.remove(); log.info("remove topic[&#123;&#125; &#123;&#125;], from topicQueueTable, because channel destroyed", topic, queueData); &#125; &#125; //如果该topic的队列为空,则移除该topic if (queueDataList.isEmpty()) &#123; itTopicQueueTable.remove(); log.info("remove topic[&#123;&#125;] all queue, from topicQueueTable, because channel destroyed", topic); &#125; &#125;&#125; 1234//释放写锁finally &#123; this.lock.writeLock().unlock();&#125; 路由发现RocketMQ路由发现是非实时的，当Topic路由出现变化后，NameServer不会主动推送给客户端，而是由客户端定时拉取主题最新的路由。 代码：DefaultRequestProcessor#getRouteInfoByTopic 12345678910111213141516171819202122232425262728public RemotingCommand getRouteInfoByTopic(ChannelHandlerContext ctx, RemotingCommand request) throws RemotingCommandException &#123; final RemotingCommand response = RemotingCommand.createResponseCommand(null); final GetRouteInfoRequestHeader requestHeader = (GetRouteInfoRequestHeader) request.decodeCommandCustomHeader(GetRouteInfoRequestHeader.class); //调用RouteInfoManager的方法,从路由表topicQueueTable、brokerAddrTable、filterServerTable中分别填充TopicRouteData的List&lt;QueueData&gt;、List&lt;BrokerData&gt;、filterServer TopicRouteData topicRouteData = this.namesrvController.getRouteInfoManager().pickupTopicRouteData(requestHeader.getTopic()); //如果找到主题对应你的路由信息并且该主题为顺序消息，则从NameServer KVConfig中获取关于顺序消息相关的配置填充路由信息 if (topicRouteData != null) &#123; if (this.namesrvController.getNamesrvConfig().isOrderMessageEnable()) &#123; String orderTopicConf = this.namesrvController.getKvConfigManager().getKVConfig(NamesrvUtil.NAMESPACE_ORDER_TOPIC_CONFIG, requestHeader.getTopic()); topicRouteData.setOrderTopicConf(orderTopicConf); &#125; byte[] content = topicRouteData.encode(); response.setBody(content); response.setCode(ResponseCode.SUCCESS); response.setRemark(null); return response; &#125; response.setCode(ResponseCode.TOPIC_NOT_EXIST); response.setRemark("No topic route info in name server for the topic: " + requestHeader.getTopic() + FAQUrl.suggestTodo(FAQUrl.APPLY_TOPIC_URL)); return response;&#125; 小结 Producer消息生产者的代码都在client模块中，相对于RocketMQ来讲，消息生产者就是客户端，也是消息的提供者。 方法和属性主要方法介绍 12//创建主题void createTopic(final String key, final String newTopic, final int queueNum) throws MQClientException; 12//根据时间戳从队列中查找消息偏移量long searchOffset(final MessageQueue mq, final long timestamp) 12//查找消息队列中最大的偏移量long maxOffset(final MessageQueue mq) throws MQClientException; 12//查找消息队列中最小的偏移量long minOffset(final MessageQueue mq) 123//根据偏移量查找消息MessageExt viewMessage(final String offsetMsgId) throws RemotingException, MQBrokerException, InterruptedException, MQClientException; 123//根据条件查找消息QueryResult queryMessage(final String topic, final String key, final int maxNum, final long begin, final long end) throws MQClientException, InterruptedException; 12//根据消息ID和主题查找消息MessageExt viewMessage(String topic,String msgId) throws RemotingException, MQBrokerException, InterruptedException, MQClientException; 12//启动void start() throws MQClientException; 12//关闭void shutdown(); 12//查找该主题下所有消息List&lt;MessageQueue&gt; fetchPublishMessageQueues(final String topic) throws MQClientException; 123//同步发送消息SendResult send(final Message msg) throws MQClientException, RemotingException, MQBrokerException, InterruptedException; 123//同步超时发送消息SendResult send(final Message msg, final long timeout) throws MQClientException, RemotingException, MQBrokerException, InterruptedException; 123//异步发送消息void send(final Message msg, final SendCallback sendCallback) throws MQClientException, RemotingException, InterruptedException; 123//异步超时发送消息void send(final Message msg, final SendCallback sendCallback, final long timeout) throws MQClientException, RemotingException, InterruptedException; 123//发送单向消息void sendOneway(final Message msg) throws MQClientException, RemotingException, InterruptedException; 123//选择指定队列同步发送消息SendResult send(final Message msg, final MessageQueue mq) throws MQClientException, RemotingException, MQBrokerException, InterruptedException; 123//选择指定队列异步发送消息void send(final Message msg, final MessageQueue mq, final SendCallback sendCallback) throws MQClientException, RemotingException, InterruptedException; 123//选择指定队列单项发送消息void sendOneway(final Message msg, final MessageQueue mq) throws MQClientException, RemotingException, InterruptedException; 12//批量发送消息SendResult send(final Collection&lt;Message&gt; msgs) throws MQClientException, RemotingException, MQBrokerException,InterruptedException; 属性介绍 123456789producerGroup：生产者所属组createTopicKey：默认TopicdefaultTopicQueueNums：默认主题在每一个Broker队列数量sendMsgTimeout：发送消息默认超时时间，默认3scompressMsgBodyOverHowmuch：消息体超过该值则启用压缩，默认4kretryTimesWhenSendFailed：同步方式发送消息重试次数，默认为2，总共执行3次retryTimesWhenSendAsyncFailed：异步方法发送消息重试次数，默认为2retryAnotherBrokerWhenNotStoreOK：消息重试时选择另外一个Broker时，是否不等待存储结果就返回，默认为falsemaxMessageSize：允许发送的最大消息长度，默认为4M 启动流程 代码：DefaultMQProducerImpl#start 12345678//检查生产者组是否满足要求this.checkConfig();//更改当前instanceName为进程IDif (!this.defaultMQProducer.getProducerGroup().equals(MixAll.CLIENT_INNER_PRODUCER_GROUP)) &#123; this.defaultMQProducer.changeInstanceNameToPID();&#125;//获得MQ客户端实例this.mQClientFactory = MQClientManager.getInstance().getAndCreateMQClientInstance(this.defaultMQProducer, rpcHook); 整个JVM中只存在一个MQClientManager实例，维护一个MQClientInstance缓存表 ConcurrentMap factoryTable = new ConcurrentHashMap(); 同一个clientId只会创建一个MQClientInstance。 MQClientInstance封装了RocketMQ网络处理API，是消息生产者和消息消费者与NameServer、Broker打交道的网络通道 代码：MQClientManager#getAndCreateMQClientInstance 12345678910111213141516171819202122public MQClientInstance getAndCreateMQClientInstance(final ClientConfig clientConfig, RPCHook rpcHook) &#123; //构建客户端ID String clientId = clientConfig.buildMQClientId(); //根据客户端ID或者客户端实例 MQClientInstance instance = this.factoryTable.get(clientId); //实例如果为空就创建新的实例,并添加到实例表中 if (null == instance) &#123; instance = new MQClientInstance(clientConfig.cloneClientConfig(), this.factoryIndexGenerator.getAndIncrement(), clientId, rpcHook); MQClientInstance prev = this.factoryTable.putIfAbsent(clientId, instance); if (prev != null) &#123; instance = prev; log.warn("Returned Previous MQClientInstance for clientId:[&#123;&#125;]", clientId); &#125; else &#123; log.info("Created new MQClientInstance for clientId:[&#123;&#125;]", clientId); &#125; &#125; return instance;&#125; 代码：DefaultMQProducerImpl#start 123456789101112//注册当前生产者到到MQClientInstance管理中,方便后续调用网路请求boolean registerOK = mQClientFactory.registerProducer(this.defaultMQProducer.getProducerGroup(), this);if (!registerOK) &#123; this.serviceState = ServiceState.CREATE_JUST; throw new MQClientException("The producer group[" + this.defaultMQProducer.getProducerGroup() + "] has been created before, specify another name please." + FAQUrl.suggestTodo(FAQUrl.GROUP_NAME_DUPLICATE_URL), null);&#125;//启动生产者if (startFactory) &#123; mQClientFactory.start();&#125; 消息发送 代码：DefaultMQProducerImpl#send(Message msg) 1234//发送消息public SendResult send(Message msg) &#123; return send(msg, this.defaultMQProducer.getSendMsgTimeout());&#125; 代码：DefaultMQProducerImpl#send(Message msg,long timeout) 1234//发送消息,默认超时时间为3spublic SendResult send(Message msg,long timeout)&#123; return this.sendDefaultImpl(msg, CommunicationMode.SYNC, null, timeout);&#125; 代码：DefaultMQProducerImpl#sendDefaultImpl 12//校验消息Validators.checkMessage(msg, this.defaultMQProducer); 验证消息代码：Validators#checkMessage 1234567891011121314151617181920212223public static void checkMessage(Message msg, DefaultMQProducer defaultMQProducer) throws MQClientException &#123; //判断是否为空 if (null == msg) &#123; throw new MQClientException(ResponseCode.MESSAGE_ILLEGAL, "the message is null"); &#125; // 校验主题 Validators.checkTopic(msg.getTopic()); // 校验消息体 if (null == msg.getBody()) &#123; throw new MQClientException(ResponseCode.MESSAGE_ILLEGAL, "the message body is null"); &#125; if (0 == msg.getBody().length) &#123; throw new MQClientException(ResponseCode.MESSAGE_ILLEGAL, "the message body length is zero"); &#125; if (msg.getBody().length &gt; defaultMQProducer.getMaxMessageSize()) &#123; throw new MQClientException(ResponseCode.MESSAGE_ILLEGAL, "the message body size over max value, MAX: " + defaultMQProducer.getMaxMessageSize()); &#125;&#125; 查找路由代码：DefaultMQProducerImpl#tryToFindTopicPublishInfo 12345678910111213141516171819private TopicPublishInfo tryToFindTopicPublishInfo(final String topic) &#123; //从缓存中获得主题的路由信息 TopicPublishInfo topicPublishInfo = this.topicPublishInfoTable.get(topic); //路由信息为空,则从NameServer获取路由 if (null == topicPublishInfo || !topicPublishInfo.ok()) &#123; this.topicPublishInfoTable.putIfAbsent(topic, new TopicPublishInfo()); this.mQClientFactory.updateTopicRouteInfoFromNameServer(topic); topicPublishInfo = this.topicPublishInfoTable.get(topic); &#125; if (topicPublishInfo.isHaveTopicRouterInfo() || topicPublishInfo.ok()) &#123; return topicPublishInfo; &#125; else &#123; //如果未找到当前主题的路由信息,则用默认主题继续查找 this.mQClientFactory.updateTopicRouteInfoFromNameServer(topic, true, this.defaultMQProducer); topicPublishInfo = this.topicPublishInfoTable.get(topic); return topicPublishInfo; &#125;&#125; 代码：TopicPublishInfo 1234567public class TopicPublishInfo &#123; private boolean orderTopic = false; //是否是顺序消息 private boolean haveTopicRouterInfo = false; private List&lt;MessageQueue&gt; messageQueueList = new ArrayList&lt;MessageQueue&gt;(); //该主题消息队列 private volatile ThreadLocalIndex sendWhichQueue = new ThreadLocalIndex();//每选择一次消息队列,该值+1 private TopicRouteData topicRouteData;//关联Topic路由元信息&#125; 代码：MQClientInstance#updateTopicRouteInfoFromNameServer 12345678910111213141516TopicRouteData topicRouteData;//使用默认主题从NameServer获取路由信息if (isDefault &amp;&amp; defaultMQProducer != null) &#123; topicRouteData = this.mQClientAPIImpl.getDefaultTopicRouteInfoFromNameServer(defaultMQProducer.getCreateTopicKey(), 1000 * 3); if (topicRouteData != null) &#123; for (QueueData data : topicRouteData.getQueueDatas()) &#123; int queueNums = Math.min(defaultMQProducer.getDefaultTopicQueueNums(), data.getReadQueueNums()); data.setReadQueueNums(queueNums); data.setWriteQueueNums(queueNums); &#125; &#125;&#125; else &#123; //使用指定主题从NameServer获取路由信息 topicRouteData = this.mQClientAPIImpl.getTopicRouteInfoFromNameServer(topic, 1000 * 3);&#125; 代码：MQClientInstance#updateTopicRouteInfoFromNameServer 12345678//判断路由是否需要更改TopicRouteData old = this.topicRouteTable.get(topic);boolean changed = topicRouteDataIsChange(old, topicRouteData);if (!changed) &#123; changed = this.isNeedUpdateTopicRouteInfo(topic);&#125; else &#123; log.info("the topic[&#123;&#125;] route info changed, old[&#123;&#125;] ,new[&#123;&#125;]", topic, old, topicRouteData);&#125; 代码：MQClientInstance#updateTopicRouteInfoFromNameServer 123456789101112131415if (changed) &#123; //将topicRouteData转换为发布队列 TopicPublishInfo publishInfo = topicRouteData2TopicPublishInfo(topic, topicRouteData); publishInfo.setHaveTopicRouterInfo(true); //遍历生产 Iterator&lt;Entry&lt;String, MQProducerInner&gt;&gt; it = this.producerTable.entrySet().iterator(); while (it.hasNext()) &#123; Entry&lt;String, MQProducerInner&gt; entry = it.next(); MQProducerInner impl = entry.getValue(); if (impl != null) &#123; //生产者不为空时,更新publishInfo信息 impl.updateTopicPublishInfo(topic, publishInfo); &#125; &#125;&#125; 代码：MQClientInstance#topicRouteData2TopicPublishInfo 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public static TopicPublishInfo topicRouteData2TopicPublishInfo(final String topic, final TopicRouteData route) &#123; //创建TopicPublishInfo对象 TopicPublishInfo info = new TopicPublishInfo(); //关联topicRoute info.setTopicRouteData(route); //顺序消息,更新TopicPublishInfo if (route.getOrderTopicConf() != null &amp;&amp; route.getOrderTopicConf().length() &gt; 0) &#123; String[] brokers = route.getOrderTopicConf().split(";"); for (String broker : brokers) &#123; String[] item = broker.split(":"); int nums = Integer.parseInt(item[1]); for (int i = 0; i &lt; nums; i++) &#123; MessageQueue mq = new MessageQueue(topic, item[0], i); info.getMessageQueueList().add(mq); &#125; &#125; info.setOrderTopic(true); &#125; else &#123; //非顺序消息更新TopicPublishInfo List&lt;QueueData&gt; qds = route.getQueueDatas(); Collections.sort(qds); //遍历topic队列信息 for (QueueData qd : qds) &#123; //是否是写队列 if (PermName.isWriteable(qd.getPerm())) &#123; BrokerData brokerData = null; //遍历写队列Broker for (BrokerData bd : route.getBrokerDatas()) &#123; //根据名称获得读队列对应的Broker if (bd.getBrokerName().equals(qd.getBrokerName())) &#123; brokerData = bd; break; &#125; &#125; if (null == brokerData) &#123; continue; &#125; if (!brokerData.getBrokerAddrs().containsKey(MixAll.MASTER_ID)) &#123; continue; &#125; //封装TopicPublishInfo写队列 for (int i = 0; i &lt; qd.getWriteQueueNums(); i++) &#123; MessageQueue mq = new MessageQueue(topic, qd.getBrokerName(), i); info.getMessageQueueList().add(mq); &#125; &#125; &#125; info.setOrderTopic(false); &#125; //返回TopicPublishInfo对象 return info;&#125; 选择队列 默认不启用Broker故障延迟机制 代码：TopicPublishInfo#selectOneMessageQueue(lastBrokerName) 1234567891011121314151617181920212223public MessageQueue selectOneMessageQueue(final String lastBrokerName) &#123; //第一次选择队列 if (lastBrokerName == null) &#123; return selectOneMessageQueue(); &#125; else &#123; //sendWhichQueue int index = this.sendWhichQueue.getAndIncrement(); //遍历消息队列集合 for (int i = 0; i &lt; this.messageQueueList.size(); i++) &#123; //sendWhichQueue自增后取模 int pos = Math.abs(index++) % this.messageQueueList.size(); if (pos &lt; 0) pos = 0; //规避上次Broker队列 MessageQueue mq = this.messageQueueList.get(pos); if (!mq.getBrokerName().equals(lastBrokerName)) &#123; return mq; &#125; &#125; //如果以上情况都不满足,返回sendWhichQueue取模后的队列 return selectOneMessageQueue(); &#125;&#125; 代码：TopicPublishInfo#selectOneMessageQueue() 1234567891011//第一次选择队列public MessageQueue selectOneMessageQueue() &#123; //sendWhichQueue自增 int index = this.sendWhichQueue.getAndIncrement(); //对队列大小取模 int pos = Math.abs(index) % this.messageQueueList.size(); if (pos &lt; 0) pos = 0; //返回对应的队列 return this.messageQueueList.get(pos);&#125; 启用Broker故障延迟机制 12345678910111213141516171819202122232425262728293031323334353637383940414243public MessageQueue selectOneMessageQueue(final TopicPublishInfo tpInfo, final String lastBrokerName) &#123; //Broker故障延迟机制 if (this.sendLatencyFaultEnable) &#123; try &#123; //对sendWhichQueue自增 int index = tpInfo.getSendWhichQueue().getAndIncrement(); //对消息队列轮询获取一个队列 for (int i = 0; i &lt; tpInfo.getMessageQueueList().size(); i++) &#123; int pos = Math.abs(index++) % tpInfo.getMessageQueueList().size(); if (pos &lt; 0) pos = 0; MessageQueue mq = tpInfo.getMessageQueueList().get(pos); //验证该队列是否可用 if (latencyFaultTolerance.isAvailable(mq.getBrokerName())) &#123; //可用 if (null == lastBrokerName || mq.getBrokerName().equals(lastBrokerName)) return mq; &#125; &#125; //从规避的Broker中选择一个可用的Broker final String notBestBroker = latencyFaultTolerance.pickOneAtLeast(); //获得Broker的写队列集合 int writeQueueNums = tpInfo.getQueueIdByBroker(notBestBroker); if (writeQueueNums &gt; 0) &#123; //获得一个队列,指定broker和队列ID并返回 final MessageQueue mq = tpInfo.selectOneMessageQueue(); if (notBestBroker != null) &#123; mq.setBrokerName(notBestBroker); mq.setQueueId(tpInfo.getSendWhichQueue().getAndIncrement() % writeQueueNums); &#125; return mq; &#125; else &#123; latencyFaultTolerance.remove(notBestBroker); &#125; &#125; catch (Exception e) &#123; log.error("Error occurred when selecting message queue", e); &#125; return tpInfo.selectOneMessageQueue(); &#125; return tpInfo.selectOneMessageQueue(lastBrokerName);&#125; 延迟机制接口规范 12345678910public interface LatencyFaultTolerance&lt;T&gt; &#123; //更新失败条目 void updateFaultItem(final T name, final long currentLatency, final long notAvailableDuration); //判断Broker是否可用 boolean isAvailable(final T name); //移除Fault条目 void remove(final T name); //尝试从规避的Broker中选择一个可用的Broker T pickOneAtLeast();&#125; FaultItem：失败条目 12345678class FaultItem implements Comparable&lt;FaultItem&gt; &#123; //条目唯一键,这里为brokerName private final String name; //本次消息发送延迟 private volatile long currentLatency; //故障规避开始时间 private volatile long startTimestamp;&#125; 消息失败策略 123456public class MQFaultStrategy &#123; //根据currentLatency本地消息发送延迟,从latencyMax尾部向前找到第一个比currentLatency小的索引,如果没有找到,返回0 private long[] latencyMax = &#123;50L, 100L, 550L, 1000L, 2000L, 3000L, 15000L&#125;; //根据这个索引从notAvailableDuration取出对应的时间,在该时长内,Broker设置为不可用 private long[] notAvailableDuration = &#123;0L, 0L, 30000L, 60000L, 120000L, 180000L, 600000L&#125;;&#125; 原理分析 代码：DefaultMQProducerImpl#sendDefaultImpl 12345678sendResult = this.sendKernelImpl(msg, mq, communicationMode, sendCallback, topicPublishInfo, timeout - costTime);endTimestamp = System.currentTimeMillis();this.updateFaultItem(mq.getBrokerName(), endTimestamp - beginTimestampPrev, false); 如果上述发送过程出现异常，则调用DefaultMQProducerImpl#updateFaultItem 123456public void updateFaultItem(final String brokerName, final long currentLatency, boolean isolation) &#123; //参数一：broker名称 //参数二:本次消息发送延迟时间 //参数三:是否隔离 this.mqFaultStrategy.updateFaultItem(brokerName, currentLatency, isolation);&#125; 代码：MQFaultStrategy#updateFaultItem 12345678public void updateFaultItem(final String brokerName, final long currentLatency, boolean isolation) &#123; if (this.sendLatencyFaultEnable) &#123; //计算broker规避的时长 long duration = computeNotAvailableDuration(isolation ? 30000 : currentLatency); //更新该FaultItem规避时长 this.latencyFaultTolerance.updateFaultItem(brokerName, currentLatency, duration); &#125;&#125; 代码：MQFaultStrategy#computeNotAvailableDuration 12345678910private long computeNotAvailableDuration(final long currentLatency) &#123; //遍历latencyMax for (int i = latencyMax.length - 1; i &gt;= 0; i--) &#123; //找到第一个比currentLatency的latencyMax值 if (currentLatency &gt;= latencyMax[i]) return this.notAvailableDuration[i]; &#125; //没有找到则返回0 return 0;&#125; 代码：LatencyFaultToleranceImpl#updateFaultItem 1234567891011121314151617181920public void updateFaultItem(final String name, final long currentLatency, final long notAvailableDuration) &#123; //获得原FaultItem FaultItem old = this.faultItemTable.get(name); //为空新建faultItem对象,设置规避时长和开始时间 if (null == old) &#123; final FaultItem faultItem = new FaultItem(name); faultItem.setCurrentLatency(currentLatency); faultItem.setStartTimestamp(System.currentTimeMillis() + notAvailableDuration); old = this.faultItemTable.putIfAbsent(name, faultItem); if (old != null) &#123; old.setCurrentLatency(currentLatency); old.setStartTimestamp(System.currentTimeMillis() + notAvailableDuration); &#125; &#125; else &#123; //更新规避时长和开始时间 old.setCurrentLatency(currentLatency); old.setStartTimestamp(System.currentTimeMillis() + notAvailableDuration); &#125;&#125; 发送消息消息发送API核心入口DefaultMQProducerImpl#sendKernelImpl 12345678private SendResult sendKernelImpl( final Message msg, //待发送消息 final MessageQueue mq, //消息发送队列 final CommunicationMode communicationMode, //消息发送内模式 final SendCallback sendCallback, pp //异步消息回调函数 final TopicPublishInfo topicPublishInfo, //主题路由信息 final long timeout //超时时间 ) 代码：DefaultMQProducerImpl#sendKernelImpl 1234567//获得broker网络地址信息String brokerAddr = this.mQClientFactory.findBrokerAddressInPublish(mq.getBrokerName());if (null == brokerAddr) &#123; //没有找到从NameServer更新broker网络地址信息 tryToFindTopicPublishInfo(mq.getTopic()); brokerAddr = this.mQClientFactory.findBrokerAddressInPublish(mq.getBrokerName());&#125; 12345678910111213141516171819202122//为消息分类唯一IDif (!(msg instanceof MessageBatch)) &#123; MessageClientIDSetter.setUniqID(msg);&#125;boolean topicWithNamespace = false;if (null != this.mQClientFactory.getClientConfig().getNamespace()) &#123; msg.setInstanceId(this.mQClientFactory.getClientConfig().getNamespace()); topicWithNamespace = true;&#125;//消息大小超过4K,启用消息压缩int sysFlag = 0;boolean msgBodyCompressed = false;if (this.tryToCompressMessage(msg)) &#123; sysFlag |= MessageSysFlag.COMPRESSED_FLAG; msgBodyCompressed = true;&#125;//如果是事务消息,设置消息标记MessageSysFlag.TRANSACTION_PREPARED_TYPEfinal String tranMsg = msg.getProperty(MessageConst.PROPERTY_TRANSACTION_PREPARED);if (tranMsg != null &amp;&amp; Boolean.parseBoolean(tranMsg)) &#123; sysFlag |= MessageSysFlag.TRANSACTION_PREPARED_TYPE;&#125; 123456789101112131415161718192021//如果注册了消息发送钩子函数,在执行消息发送前的增强逻辑if (this.hasSendMessageHook()) &#123; context = new SendMessageContext(); context.setProducer(this); context.setProducerGroup(this.defaultMQProducer.getProducerGroup()); context.setCommunicationMode(communicationMode); context.setBornHost(this.defaultMQProducer.getClientIP()); context.setBrokerAddr(brokerAddr); context.setMessage(msg); context.setMq(mq); context.setNamespace(this.defaultMQProducer.getNamespace()); String isTrans = msg.getProperty(MessageConst.PROPERTY_TRANSACTION_PREPARED); if (isTrans != null &amp;&amp; isTrans.equals("true")) &#123; context.setMsgType(MessageType.Trans_Msg_Half); &#125; if (msg.getProperty("__STARTDELIVERTIME") != null || msg.getProperty(MessageConst.PROPERTY_DELAY_TIME_LEVEL) != null) &#123; context.setMsgType(MessageType.Delay_Msg); &#125; this.executeSendMessageHookBefore(context);&#125; 代码：SendMessageHook 1234567public interface SendMessageHook &#123; String hookName(); void sendMessageBefore(final SendMessageContext context); void sendMessageAfter(final SendMessageContext context);&#125; 代码：DefaultMQProducerImpl#sendKernelImpl 1234567891011121314151617181920212223242526272829303132333435363738//构建消息发送请求包SendMessageRequestHeader requestHeader = new SendMessageRequestHeader();//生产者组requestHeader.setProducerGroup(this.defaultMQProducer.getProducerGroup());//主题requestHeader.setTopic(msg.getTopic());//默认创建主题KeyrequestHeader.setDefaultTopic(this.defaultMQProducer.getCreateTopicKey());//该主题在单个Broker默认队列树requestHeader.setDefaultTopicQueueNums(this.defaultMQProducer.getDefaultTopicQueueNums());//队列IDrequestHeader.setQueueId(mq.getQueueId());//消息系统标记requestHeader.setSysFlag(sysFlag);//消息发送时间requestHeader.setBornTimestamp(System.currentTimeMillis());//消息标记requestHeader.setFlag(msg.getFlag());//消息扩展信息requestHeader.setProperties(MessageDecoder.messageProperties2String(msg.getProperties()));//消息重试次数requestHeader.setReconsumeTimes(0);requestHeader.setUnitMode(this.isUnitMode());//是否是批量消息等requestHeader.setBatch(msg instanceof MessageBatch);if (requestHeader.getTopic().startsWith(MixAll.RETRY_GROUP_TOPIC_PREFIX)) &#123; String reconsumeTimes = MessageAccessor.getReconsumeTime(msg); if (reconsumeTimes != null) &#123; requestHeader.setReconsumeTimes(Integer.valueOf(reconsumeTimes)); MessageAccessor.clearProperty(msg, MessageConst.PROPERTY_RECONSUME_TIME); &#125; String maxReconsumeTimes = MessageAccessor.getMaxReconsumeTimes(msg); if (maxReconsumeTimes != null) &#123; requestHeader.setMaxReconsumeTimes(Integer.valueOf(maxReconsumeTimes)); MessageAccessor.clearProperty(msg, MessageConst.PROPERTY_MAX_RECONSUME_TIMES); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859case ASYNC: //异步发送 Message tmpMessage = msg; boolean messageCloned = false; if (msgBodyCompressed) &#123; //If msg body was compressed, msgbody should be reset using prevBody. //Clone new message using commpressed message body and recover origin massage. //Fix bug:https://github.com/apache/rocketmq-externals/issues/66 tmpMessage = MessageAccessor.cloneMessage(msg); messageCloned = true; msg.setBody(prevBody); &#125; if (topicWithNamespace) &#123; if (!messageCloned) &#123; tmpMessage = MessageAccessor.cloneMessage(msg); messageCloned = true; &#125; msg.setTopic(NamespaceUtil.withoutNamespace(msg.getTopic(), this.defaultMQProducer.getNamespace())); &#125; long costTimeAsync = System.currentTimeMillis() - beginStartTime; if (timeout &lt; costTimeAsync) &#123; throw new RemotingTooMuchRequestException("sendKernelImpl call timeout"); &#125; sendResult = this.mQClientFactory.getMQClientAPIImpl().sendMessage( brokerAddr, mq.getBrokerName(), tmpMessage, requestHeader, timeout - costTimeAsync, communicationMode, sendCallback, topicPublishInfo, this.mQClientFactory, this.defaultMQProducer.getRetryTimesWhenSendAsyncFailed(), context, this); break;case ONEWAY:case SYNC: //同步发送 long costTimeSync = System.currentTimeMillis() - beginStartTime; if (timeout &lt; costTimeSync) &#123; throw new RemotingTooMuchRequestException("sendKernelImpl call timeout"); &#125; sendResult = this.mQClientFactory.getMQClientAPIImpl().sendMessage( brokerAddr, mq.getBrokerName(), msg, requestHeader, timeout - costTimeSync, communicationMode, context, this); break; default: assert false; break;&#125; 12345//如果注册了钩子函数,则发送完毕后执行钩子函数if (this.hasSendMessageHook()) &#123; context.setSendResult(sendResult); this.executeSendMessageHookAfter(context);&#125; 批量消息发送 批量消息发送是将同一个主题的多条消息一起打包发送到消息服务端，减少网络调用次数，提高网络传输效率。当然，并不是在同一批次中发送的消息数量越多越好，其判断依据是单条消息的长度，如果单条消息内容比较长，则打包多条消息发送会影响其他线程发送消息的响应时间，并且单批次消息总长度不能超过DefaultMQProducer#maxMessageSize。 批量消息发送要解决的问题是如何将这些消息编码以便服务端能够正确解码出每条消息的消息内容。 代码：DefaultMQProducer#send 12345public SendResult send(Collection&lt;Message&gt; msgs) throws MQClientException, RemotingException, MQBrokerException, InterruptedException &#123; //压缩消息集合成一条消息,然后发送出去 return this.defaultMQProducerImpl.send(batch(msgs));&#125; 代码：DefaultMQProducer#batch 1234567891011121314151617181920private MessageBatch batch(Collection&lt;Message&gt; msgs) throws MQClientException &#123; MessageBatch msgBatch; try &#123; //将集合消息封装到MessageBatch msgBatch = MessageBatch.generateFromList(msgs); //遍历消息集合,检查消息合法性,设置消息ID,设置Topic for (Message message : msgBatch) &#123; Validators.checkMessage(message, this); MessageClientIDSetter.setUniqID(message); message.setTopic(withNamespace(message.getTopic())); &#125; //压缩消息,设置消息body msgBatch.setBody(msgBatch.encode()); &#125; catch (Exception e) &#123; throw new MQClientException("Failed to initiate the MessageBatch", e); &#125; //设置msgBatch的topic msgBatch.setTopic(withNamespace(msgBatch.getTopic())); return msgBatch;&#125; 消息存储消息存储核心类 123456789101112131415161718private final MessageStoreConfig messageStoreConfig; //消息配置属性private final CommitLog commitLog; //CommitLog文件存储的实现类private final ConcurrentMap&lt;String/* topic */, ConcurrentMap&lt;Integer/* queueId */, ConsumeQueue&gt;&gt; consumeQueueTable; //消息队列存储缓存表,按照消息主题分组private final FlushConsumeQueueService flushConsumeQueueService; //消息队列文件刷盘线程private final CleanCommitLogService cleanCommitLogService; //清除CommitLog文件服务private final CleanConsumeQueueService cleanConsumeQueueService; //清除ConsumerQueue队列文件服务private final IndexService indexService; //索引实现类private final AllocateMappedFileService allocateMappedFileService; //MappedFile分配服务private final ReputMessageService reputMessageService;//CommitLog消息分发,根据CommitLog文件构建ConsumerQueue、IndexFile文件private final HAService haService; //存储HA机制private final ScheduleMessageService scheduleMessageService; //消息服务调度线程private final StoreStatsService storeStatsService; //消息存储服务private final TransientStorePool transientStorePool; //消息堆外内存缓存private final BrokerStatsManager brokerStatsManager; //Broker状态管理器private final MessageArrivingListener messageArrivingListener; //消息拉取长轮询模式消息达到监听器private final BrokerConfig brokerConfig; //Broker配置类private StoreCheckpoint storeCheckpoint; //文件刷盘监测点private final LinkedList&lt;CommitLogDispatcher&gt; dispatcherList; //CommitLog文件转发请求 消息存储流程 消息存储入口：DefaultMessageStore#putMessage 12345678910111213141516171819202122232425262728293031323334//判断Broker角色如果是从节点,则无需写入if (BrokerRole.SLAVE == this.messageStoreConfig.getBrokerRole()) &#123; long value = this.printTimes.getAndIncrement(); if ((value % 50000) == 0) &#123; log.warn("message store is slave mode, so putMessage is forbidden "); &#125; return new PutMessageResult(PutMessageStatus.SERVICE_NOT_AVAILABLE, null);&#125;//判断当前写入状态如果是正在写入,则不能继续if (!this.runningFlags.isWriteable()) &#123; long value = this.printTimes.getAndIncrement(); return new PutMessageResult(PutMessageStatus.SERVICE_NOT_AVAILABLE, null);&#125; else &#123; this.printTimes.set(0);&#125;//判断消息主题长度是否超过最大限制if (msg.getTopic().length() &gt; Byte.MAX_VALUE) &#123; log.warn("putMessage message topic length too long " + msg.getTopic().length()); return new PutMessageResult(PutMessageStatus.MESSAGE_ILLEGAL, null);&#125;//判断消息属性长度是否超过限制if (msg.getPropertiesString() != null &amp;&amp; msg.getPropertiesString().length() &gt; Short.MAX_VALUE) &#123; log.warn("putMessage message properties length too long " + msg.getPropertiesString().length()); return new PutMessageResult(PutMessageStatus.PROPERTIES_SIZE_EXCEEDED, null);&#125;//判断系统PageCache缓存去是否占用if (this.isOSPageCacheBusy()) &#123; return new PutMessageResult(PutMessageStatus.OS_PAGECACHE_BUSY, null);&#125;//将消息写入CommitLog文件PutMessageResult result = this.commitLog.putMessage(msg); 代码：CommitLog#putMessage 12345678910111213141516//记录消息存储时间msg.setStoreTimestamp(beginLockTimestamp);//判断如果mappedFile如果为空或者已满,创建新的mappedFile文件if (null == mappedFile || mappedFile.isFull()) &#123; mappedFile = this.mappedFileQueue.getLastMappedFile(0); &#125;//如果创建失败,直接返回if (null == mappedFile) &#123; log.error("create mapped file1 error, topic: " + msg.getTopic() + " clientAddr: " + msg.getBornHostString()); beginTimeInLock = 0; return new PutMessageResult(PutMessageStatus.CREATE_MAPEDFILE_FAILED, null);&#125;//写入消息到mappedFile中result = mappedFile.appendMessage(msg, this.appendMessageCallback); 代码：MappedFile#appendMessagesInner 123456789101112131415161718192021//获得文件的写入指针int currentPos = this.wrotePosition.get();//如果指针大于文件大小则直接返回if (currentPos &lt; this.fileSize) &#123; //通过writeBuffer.slice()创建一个与MappedFile共享的内存区,并设置position为当前指针 ByteBuffer byteBuffer = writeBuffer != null ? writeBuffer.slice() : this.mappedByteBuffer.slice(); byteBuffer.position(currentPos); AppendMessageResult result = null; if (messageExt instanceof MessageExtBrokerInner) &#123; //通过回调方法写入 result = cb.doAppend(this.getFileFromOffset(), byteBuffer, this.fileSize - currentPos, (MessageExtBrokerInner) messageExt); &#125; else if (messageExt instanceof MessageExtBatch) &#123; result = cb.doAppend(this.getFileFromOffset(), byteBuffer, this.fileSize - currentPos, (MessageExtBatch) messageExt); &#125; else &#123; return new AppendMessageResult(AppendMessageStatus.UNKNOWN_ERROR); &#125; this.wrotePosition.addAndGet(result.getWroteBytes()); this.storeTimestamp = result.getStoreTimestamp(); return result;&#125; 代码：CommitLog#doAppend 123456789101112131415161718192021222324252627282930313233343536//文件写入位置long wroteOffset = fileFromOffset + byteBuffer.position();//设置消息IDthis.resetByteBuffer(hostHolder, 8);String msgId = MessageDecoder.createMessageId(this.msgIdMemory, msgInner.getStoreHostBytes(hostHolder), wroteOffset);//获得该消息在消息队列中的偏移量keyBuilder.setLength(0);keyBuilder.append(msgInner.getTopic());keyBuilder.append('-');keyBuilder.append(msgInner.getQueueId());String key = keyBuilder.toString();Long queueOffset = CommitLog.this.topicQueueTable.get(key);if (null == queueOffset) &#123; queueOffset = 0L; CommitLog.this.topicQueueTable.put(key, queueOffset);&#125;//获得消息属性长度final byte[] propertiesData =msgInner.getPropertiesString() == null ? null : msgInner.getPropertiesString().getBytes(MessageDecoder.CHARSET_UTF8);final int propertiesLength = propertiesData == null ? 0 : propertiesData.length;if (propertiesLength &gt; Short.MAX_VALUE) &#123; log.warn("putMessage message properties length too long. length=&#123;&#125;", propertiesData.length); return new AppendMessageResult(AppendMessageStatus.PROPERTIES_SIZE_EXCEEDED);&#125;//获得消息主题大小final byte[] topicData = msgInner.getTopic().getBytes(MessageDecoder.CHARSET_UTF8);final int topicLength = topicData.length;//获得消息体大小final int bodyLength = msgInner.getBody() == null ? 0 : msgInner.getBody().length;//计算消息总长度final int msgLen = calMsgLength(bodyLength, topicLength, propertiesLength); 代码：CommitLog#calMsgLength 123456789101112131415161718192021protected static int calMsgLength(int bodyLength, int topicLength, int propertiesLength) &#123; final int msgLen = 4 //TOTALSIZE + 4 //MAGICCODE + 4 //BODYCRC + 4 //QUEUEID + 4 //FLAG + 8 //QUEUEOFFSET + 8 //PHYSICALOFFSET + 4 //SYSFLAG + 8 //BORNTIMESTAMP + 8 //BORNHOST + 8 //STORETIMESTAMP + 8 //STOREHOSTADDRESS + 4 //RECONSUMETIMES + 8 //Prepared Transaction Offset + 4 + (bodyLength &gt; 0 ? bodyLength : 0) //BODY + 1 + topicLength //TOPIC + 2 + (propertiesLength &gt; 0 ? propertiesLength : 0) //propertiesLength + 0; return msgLen;&#125; 代码：CommitLog#doAppend 12345678910111213141516171819202122232425262728293031323334353637383940414243//消息长度不能超过4Mif (msgLen &gt; this.maxMessageSize) &#123; CommitLog.log.warn("message size exceeded, msg total size: " + msgLen + ", msg body size: " + bodyLength + ", maxMessageSize: " + this.maxMessageSize); return new AppendMessageResult(AppendMessageStatus.MESSAGE_SIZE_EXCEEDED);&#125;//消息是如果没有足够的存储空间则新创建CommitLog文件if ((msgLen + END_FILE_MIN_BLANK_LENGTH) &gt; maxBlank) &#123; this.resetByteBuffer(this.msgStoreItemMemory, maxBlank); // 1 TOTALSIZE this.msgStoreItemMemory.putInt(maxBlank); // 2 MAGICCODE this.msgStoreItemMemory.putInt(CommitLog.BLANK_MAGIC_CODE); // 3 The remaining space may be any value // Here the length of the specially set maxBlank final long beginTimeMills = CommitLog.this.defaultMessageStore.now(); byteBuffer.put(this.msgStoreItemMemory.array(), 0, maxBlank); return new AppendMessageResult(AppendMessageStatus.END_OF_FILE, wroteOffset, maxBlank, msgId, msgInner.getStoreTimestamp(), queueOffset, CommitLog.this.defaultMessageStore.now() - beginTimeMills);&#125;//将消息存储到ByteBuffer中,返回AppendMessageResultfinal long beginTimeMills = CommitLog.this.defaultMessageStore.now();// Write messages to the queue bufferbyteBuffer.put(this.msgStoreItemMemory.array(), 0, msgLen);AppendMessageResult result = new AppendMessageResult(AppendMessageStatus.PUT_OK, wroteOffset, msgLen, msgId,msgInner.getStoreTimestamp(), queueOffset, CommitLog.this.defaultMessageStore.now() -beginTimeMills);switch (tranType) &#123; case MessageSysFlag.TRANSACTION_PREPARED_TYPE: case MessageSysFlag.TRANSACTION_ROLLBACK_TYPE: break; case MessageSysFlag.TRANSACTION_NOT_TYPE: case MessageSysFlag.TRANSACTION_COMMIT_TYPE: //更新消息队列偏移量 CommitLog.this.topicQueueTable.put(key, ++queueOffset); break; default: break;&#125; 代码：CommitLog#putMessage 123456//释放锁putMessageLock.unlock();//刷盘handleDiskFlush(result, putMessageResult, msg);//执行HA主从同步handleHA(result, putMessageResult, msg); 存储文件 commitLog：消息存储目录 config：运行期间一些配置信息 consumerqueue：消息消费队列存储目录 index：消息索引文件存储目录 abort：如果存在改文件寿命Broker非正常关闭 checkpoint：文件检查点，存储CommitLog文件最后一次刷盘时间戳、consumerquueue最后一次刷盘时间，index索引文件最后一次刷盘时间戳。 存储文件内存映射RocketMQ通过使用内存映射文件提高IO访问性能，无论是CommitLog、ConsumerQueue还是IndexFile，单个文件都被设计为固定长度，如果一个文件写满以后再创建一个新文件，文件名就为该文件第一条消息对应的全局物理偏移量。 MappedFileQueue 123456String storePath; //存储目录int mappedFileSize; // 单个文件大小CopyOnWriteArrayList&lt;MappedFile&gt; mappedFiles; //MappedFile文件集合AllocateMappedFileService allocateMappedFileService; //创建MapFile服务类long flushedWhere = 0; //当前刷盘指针long committedWhere = 0; //当前数据提交指针,内存中ByteBuffer当前的写指针,该值大于等于flushWhere 根据存储时间查询MappedFile 12345678910111213141516public MappedFile getMappedFileByTime(final long timestamp) &#123; Object[] mfs = this.copyMappedFiles(0); if (null == mfs) return null; //遍历MappedFile文件数组 for (int i = 0; i &lt; mfs.length; i++) &#123; MappedFile mappedFile = (MappedFile) mfs[i]; //MappedFile文件的最后修改时间大于指定时间戳则返回该文件 if (mappedFile.getLastModifiedTimestamp() &gt;= timestamp) &#123; return mappedFile; &#125; &#125; return (MappedFile) mfs[mfs.length - 1];&#125; 根据消息偏移量offset查找MappedFile 1234567891011121314151617181920212223242526272829303132333435363738394041424344public MappedFile findMappedFileByOffset(final long offset, final boolean returnFirstOnNotFound) &#123; try &#123; //获得第一个MappedFile文件 MappedFile firstMappedFile = this.getFirstMappedFile(); //获得最后一个MappedFile文件 MappedFile lastMappedFile = this.getLastMappedFile(); //第一个文件和最后一个文件均不为空,则进行处理 if (firstMappedFile != null &amp;&amp; lastMappedFile != null) &#123; if (offset &lt; firstMappedFile.getFileFromOffset() || offset &gt;= lastMappedFile.getFileFromOffset() + this.mappedFileSize) &#123; &#125; else &#123; //获得文件索引 int index = (int) ((offset / this.mappedFileSize) - (firstMappedFile.getFileFromOffset() / this.mappedFileSize)); MappedFile targetFile = null; try &#123; //根据索引返回目标文件 targetFile = this.mappedFiles.get(index); &#125; catch (Exception ignored) &#123; &#125; if (targetFile != null &amp;&amp; offset &gt;= targetFile.getFileFromOffset() &amp;&amp; offset &lt; targetFile.getFileFromOffset() + this.mappedFileSize) &#123; return targetFile; &#125; for (MappedFile tmpMappedFile : this.mappedFiles) &#123; if (offset &gt;= tmpMappedFile.getFileFromOffset() &amp;&amp; offset &lt; tmpMappedFile.getFileFromOffset() + this.mappedFileSize) &#123; return tmpMappedFile; &#125; &#125; &#125; if (returnFirstOnNotFound) &#123; return firstMappedFile; &#125; &#125; &#125; catch (Exception e) &#123; log.error("findMappedFileByOffset Exception", e); &#125; return null;&#125; 获取存储文件最小偏移量 12345678910111213public long getMinOffset() &#123; if (!this.mappedFiles.isEmpty()) &#123; try &#123; return this.mappedFiles.get(0).getFileFromOffset(); &#125; catch (IndexOutOfBoundsException e) &#123; //continue; &#125; catch (Exception e) &#123; log.error("getMinOffset has exception.", e); &#125; &#125; return -1;&#125; 获取存储文件最大偏移量 1234567public long getMaxOffset() &#123; MappedFile mappedFile = getLastMappedFile(); if (mappedFile != null) &#123; return mappedFile.getFileFromOffset() + mappedFile.getReadPosition(); &#125; return 0;&#125; 返回存储文件当前写指针 1234567public long getMaxWrotePosition() &#123; MappedFile mappedFile = getLastMappedFile(); if (mappedFile != null) &#123; return mappedFile.getFileFromOffset() + mappedFile.getWrotePosition(); &#125; return 0;&#125; MappedFile 12345678910111213141516int OS_PAGE_SIZE = 1024 * 4; //操作系统每页大小,默认4KAtomicLong TOTAL_MAPPED_VIRTUAL_MEMORY = new AtomicLong(0); //当前JVM实例中MappedFile虚拟内存AtomicInteger TOTAL_MAPPED_FILES = new AtomicInteger(0); //当前JVM实例中MappedFile对象个数AtomicInteger wrotePosition = new AtomicInteger(0); //当前文件的写指针AtomicInteger committedPosition = new AtomicInteger(0); //当前文件的提交指针AtomicInteger flushedPosition = new AtomicInteger(0); //刷写到磁盘指针int fileSize; //文件大小FileChannel fileChannel; //文件通道 ByteBuffer writeBuffer = null; //堆外内存ByteBufferTransientStorePool transientStorePool = null; //堆外内存池String fileName; //文件名称long fileFromOffset; //该文件的处理偏移量File file; //物理文件MappedByteBuffer mappedByteBuffer; //物理文件对应的内存映射Buffervolatile long storeTimestamp = 0; //文件最后一次内容写入时间boolean firstCreateInQueue = false; //是否是MappedFileQueue队列中第一个文件 MappedFile初始化 未开启transientStorePoolEnable。transientStorePoolEnable=true为true表示数据先存储到堆外内存，然后通过Commit线程将数据提交到内存映射Buffer中，再通过Flush线程将内存映射Buffer中数据持久化磁盘。 123456789101112131415161718192021222324252627private void init(final String fileName, final int fileSize) throws IOException &#123; this.fileName = fileName; this.fileSize = fileSize; this.file = new File(fileName); this.fileFromOffset = Long.parseLong(this.file.getName()); boolean ok = false; ensureDirOK(this.file.getParent()); try &#123; this.fileChannel = new RandomAccessFile(this.file, "rw").getChannel(); this.mappedByteBuffer = this.fileChannel.map(MapMode.READ_WRITE, 0, fileSize); TOTAL_MAPPED_VIRTUAL_MEMORY.addAndGet(fileSize); TOTAL_MAPPED_FILES.incrementAndGet(); ok = true; &#125; catch (FileNotFoundException e) &#123; log.error("create file channel " + this.fileName + " Failed. ", e); throw e; &#125; catch (IOException e) &#123; log.error("map file " + this.fileName + " Failed. ", e); throw e; &#125; finally &#123; if (!ok &amp;&amp; this.fileChannel != null) &#123; this.fileChannel.close(); &#125; &#125;&#125; 开启transientStorePoolEnable 123456public void init(final String fileName, final int fileSize, final TransientStorePool transientStorePool) throws IOException &#123; init(fileName, fileSize); this.writeBuffer = transientStorePool.borrowBuffer(); //初始化writeBuffer this.transientStorePool = transientStorePool;&#125; MappedFile提交 提交数据到FileChannel，commitLeastPages为本次提交最小的页数，如果待提交数据不满commitLeastPages，则不执行本次提交操作。如果writeBuffer如果为空，直接返回writePosition指针，无需执行commit操作，表名commit操作主体是writeBuffer。 1234567891011121314151617181920212223public int commit(final int commitLeastPages) &#123; if (writeBuffer == null) &#123; //no need to commit data to file channel, so just regard wrotePosition as committedPosition. return this.wrotePosition.get(); &#125; //判断是否满足提交条件 if (this.isAbleToCommit(commitLeastPages)) &#123; if (this.hold()) &#123; commit0(commitLeastPages); this.release(); &#125; else &#123; log.warn("in commit, hold failed, commit offset = " + this.committedPosition.get()); &#125; &#125; // 所有数据提交后,清空缓冲区 if (writeBuffer != null &amp;&amp; this.transientStorePool != null &amp;&amp; this.fileSize == this.committedPosition.get()) &#123; this.transientStorePool.returnBuffer(writeBuffer); this.writeBuffer = null; &#125; return this.committedPosition.get();&#125; MappedFile#isAbleToCommit 判断是否执行commit操作，如果文件已满返回true；如果commitLeastpages大于0，则比较writePosition与上一次提交的指针commitPosition的差值，除以OS_PAGE_SIZE得到当前脏页的数量，如果大于commitLeastPages则返回true，如果commitLeastpages小于0表示只要存在脏页就提交。 1234567891011121314151617protected boolean isAbleToCommit(final int commitLeastPages) &#123; //已经刷盘指针 int flush = this.committedPosition.get(); //文件写指针 int write = this.wrotePosition.get(); //写满刷盘 if (this.isFull()) &#123; return true; &#125; if (commitLeastPages &gt; 0) &#123; //文件内容达到commitLeastPages页数,则刷盘 return ((write / OS_PAGE_SIZE) - (flush / OS_PAGE_SIZE)) &gt;= commitLeastPages; &#125; return write &gt; flush;&#125; MappedFile#commit0 具体提交的实现，首先创建WriteBuffer区共享缓存区，然后将新创建的position回退到上一次提交的位置（commitPosition），设置limit为wrotePosition（当前最大有效数据指针），然后把commitPosition到wrotePosition的数据写入到FileChannel中，然后更新committedPosition指针为wrotePosition。commit的作用就是将MappedFile的writeBuffer中数据提交到文件通道FileChannel中。 12345678910111213141516171819202122232425protected void commit0(final int commitLeastPages) &#123; //写指针 int writePos = this.wrotePosition.get(); //上次提交指针 int lastCommittedPosition = this.committedPosition.get(); if (writePos - this.committedPosition.get() &gt; 0) &#123; try &#123; //复制共享内存区域 ByteBuffer byteBuffer = writeBuffer.slice(); //设置提交位置是上次提交位置 byteBuffer.position(lastCommittedPosition); //最大提交数量 byteBuffer.limit(writePos); //设置fileChannel位置为上次提交位置 this.fileChannel.position(lastCommittedPosition); //将lastCommittedPosition到writePos的数据复制到FileChannel中 this.fileChannel.write(byteBuffer); //重置提交位置 this.committedPosition.set(writePos); &#125; catch (Throwable e) &#123; log.error("Error occurred when commit data to FileChannel.", e); &#125; &#125;&#125; MappedFile#flush 刷写磁盘，直接调用MappedByteBuffer或fileChannel的force方法将内存中的数据持久化到磁盘，那么flushedPosition应该等于MappedByteBuffer中的写指针；如果writeBuffer不为空，则flushPosition应该等于上一次的commit指针；因为上一次提交的数据就是进入到MappedByteBuffer中的数据；如果writeBuffer为空，数据时直接进入到MappedByteBuffer，wrotePosition代表的是MappedByteBuffer中的指针，故设置flushPosition为wrotePosition。 12345678910111213141516171819202122232425262728public int flush(final int flushLeastPages) &#123; //数据达到刷盘条件 if (this.isAbleToFlush(flushLeastPages)) &#123; //加锁，同步刷盘 if (this.hold()) &#123; //获得读指针 int value = getReadPosition(); try &#123; //数据从writeBuffer提交数据到fileChannel再刷新到磁盘 if (writeBuffer != null || this.fileChannel.position() != 0) &#123; this.fileChannel.force(false); &#125; else &#123; //从mmap刷新数据到磁盘 this.mappedByteBuffer.force(); &#125; &#125; catch (Throwable e) &#123; log.error("Error occurred when force data to disk.", e); &#125; //更新刷盘位置 this.flushedPosition.set(value); this.release(); &#125; else &#123; log.warn("in flush, hold failed, flush offset = " + this.flushedPosition.get()); this.flushedPosition.set(getReadPosition()); &#125; &#125; return this.getFlushedPosition();&#125; MappedFile#getReadPosition 获取当前文件最大可读指针。如果writeBuffer为空，则直接返回当前的写指针；如果writeBuffer不为空，则返回上一次提交的指针。在MappedFile设置中,只有提交了的数据（写入到MappedByteBuffer或FileChannel中的数据）才是安全的数据 1234public int getReadPosition() &#123; //如果writeBuffer为空,刷盘的位置就是应该等于上次commit的位置,如果为空则为mmap的写指针 return this.writeBuffer == null ? this.wrotePosition.get() : this.committedPosition.get();&#125; MappedFile#selectMappedBuffer 查找pos到当前最大可读之间的数据，由于在整个写入期间都未曾改MappedByteBuffer的指针，如果mappedByteBuffer.slice()方法返回的共享缓存区空间为整个MappedFile，然后通过设置ByteBuffer的position为待查找的值，读取字节长度当前可读最大长度，最终返回的ByteBuffer的limit为size。整个共享缓存区的容量为（MappedFile#fileSize-pos）。故在操作SelectMappedBufferResult不能对包含在里面的ByteBuffer调用filp方法。 123456789101112131415161718192021public SelectMappedBufferResult selectMappedBuffer(int pos) &#123; //获得最大可读指针 int readPosition = getReadPosition(); //pos小于最大可读指针,并且大于0 if (pos &lt; readPosition &amp;&amp; pos &gt;= 0) &#123; if (this.hold()) &#123; //复制mappedByteBuffer读共享区 ByteBuffer byteBuffer = this.mappedByteBuffer.slice(); //设置读指针位置 byteBuffer.position(pos); //获得可读范围 int size = readPosition - pos; //设置最大刻度范围 ByteBuffer byteBufferNew = byteBuffer.slice(); byteBufferNew.limit(size); return new SelectMappedBufferResult(this.fileFromOffset + pos, byteBufferNew, size, this); &#125; &#125; return null;&#125; MappedFile#shutdown MappedFile文件销毁的实现方法为public boolean destory(long intervalForcibly)，intervalForcibly表示拒绝被销毁的最大存活时间。 123456789101112131415public void shutdown(final long intervalForcibly) &#123; if (this.available) &#123; //关闭MapedFile this.available = false; //设置当前关闭时间戳 this.firstShutdownTimestamp = System.currentTimeMillis(); //释放资源 this.release(); &#125; else if (this.getRefCount() &gt; 0) &#123; if ((System.currentTimeMillis() - this.firstShutdownTimestamp) &gt;= intervalForcibly) &#123; this.refCount.set(-1000 - this.getRefCount()); this.release(); &#125; &#125;&#125; TransientStorePool短暂的存储池。RocketMQ单独创建一个MappedByteBuffer内存缓存池，用来临时存储数据，数据先写入该内存映射中，然后由commit线程定时将数据从该内存复制到与目标物理文件对应的内存映射中。RocketMQ引入该机制主要的原因是提供一种内存锁定，将当前堆外内存一直锁定在内存中，避免被进程将内存交换到磁盘。 123private final int poolSize; //availableBuffers个数private final int fileSize; //每隔ByteBuffer大小private final Deque&lt;ByteBuffer&gt; availableBuffers; //ByteBuffer容器。双端队列 初始化 123456789101112public void init() &#123; //创建poolSize个堆外内存 for (int i = 0; i &lt; poolSize; i++) &#123; ByteBuffer byteBuffer = ByteBuffer.allocateDirect(fileSize); final long address = ((DirectBuffer) byteBuffer).address(); Pointer pointer = new Pointer(address); //使用com.sun.jna.Library类库将该批内存锁定,避免被置换到交换区,提高存储性能 LibC.INSTANCE.mlock(pointer, new NativeLong(fileSize)); availableBuffers.offer(byteBuffer); &#125;&#125; 实时更新消息消费队列与索引文件消息消费队文件、消息属性索引文件都是基于CommitLog文件构建的，当消息生产者提交的消息存储在CommitLog文件中，ConsumerQueue、IndexFile需要及时更新，否则消息无法及时被消费，根据消息属性查找消息也会出现较大延迟。RocketMQ通过开启一个线程ReputMessageService来准实时转发CommitLog文件更新事件，相应的任务处理器根据转发的消息及时更新ConsumerQueue、IndexFile文件。 代码：DefaultMessageStore：start 1234//设置CommitLog内存中最大偏移量this.reputMessageService.setReputFromOffset(maxPhysicalPosInLogicQueue);//启动this.reputMessageService.start(); 代码：DefaultMessageStore：run 1234567891011121314public void run() &#123; DefaultMessageStore.log.info(this.getServiceName() + " service started"); //每隔1毫秒就继续尝试推送消息到消息消费队列和索引文件 while (!this.isStopped()) &#123; try &#123; Thread.sleep(1); this.doReput(); &#125; catch (Exception e) &#123; DefaultMessageStore.log.warn(this.getServiceName() + " service has exception. ", e); &#125; &#125; DefaultMessageStore.log.info(this.getServiceName() + " service end");&#125; 代码：DefaultMessageStore：deReput 1234567891011//从result中循环遍历消息,一次读一条,创建DispatherRequest对象。for (int readSize = 0; readSize &lt; result.getSize() &amp;&amp; doNext; ) &#123; DispatchRequest dispatchRequest = DefaultMessageStore.this.commitLog.checkMessageAndReturnSize(result.getByteBuffer(), false, false); int size = dispatchRequest.getBufferSize() == -1 ? dispatchRequest.getMsgSize() : dispatchRequest.getBufferSize(); if (dispatchRequest.isSuccess()) &#123; if (size &gt; 0) &#123; DefaultMessageStore.this.doDispatch(dispatchRequest); &#125; &#125;&#125; DispatchRequest 1234567891011121314String topic; //消息主题名称int queueId; //消息队列IDlong commitLogOffset; //消息物理偏移量int msgSize; //消息长度long tagsCode; //消息过滤tag hashCodelong storeTimestamp; //消息存储时间戳long consumeQueueOffset; //消息队列偏移量String keys; //消息索引keyboolean success; //是否成功解析到完整的消息String uniqKey; //消息唯一键int sysFlag; //消息系统标记long preparedTransactionOffset; //消息预处理事务偏移量Map&lt;String, String&gt; propertiesMap; //消息属性byte[] bitMap; //位图 转发到ConsumerQueue 12345678910111213141516class CommitLogDispatcherBuildConsumeQueue implements CommitLogDispatcher &#123; @Override public void dispatch(DispatchRequest request) &#123; final int tranType = MessageSysFlag.getTransactionValue(request.getSysFlag()); switch (tranType) &#123; case MessageSysFlag.TRANSACTION_NOT_TYPE: case MessageSysFlag.TRANSACTION_COMMIT_TYPE: //消息分发 DefaultMessageStore.this.putMessagePositionInfo(request); break; case MessageSysFlag.TRANSACTION_PREPARED_TYPE: case MessageSysFlag.TRANSACTION_ROLLBACK_TYPE: break; &#125; &#125;&#125; 代码：DefaultMessageStore#putMessagePositionInfo 123456public void putMessagePositionInfo(DispatchRequest dispatchRequest) &#123; //获得消费队列 ConsumeQueue cq = this.findConsumeQueue(dispatchRequest.getTopic(), dispatchRequest.getQueueId()); //消费队列分发消息 cq.putMessagePositionInfoWrapper(dispatchRequest);&#125; 代码：DefaultMessageStore#putMessagePositionInfo 123456789101112//依次将消息偏移量、消息长度、tag写入到ByteBuffer中this.byteBufferIndex.flip();this.byteBufferIndex.limit(CQ_STORE_UNIT_SIZE);this.byteBufferIndex.putLong(offset);this.byteBufferIndex.putInt(size);this.byteBufferIndex.putLong(tagsCode);//获得内存映射文件MappedFile mappedFile = this.mappedFileQueue.getLastMappedFile(expectLogicOffset);if (mappedFile != null) &#123; //将消息追加到内存映射文件,异步输盘 return mappedFile.appendMessage(this.byteBufferIndex.array());&#125; 转发到Index 123456789class CommitLogDispatcherBuildIndex implements CommitLogDispatcher &#123; @Override public void dispatch(DispatchRequest request) &#123; if (DefaultMessageStore.this.messageStoreConfig.isMessageIndexEnable()) &#123; DefaultMessageStore.this.indexService.buildIndex(request); &#125; &#125;&#125; 代码：DefaultMessageStore#buildIndex 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public void buildIndex(DispatchRequest req) &#123; //获得索引文件 IndexFile indexFile = retryGetAndCreateIndexFile(); if (indexFile != null) &#123; //获得文件最大物理偏移量 long endPhyOffset = indexFile.getEndPhyOffset(); DispatchRequest msg = req; String topic = msg.getTopic(); String keys = msg.getKeys(); //如果该消息的物理偏移量小于索引文件中的最大物理偏移量,则说明是重复数据,忽略本次索引构建 if (msg.getCommitLogOffset() &lt; endPhyOffset) &#123; return; &#125; final int tranType = MessageSysFlag.getTransactionValue(msg.getSysFlag()); switch (tranType) &#123; case MessageSysFlag.TRANSACTION_NOT_TYPE: case MessageSysFlag.TRANSACTION_PREPARED_TYPE: case MessageSysFlag.TRANSACTION_COMMIT_TYPE: break; case MessageSysFlag.TRANSACTION_ROLLBACK_TYPE: return; &#125; //如果消息ID不为空,则添加到Hash索引中 if (req.getUniqKey() != null) &#123; indexFile = putKey(indexFile, msg, buildKey(topic, req.getUniqKey())); if (indexFile == null) &#123; return; &#125; &#125; //构建索引key,RocketMQ支持为同一个消息建立多个索引,多个索引键空格隔开. if (keys != null &amp;&amp; keys.length() &gt; 0) &#123; String[] keyset = keys.split(MessageConst.KEY_SEPARATOR); for (int i = 0; i &lt; keyset.length; i++) &#123; String key = keyset[i]; if (key.length() &gt; 0) &#123; indexFile = putKey(indexFile, msg, buildKey(topic, key)); if (indexFile == null) &#123; return; &#125; &#125; &#125; &#125; &#125; else &#123; log.error("build index error, stop building index"); &#125;&#125; 消息队列和索引文件恢复由于RocketMQ存储首先将消息全量存储在CommitLog文件中，然后异步生成转发任务更新ConsumerQueue和Index文件。如果消息成功存储到CommitLog文件中，转发任务未成功执行，此时消息服务器Broker由于某个愿意宕机，导致CommitLog、ConsumerQueue、IndexFile文件数据不一致。如果不加以人工修复的话，会有一部分消息即便在CommitLog中文件中存在，但由于没有转发到ConsumerQueue，这部分消息将永远复发被消费者消费。 存储文件加载代码：DefaultMessageStore#load 判断上一次是否异常退出。实现机制是Broker在启动时创建abort文件，在退出时通过JVM钩子函数删除abort文件。如果下次启动时存在abort文件。说明Broker时异常退出的，CommitLog与ConsumerQueue数据有可能不一致，需要进行修复。 123456789//判断临时文件是否存在boolean lastExitOK = !this.isTempFileExist();//根据临时文件判断当前Broker是否异常退出private boolean isTempFileExist() &#123; String fileName = StorePathConfigHelper .getAbortFile(this.messageStoreConfig.getStorePathRootDir()); File file = new File(fileName); return file.exists();&#125; 代码：DefaultMessageStore#load 12345678910111213141516171819//加载延时队列if (null != scheduleMessageService) &#123; result = result &amp;&amp; this.scheduleMessageService.load();&#125;// 加载CommitLog文件result = result &amp;&amp; this.commitLog.load();// 加载消费队列文件result = result &amp;&amp; this.loadConsumeQueue();if (result) &#123; //加载存储监测点,监测点主要记录CommitLog文件、ConsumerQueue文件、Index索引文件的刷盘点 this.storeCheckpoint =new StoreCheckpoint(StorePathConfigHelper.getStoreCheckpoint(this.messageStoreConfig.getStorePathRootDir())); //加载index文件 this.indexService.load(lastExitOK); //根据Broker是否异常退出,执行不同的恢复策略 this.recover(lastExitOK);&#125; 代码：MappedFileQueue#load 加载CommitLog到映射文件 1234567891011121314151617181920212223242526272829303132//指向CommitLog文件目录File dir = new File(this.storePath);//获得文件数组File[] files = dir.listFiles();if (files != null) &#123; // 文件排序 Arrays.sort(files); //遍历文件 for (File file : files) &#123; //如果文件大小和配置文件不一致,退出 if (file.length() != this.mappedFileSize) &#123; return false; &#125; try &#123; //创建映射文件 MappedFile mappedFile = new MappedFile(file.getPath(), mappedFileSize); mappedFile.setWrotePosition(this.mappedFileSize); mappedFile.setFlushedPosition(this.mappedFileSize); mappedFile.setCommittedPosition(this.mappedFileSize); //将映射文件添加到队列 this.mappedFiles.add(mappedFile); log.info("load " + file.getPath() + " OK"); &#125; catch (IOException e) &#123; log.error("load file " + file + " error", e); return false; &#125; &#125;&#125;return true; 代码：DefaultMessageStore#loadConsumeQueue 加载消息消费队列 12345678910111213141516171819202122232425262728293031323334353637383940//执行消费队列目录File dirLogic = new File(StorePathConfigHelper.getStorePathConsumeQueue(this.messageStoreConfig.getStorePathRootDir()));//遍历消费队列目录File[] fileTopicList = dirLogic.listFiles();if (fileTopicList != null) &#123; for (File fileTopic : fileTopicList) &#123; //获得子目录名称,即topic名称 String topic = fileTopic.getName(); //遍历子目录下的消费队列文件 File[] fileQueueIdList = fileTopic.listFiles(); if (fileQueueIdList != null) &#123; //遍历文件 for (File fileQueueId : fileQueueIdList) &#123; //文件名称即队列ID int queueId; try &#123; queueId = Integer.parseInt(fileQueueId.getName()); &#125; catch (NumberFormatException e) &#123; continue; &#125; //创建消费队列并加载到内存 ConsumeQueue logic = new ConsumeQueue( topic, queueId, StorePathConfigHelper.getStorePathConsumeQueue(this.messageStoreConfig.getStorePathRootDir()), this.getMessageStoreConfig().getMapedFileSizeConsumeQueue(), this); this.putConsumeQueue(topic, queueId, logic); if (!logic.load()) &#123; return false; &#125; &#125; &#125; &#125;&#125;log.info("load logics queue all over, OK");return true; 代码：IndexService#load 加载索引文件 12345678910111213141516171819202122232425262728293031323334353637public boolean load(final boolean lastExitOK) &#123; //索引文件目录 File dir = new File(this.storePath); //遍历索引文件 File[] files = dir.listFiles(); if (files != null) &#123; //文件排序 Arrays.sort(files); //遍历文件 for (File file : files) &#123; try &#123; //加载索引文件 IndexFile f = new IndexFile(file.getPath(), this.hashSlotNum, this.indexNum, 0, 0); f.load(); if (!lastExitOK) &#123; //索引文件上次的刷盘时间小于该索引文件的消息时间戳,该文件将立即删除 if (f.getEndTimestamp() &gt; this.defaultMessageStore.getStoreCheckpoint() .getIndexMsgTimestamp()) &#123; f.destroy(0); continue; &#125; &#125; //将索引文件添加到队列 log.info("load index file OK, " + f.getFileName()); this.indexFileList.add(f); &#125; catch (IOException e) &#123; log.error("load file &#123;&#125; error", file, e); return false; &#125; catch (NumberFormatException e) &#123; log.error("load file &#123;&#125; error", file, e); &#125; &#125; &#125; return true;&#125; 代码：DefaultMessageStore#recover 文件恢复，根据Broker是否正常退出执行不同的恢复策略 1234567891011121314private void recover(final boolean lastExitOK) &#123; //获得最大的物理便宜消费队列 long maxPhyOffsetOfConsumeQueue = this.recoverConsumeQueue(); if (lastExitOK) &#123; //正常恢复 this.commitLog.recoverNormally(maxPhyOffsetOfConsumeQueue); &#125; else &#123; //异常恢复 this.commitLog.recoverAbnormally(maxPhyOffsetOfConsumeQueue); &#125; //在CommitLog中保存每个消息消费队列当前的存储逻辑偏移量 this.recoverTopicQueueTable();&#125; 代码：DefaultMessageStore#recoverTopicQueueTable 恢复ConsumerQueue后，将在CommitLog实例中保存每隔消息队列当前的存储逻辑偏移量，这也是消息中不仅存储主题、消息队列ID、还存储了消息队列的关键所在。 1234567891011121314public void recoverTopicQueueTable() &#123; HashMap&lt;String/* topic-queueid */, Long/* offset */&gt; table = new HashMap&lt;String, Long&gt;(1024); //CommitLog最小偏移量 long minPhyOffset = this.commitLog.getMinOffset(); //遍历消费队列,将消费队列保存在CommitLog中 for (ConcurrentMap&lt;Integer, ConsumeQueue&gt; maps : this.consumeQueueTable.values()) &#123; for (ConsumeQueue logic : maps.values()) &#123; String key = logic.getTopic() + "-" + logic.getQueueId(); table.put(key, logic.getMaxOffsetInQueue()); logic.correctMinOffset(minPhyOffset); &#125; &#125; this.commitLog.setTopicQueueTable(table);&#125; 正常恢复代码：CommitLog#recoverNormally 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public void recoverNormally(long maxPhyOffsetOfConsumeQueue) &#123; final List&lt;MappedFile&gt; mappedFiles = this.mappedFileQueue.getMappedFiles(); if (!mappedFiles.isEmpty()) &#123; //Broker正常停止再重启时,从倒数第三个开始恢复,如果不足3个文件,则从第一个文件开始恢复。 int index = mappedFiles.size() - 3; if (index &lt; 0) index = 0; MappedFile mappedFile = mappedFiles.get(index); ByteBuffer byteBuffer = mappedFile.sliceByteBuffer(); long processOffset = mappedFile.getFileFromOffset(); //代表当前已校验通过的offset long mappedFileOffset = 0; while (true) &#123; //查找消息 DispatchRequest dispatchRequest = this.checkMessageAndReturnSize(byteBuffer, checkCRCOnRecover); //消息长度 int size = dispatchRequest.getMsgSize(); //查找结果为true,并且消息长度大于0,表示消息正确.mappedFileOffset向前移动本消息长度 if (dispatchRequest.isSuccess() &amp;&amp; size &gt; 0) &#123; mappedFileOffset += size; &#125; //如果查找结果为true且消息长度等于0,表示已到该文件末尾,如果还有下一个文件,则重置processOffset和MappedFileOffset重复查找下一个文件,否则跳出循环。 else if (dispatchRequest.isSuccess() &amp;&amp; size == 0) &#123; index++; if (index &gt;= mappedFiles.size()) &#123; // Current branch can not happen break; &#125; else &#123; //取出每个文件 mappedFile = mappedFiles.get(index); byteBuffer = mappedFile.sliceByteBuffer(); processOffset = mappedFile.getFileFromOffset(); mappedFileOffset = 0; &#125; &#125; // 查找结果为false，表明该文件未填满所有消息，跳出循环，结束循环 else if (!dispatchRequest.isSuccess()) &#123; log.info("recover physics file end, " + mappedFile.getFileName()); break; &#125; &#125; //更新MappedFileQueue的flushedWhere和committedWhere指针 processOffset += mappedFileOffset; this.mappedFileQueue.setFlushedWhere(processOffset); this.mappedFileQueue.setCommittedWhere(processOffset); //删除offset之后的所有文件 this.mappedFileQueue.truncateDirtyFiles(processOffset); if (maxPhyOffsetOfConsumeQueue &gt;= processOffset) &#123; this.defaultMessageStore.truncateDirtyLogicFiles(processOffset); &#125; &#125; else &#123; this.mappedFileQueue.setFlushedWhere(0); this.mappedFileQueue.setCommittedWhere(0); this.defaultMessageStore.destroyLogics(); &#125;&#125; 代码：MappedFileQueue#truncateDirtyFiles 123456789101112131415161718192021222324public void truncateDirtyFiles(long offset) &#123; List&lt;MappedFile&gt; willRemoveFiles = new ArrayList&lt;MappedFile&gt;(); //遍历目录下文件 for (MappedFile file : this.mappedFiles) &#123; //文件尾部的偏移量 long fileTailOffset = file.getFileFromOffset() + this.mappedFileSize; //文件尾部的偏移量大于offset if (fileTailOffset &gt; offset) &#123; //offset大于文件的起始偏移量 if (offset &gt;= file.getFileFromOffset()) &#123; //更新wrotePosition、committedPosition、flushedPosistion file.setWrotePosition((int) (offset % this.mappedFileSize)); file.setCommittedPosition((int) (offset % this.mappedFileSize)); file.setFlushedPosition((int) (offset % this.mappedFileSize)); &#125; else &#123; //offset小于文件的起始偏移量,说明该文件是有效文件后面创建的,释放mappedFile占用内存,删除文件 file.destroy(1000); willRemoveFiles.add(file); &#125; &#125; &#125; this.deleteExpiredFile(willRemoveFiles);&#125; 异常恢复Broker异常停止文件恢复的实现为CommitLog#recoverAbnormally。异常文件恢复步骤与正常停止文件恢复流程基本相同，其主要差别有两个。首先，正常停止默认从倒数第三个文件开始进行恢复，而异常停止则需要从最后一个文件往前走，找到第一个消息存储正常的文件。其次，如果CommitLog目录没有消息文件，如果消息消费队列目录下存在文件，则需要销毁。 代码：CommitLog#recoverAbnormally 12345678910111213141516171819202122232425if (!mappedFiles.isEmpty()) &#123; // Looking beginning to recover from which file int index = mappedFiles.size() - 1; MappedFile mappedFile = null; for (; index &gt;= 0; index--) &#123; mappedFile = mappedFiles.get(index); //判断消息文件是否是一个正确的文件 if (this.isMappedFileMatchedRecover(mappedFile)) &#123; log.info("recover from this mapped file " + mappedFile.getFileName()); break; &#125; &#125; //根据索引取出mappedFile文件 if (index &lt; 0) &#123; index = 0; mappedFile = mappedFiles.get(index); &#125; //...验证消息的合法性,并将消息转发到消息消费队列和索引文件 &#125;else&#123; //未找到mappedFile,重置flushWhere、committedWhere都为0，销毁消息队列文件 this.mappedFileQueue.setFlushedWhere(0); this.mappedFileQueue.setCommittedWhere(0); this.defaultMessageStore.destroyLogics();&#125; 刷盘机制RocketMQ的存储是基于JDK NIO的内存映射机制（MappedByteBuffer）的，消息存储首先将消息追加到内存，再根据配置的刷盘策略在不同时间进行刷写磁盘。 同步刷盘消息追加到内存后，立即将数据刷写到磁盘文件 代码：CommitLog#handleDiskFlush 123456789101112//刷盘服务final GroupCommitService service = (GroupCommitService) this.flushCommitLogService;if (messageExt.isWaitStoreMsgOK()) &#123; //封装刷盘请求 GroupCommitRequest request = new GroupCommitRequest(result.getWroteOffset() + result.getWroteBytes()); //提交刷盘请求 service.putRequest(request); //线程阻塞5秒，等待刷盘结束 boolean flushOK = request.waitForFlush(this.defaultMessageStore.getMessageStoreConfig().getSyncFlushTimeout()); if (!flushOK) &#123; putMessageResult.setPutMessageStatus(PutMessageStatus.FLUSH_DISK_TIMEOUT); &#125; GroupCommitRequest 123long nextOffset; //刷盘点偏移量CountDownLatch countDownLatch = new CountDownLatch(1); //倒计树锁存器volatile boolean flushOK = false; //刷盘结果;默认为false 代码：GroupCommitService#run 123456789101112131415public void run() &#123; CommitLog.log.info(this.getServiceName() + " service started"); while (!this.isStopped()) &#123; try &#123; //线程等待10ms this.waitForRunning(10); //执行提交 this.doCommit(); &#125; catch (Exception e) &#123; CommitLog.log.warn(this.getServiceName() + " service has exception. ", e); &#125; &#125; ...&#125; 代码：GroupCommitService#doCommit 123456789101112131415161718192021222324252627282930313233private void doCommit() &#123; //加锁 synchronized (this.requestsRead) &#123; if (!this.requestsRead.isEmpty()) &#123; //遍历requestsRead for (GroupCommitRequest req : this.requestsRead) &#123; // There may be a message in the next file, so a maximum of // two times the flush boolean flushOK = false; for (int i = 0; i &lt; 2 &amp;&amp; !flushOK; i++) &#123; flushOK = CommitLog.this.mappedFileQueue.getFlushedWhere() &gt;= req.getNextOffset(); //刷盘 if (!flushOK) &#123; CommitLog.this.mappedFileQueue.flush(0); &#125; &#125; //唤醒发送消息客户端 req.wakeupCustomer(flushOK); &#125; //更新刷盘监测点 long storeTimestamp = CommitLog.this.mappedFileQueue.getStoreTimestamp(); if (storeTimestamp &gt; 0) &#123; CommitLog.this.defaultMessageStore.getStoreCheckpoint().setPhysicMsgTimestamp(storeTimestamp); &#125; this.requestsRead.clear(); &#125; else &#123; // Because of individual messages is set to not sync flush, it // will come to this process CommitLog.this.mappedFileQueue.flush(0); &#125; &#125;&#125; 异步刷盘在消息追加到内存后，立即返回给消息发送端。如果开启transientStorePoolEnable，RocketMQ会单独申请一个与目标物理文件（commitLog）同样大小的堆外内存，该堆外内存将使用内存锁定，确保不会被置换到虚拟内存中去，消息首先追加到堆外内存，然后提交到物理文件的内存映射中，然后刷写到磁盘。如果未开启transientStorePoolEnable，消息直接追加到物理文件直接映射文件中，然后刷写到磁盘中。 开启transientStorePoolEnable后异步刷盘步骤: 将消息直接追加到ByteBuffer（堆外内存） CommitRealTimeService线程每隔200ms将ByteBuffer新追加内容提交到MappedByteBuffer中 MappedByteBuffer在内存中追加提交的内容，wrotePosition指针向后移动 commit操作成功返回，将committedPosition位置恢复 FlushRealTimeService线程默认每500ms将MappedByteBuffer中新追加的内存刷写到磁盘 代码：CommitLog$CommitRealTimeService#run 提交线程工作机制 12345678910111213141516171819202122232425262728293031//间隔时间,默认200msint interval = CommitLog.this.defaultMessageStore.getMessageStoreConfig().getCommitIntervalCommitLog();//一次提交的至少页数int commitDataLeastPages = CommitLog.this.defaultMessageStore.getMessageStoreConfig().getCommitCommitLogLeastPages();//两次真实提交的最大间隔,默认200msint commitDataThoroughInterval =CommitLog.this.defaultMessageStore.getMessageStoreConfig().getCommitCommitLogThoroughInterval();//上次提交间隔超过commitDataThoroughInterval,则忽略提交commitDataThoroughInterval参数,直接提交long begin = System.currentTimeMillis();if (begin &gt;= (this.lastCommitTimestamp + commitDataThoroughInterval)) &#123; this.lastCommitTimestamp = begin; commitDataLeastPages = 0;&#125;//执行提交操作,将待提交数据提交到物理文件的内存映射区boolean result = CommitLog.this.mappedFileQueue.commit(commitDataLeastPages);long end = System.currentTimeMillis();if (!result) &#123; this.lastCommitTimestamp = end; // result = false means some data committed. //now wake up flush thread. //唤醒刷盘线程 flushCommitLogService.wakeup();&#125;if (end - begin &gt; 500) &#123; log.info("Commit data to file costs &#123;&#125; ms", end - begin);&#125;this.waitForRunning(interval); 代码：CommitLog$FlushRealTimeService#run 刷盘线程工作机制 1234567891011121314151617181920212223242526272829303132//表示await方法等待,默认falseboolean flushCommitLogTimed = CommitLog.this.defaultMessageStore.getMessageStoreConfig().isFlushCommitLogTimed();//线程执行时间间隔int interval = CommitLog.this.defaultMessageStore.getMessageStoreConfig().getFlushIntervalCommitLog();//一次刷写任务至少包含页数int flushPhysicQueueLeastPages = CommitLog.this.defaultMessageStore.getMessageStoreConfig().getFlushCommitLogLeastPages();//两次真实刷写任务最大间隔int flushPhysicQueueThoroughInterval =CommitLog.this.defaultMessageStore.getMessageStoreConfig().getFlushCommitLogThoroughInterval();...//距离上次提交间隔超过flushPhysicQueueThoroughInterval,则本次刷盘任务将忽略flushPhysicQueueLeastPages,直接提交long currentTimeMillis = System.currentTimeMillis();if (currentTimeMillis &gt;= (this.lastFlushTimestamp + flushPhysicQueueThoroughInterval)) &#123; this.lastFlushTimestamp = currentTimeMillis; flushPhysicQueueLeastPages = 0; printFlushProgress = (printTimes++ % 10) == 0;&#125;...//执行一次刷盘前,先等待指定时间间隔if (flushCommitLogTimed) &#123; Thread.sleep(interval);&#125; else &#123; this.waitForRunning(interval);&#125;...long begin = System.currentTimeMillis();//刷写磁盘CommitLog.this.mappedFileQueue.flush(flushPhysicQueueLeastPages);long storeTimestamp = CommitLog.this.mappedFileQueue.getStoreTimestamp();if (storeTimestamp &gt; 0) &#123;//更新存储监测点文件的时间戳CommitLog.this.defaultMessageStore.getStoreCheckpoint().setPhysicMsgTimestamp(storeTimestamp); 过期文件删除机制由于RocketMQ操作CommitLog、ConsumerQueue文件是基于内存映射机制并在启动的时候回加载CommitLog、ConsumerQueue目录下的所有文件，为了避免内存与磁盘的浪费，不可能将消息永久存储在消息服务器上，所以要引入一种机制来删除已过期的文件。RocketMQ顺序写CommitLog、ConsumerQueue文件，所有写操作全部落在最后一个CommitLog或者ConsumerQueue文件上，之前的文件在下一个文件创建后将不会再被更新。RocketMQ清除过期文件的方法时：如果当前文件在在一定时间间隔内没有再次被消费，则认为是过期文件，可以被删除，RocketMQ不会关注这个文件上的消息是否全部被消费。默认每个文件的过期时间为72小时，通过在Broker配置文件中设置fileReservedTime来改变过期时间，单位为小时。 代码：DefaultMessageStore#addScheduleTask 12345678910private void addScheduleTask() &#123; //每隔10s调度一次清除文件 this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; DefaultMessageStore.this.cleanFilesPeriodically(); &#125; &#125;, 1000 * 60, this.messageStoreConfig.getCleanResourceInterval(), TimeUnit.MILLISECONDS); ...&#125; 代码：DefaultMessageStore#cleanFilesPeriodically 123456private void cleanFilesPeriodically() &#123; //清除存储文件 this.cleanCommitLogService.run(); //清除消息消费队列文件 this.cleanConsumeQueueService.run();&#125; 代码：DefaultMessageStore#deleteExpiredFiles 123456789101112131415161718private void deleteExpiredFiles() &#123; //删除的数量 int deleteCount = 0; //文件保留的时间 long fileReservedTime = DefaultMessageStore.this.getMessageStoreConfig().getFileReservedTime(); //删除物理文件的间隔 int deletePhysicFilesInterval = DefaultMessageStore.this.getMessageStoreConfig().getDeleteCommitLogFilesInterval(); //线程被占用,第一次拒绝删除后能保留的最大时间,超过该时间,文件将被强制删除 int destroyMapedFileIntervalForcibly = DefaultMessageStore.this.getMessageStoreConfig().getDestroyMapedFileIntervalForcibly();boolean timeup = this.isTimeToDelete();boolean spacefull = this.isSpaceToDelete();boolean manualDelete = this.manualDeleteFileSeveralTimes &gt; 0;if (timeup || spacefull || manualDelete) &#123; ...执行删除逻辑&#125;else&#123; ...无作为&#125; 删除文件操作的条件 指定删除文件的时间点，RocketMQ通过deleteWhen设置一天的固定时间执行一次删除过期文件操作，默认4点 磁盘空间如果不充足，删除过期文件 预留，手工触发。 代码：CleanCommitLogService#isSpaceToDelete 当磁盘空间不足时执行删除过期文件 1234567891011121314151617181920212223242526272829303132private boolean isSpaceToDelete() &#123; //磁盘分区的最大使用量 double ratio = DefaultMessageStore.this.getMessageStoreConfig().getDiskMaxUsedSpaceRatio() / 100.0; //是否需要立即执行删除过期文件操作 cleanImmediately = false; &#123; String storePathPhysic = DefaultMessageStore.this.getMessageStoreConfig().getStorePathCommitLog(); //当前CommitLog目录所在的磁盘分区的磁盘使用率 double physicRatio = UtilAll.getDiskPartitionSpaceUsedPercent(storePathPhysic); //diskSpaceWarningLevelRatio:磁盘使用率警告阈值,默认0.90 if (physicRatio &gt; diskSpaceWarningLevelRatio) &#123; boolean diskok = DefaultMessageStore.this.runningFlags.getAndMakeDiskFull(); if (diskok) &#123; DefaultMessageStore.log.error("physic disk maybe full soon " + physicRatio + ", so mark disk full"); &#125; //diskSpaceCleanForciblyRatio:强制清除阈值,默认0.85 cleanImmediately = true; &#125; else if (physicRatio &gt; diskSpaceCleanForciblyRatio) &#123; cleanImmediately = true; &#125; else &#123; boolean diskok = DefaultMessageStore.this.runningFlags.getAndMakeDiskOK(); if (!diskok) &#123; DefaultMessageStore.log.info("physic disk space OK " + physicRatio + ", so mark disk ok"); &#125; &#125; if (physicRatio &lt; 0 || physicRatio &gt; ratio) &#123; DefaultMessageStore.log.info("physic disk maybe full soon, so reclaim space, " + physicRatio); return true; &#125;&#125; 代码：MappedFileQueue#deleteExpiredFileByTime 执行文件销毁和删除 1234567891011121314151617181920212223242526272829for (int i = 0; i &lt; mfsLength; i++) &#123; //遍历每隔文件 MappedFile mappedFile = (MappedFile) mfs[i]; //计算文件存活时间 long liveMaxTimestamp = mappedFile.getLastModifiedTimestamp() + expiredTime; //如果超过72小时,执行文件删除 if (System.currentTimeMillis() &gt;= liveMaxTimestamp || cleanImmediately) &#123; if (mappedFile.destroy(intervalForcibly)) &#123; files.add(mappedFile); deleteCount++; if (files.size() &gt;= DELETE_FILES_BATCH_MAX) &#123; break; &#125; if (deleteFilesInterval &gt; 0 &amp;&amp; (i + 1) &lt; mfsLength) &#123; try &#123; Thread.sleep(deleteFilesInterval); &#125; catch (InterruptedException e) &#123; &#125; &#125; &#125; else &#123; break; &#125; &#125; else &#123; //avoid deleting files in the middle break; &#125;&#125; 小结RocketMQ的存储文件包括消息文件（Commitlog）、消息消费队列文件（ConsumerQueue）、Hash索引文件（IndexFile）、监测点文件（checkPoint）、abort（关闭异常文件）。单个消息存储文件、消息消费队列文件、Hash索引文件长度固定以便使用内存映射机制进行文件的读写操作。RocketMQ组织文件以文件的起始偏移量来命令文件，这样根据偏移量能快速定位到真实的物理文件。RocketMQ基于内存映射文件机制提供了同步刷盘和异步刷盘两种机制，异步刷盘是指在消息存储时先追加到内存映射文件，然后启动专门的刷盘线程定时将内存中的文件数据刷写到磁盘。 CommitLog，消息存储文件，RocketMQ为了保证消息发送的高吞吐量，采用单一文件存储所有主题消息，保证消息存储是完全的顺序写，但这样给文件读取带来了不便，为此RocketMQ为了方便消息消费构建了消息消费队列文件，基于主题与队列进行组织，同时RocketMQ为消息实现了Hash索引，可以为消息设置索引键，根据所以能够快速从CommitLog文件中检索消息。 当消息达到CommitLog后，会通过ReputMessageService线程接近实时地将消息转发给消息消费队列文件与索引文件。为了安全起见，RocketMQ引入abort文件，记录Broker的停机是否是正常关闭还是异常关闭，在重启Broker时为了保证CommitLog文件，消息消费队列文件与Hash索引文件的正确性，分别采用不同策略来恢复文件。 RocketMQ不会永久存储消息文件、消息消费队列文件，而是启动文件过期机制并在磁盘空间不足或者默认凌晨4点删除过期文件，文件保存72小时并且在删除文件时并不会判断该消息文件上的消息是否被消费。 Consumer消息消费概述消息消费以组的模式开展，一个消费组内可以包含多个消费者，每一个消费者组可订阅多个主题，消费组之间有ff式和广播模式两种消费模式。集群模式，主题下的同一条消息只允许被其中一个消费者消费。广播模式，主题下的同一条消息，将被集群内的所有消费者消费一次。消息服务器与消费者之间的消息传递也有两种模式：推模式、拉模式。所谓的拉模式，是消费端主动拉起拉消息请求，而推模式是消息达到消息服务器后，推送给消息消费者。RocketMQ消息推模式的实现基于拉模式，在拉模式上包装一层，一个拉取任务完成后开始下一个拉取任务。 集群模式下，多个消费者如何对消息队列进行负载呢？消息队列负载机制遵循一个通用思想：一个消息队列同一个时间只允许被一个消费者消费，一个消费者可以消费多个消息队列。 RocketMQ支持局部顺序消息消费，也就是保证同一个消息队列上的消息顺序消费。不支持消息全局顺序消费，如果要实现某一个主题的全局顺序消费，可以将该主题的队列数设置为1，牺牲高可用性。 消息消费初探消息推送模式 消息消费重要方法 12345678void sendMessageBack(final MessageExt msg, final int delayLevel, final String brokerName)：发送消息确认Set&lt;MessageQueue&gt; fetchSubscribeMessageQueues(final String topic) :获取消费者对主题分配了那些消息队列void registerMessageListener(final MessageListenerConcurrently messageListener)：注册并发事件监听器void registerMessageListener(final MessageListenerOrderly messageListener)：注册顺序消息事件监听器void subscribe(final String topic, final String subExpression)：基于主题订阅消息，消息过滤使用表达式void subscribe(final String topic, final String fullClassName,final String filterClassSource)：基于主题订阅消息，消息过滤使用类模式void subscribe(final String topic, final MessageSelector selector) ：订阅消息，并指定队列选择器void unsubscribe(final String topic)：取消消息订阅 DefaultMQPushConsumer 12345678910111213141516171819202122232425262728293031323334//消费者组private String consumerGroup; //消息消费模式private MessageModel messageModel = MessageModel.CLUSTERING; //指定消费开始偏移量（最大偏移量、最小偏移量、启动时间戳）开始消费private ConsumeFromWhere consumeFromWhere = ConsumeFromWhere.CONSUME_FROM_LAST_OFFSET;//集群模式下的消息队列负载策略private AllocateMessageQueueStrategy allocateMessageQueueStrategy;//订阅信息private Map&lt;String /* topic */, String /* sub expression */&gt; subscription = new HashMap&lt;String, String&gt;();//消息业务监听器private MessageListener messageListener;//消息消费进度存储器private OffsetStore offsetStore;//消费者最小线程数量private int consumeThreadMin = 20;//消费者最大线程数量private int consumeThreadMax = 20;//并发消息消费时处理队列最大跨度private int consumeConcurrentlyMaxSpan = 2000;//每1000次流控后打印流控日志private int pullThresholdForQueue = 1000;//推模式下任务间隔时间private long pullInterval = 0;//推模式下任务拉取的条数,默认32条private int pullBatchSize = 32;//每次传入MessageListener#consumerMessage中消息的数量private int consumeMessageBatchMaxSize = 1;//是否每次拉取消息都订阅消息private boolean postSubscriptionWhenPull = false;//消息重试次数,-1代表16次private int maxReconsumeTimes = -1;//消息消费超时时间private long consumeTimeout = 15; 消费者启动流程 代码：DefaultMQPushConsumerImpl#start 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public synchronized void start() throws MQClientException &#123; switch (this.serviceState) &#123; case CREATE_JUST: this.defaultMQPushConsumer.getMessageModel(), this.defaultMQPushConsumer.isUnitMode()); this.serviceState = ServiceState.START_FAILED; //检查消息者是否合法 this.checkConfig(); //构建主题订阅信息 this.copySubscription(); //设置消费者客户端实例名称为进程ID if (this.defaultMQPushConsumer.getMessageModel() == MessageModel.CLUSTERING) &#123; this.defaultMQPushConsumer.changeInstanceNameToPID(); &#125; //创建MQClient实例 this.mQClientFactory = MQClientManager.getInstance().getAndCreateMQClientInstance(this.defaultMQPushConsumer, this.rpcHook); //构建rebalanceImpl this.rebalanceImpl.setConsumerGroup(this.defaultMQPushConsumer.getConsumerGroup()); this.rebalanceImpl.setMessageModel(this.defaultMQPushConsumer.getMessageModel()); this.rebalanceImpl.setAllocateMessageQueueStrategy(this.defaultMQPushConsumer.getAllocateMessageQueueStrategy()); this.rebalanceImpl.setmQClientFactory(this.mQClientFactor this.pullAPIWrapper = new PullAPIWrapper( mQClientFactory, this.defaultMQPushConsumer.getConsumerGroup(), isUnitMode()); this.pullAPIWrapper.registerFilterMessageHook(filterMessageHookLis if (this.defaultMQPushConsumer.getOffsetStore() != null) &#123; this.offsetStore = this.defaultMQPushConsumer.getOffsetStore(); &#125; else &#123; switch (this.defaultMQPushConsumer.getMessageModel()) &#123; case BROADCASTING: //消息消费广播模式,将消费进度保存在本地 this.offsetStore = new LocalFileOffsetStore(this.mQClientFactory, this.defaultMQPushConsumer.getConsumerGroup()); break; case CLUSTERING: //消息消费集群模式,将消费进度保存在远端Broker this.offsetStore = new RemoteBrokerOffsetStore(this.mQClientFactory, this.defaultMQPushConsumer.getConsumerGroup()); break; default: break; &#125; this.defaultMQPushConsumer.setOffsetStore(this.offsetStore); &#125; this.offsetStore.load //创建顺序消息消费服务 if (this.getMessageListenerInner() instanceof MessageListenerOrderly) &#123; this.consumeOrderly = true; this.consumeMessageService = new ConsumeMessageOrderlyService(this, (MessageListenerOrderly) this.getMessageListenerInner()); //创建并发消息消费服务 &#125; else if (this.getMessageListenerInner() instanceof MessageListenerConcurrently) &#123; this.consumeOrderly = false; this.consumeMessageService = new ConsumeMessageConcurrentlyService(this, (MessageListenerConcurrently) this.getMessageListenerInner()); &#125; //消息消费服务启动 this.consumeMessageService.start(); //注册消费者实例 boolean registerOK = mQClientFactory.registerConsumer(this.defaultMQPushConsumer.getConsumerGroup(), this); if (!registerOK) &#123; this.serviceState = ServiceState.CREATE_JUST; this.consumeMessageService.shutdown(); throw new MQClientException("The consumer group[" + this.defaultMQPushConsumer.getConsumerGroup() + "] has been created before, specify another name please." + FAQUrl.suggestTodo(FAQUrl.GROUP_NAME_DUPLICATE_URL), null); //启动消费者客户端 mQClientFactory.start(); log.info("the consumer [&#123;&#125;] start OK.", this.defaultMQPushConsumer.getConsumerGroup()); this.serviceState = ServiceState.RUNNING; break; case RUNNING: case START_FAILED: case SHUTDOWN_ALREADY: throw new MQClientException("The PushConsumer service state not OK, maybe started once, " + this.serviceState + FAQUrl.suggestTodo(FAQUrl.CLIENT_SERVICE_NOT_OK), null); default: break; &#125; this.updateTopicSubscribeInfoWhenSubscriptionChanged(); this.mQClientFactory.checkClientInBroker(); this.mQClientFactory.sendHeartbeatToAllBrokerWithLock(); this.mQClientFactory.rebalanceImmediately();&#125; 消息拉取消息消费模式有两种模式：广播模式与集群模式。广播模式比较简单，每一个消费者需要拉取订阅主题下所有队列的消息。本文重点讲解集群模式。在集群模式下，同一个消费者组内有多个消息消费者，同一个主题存在多个消费队列，消费者通过负载均衡的方式消费消息。 消息队列负载均衡，通常的作法是一个消息队列在同一个时间只允许被一个消费消费者消费，一个消息消费者可以同时消费多个消息队列。 PullMessageService实现机制从MQClientInstance的启动流程中可以看出，RocketMQ使用一个单独的线程PullMessageService来负责消息的拉取。 代码：PullMessageService#run 1234567891011121314151617public void run() &#123; log.info(this.getServiceName() + " service started"); //循环拉取消息 while (!this.isStopped()) &#123; try &#123; //从请求队列中获取拉取消息请求 PullRequest pullRequest = this.pullRequestQueue.take(); //拉取消息 this.pullMessage(pullRequest); &#125; catch (InterruptedException ignored) &#123; &#125; catch (Exception e) &#123; log.error("Pull Message Service Run Method exception", e); &#125; &#125; log.info(this.getServiceName() + " service end");&#125; PullRequest 12345private String consumerGroup; //消费者组private MessageQueue messageQueue; //待拉取消息队列private ProcessQueue processQueue; //消息处理队列private long nextOffset; //待拉取的MessageQueue偏移量private boolean lockedFirst = false; //是否被锁定 代码：PullMessageService#pullMessage 123456789101112private void pullMessage(final PullRequest pullRequest) &#123; //获得消费者实例 final MQConsumerInner consumer = this.mQClientFactory.selectConsumer(pullRequest.getConsumerGroup()); if (consumer != null) &#123; //强转为推送模式消费者 DefaultMQPushConsumerImpl impl = (DefaultMQPushConsumerImpl) consumer; //推送消息 impl.pullMessage(pullRequest); &#125; else &#123; log.warn("No matched consumer for the PullRequest &#123;&#125;, drop it", pullRequest); &#125;&#125; ProcessQueue实现机制ProcessQueue是MessageQueue在消费端的重现、快照。PullMessageService从消息服务器默认每次拉取32条消息，按照消息的队列偏移量顺序存放在ProcessQueue中，PullMessageService然后将消息提交到消费者消费线程池，消息成功消费后从ProcessQueue中移除。 属性 1234567891011121314//消息容器private final TreeMap&lt;Long, MessageExt&gt; msgTreeMap = new TreeMap&lt;Long, MessageExt&gt;();//读写锁private final ReadWriteLock lockTreeMap = new ReentrantReadWriteLock();//ProcessQueue总消息树private final AtomicLong msgCount = new AtomicLong();//ProcessQueue队列最大偏移量private volatile long queueOffsetMax = 0L;//当前ProcessQueue是否被丢弃private volatile boolean dropped = false;//上一次拉取时间戳private volatile long lastPullTimestamp = System.currentTimeMillis();//上一次消费时间戳private volatile long lastConsumeTimestamp = System.currentTimeMillis(); 方法 12345678910111213141516//移除消费超时消息public void cleanExpiredMsg(DefaultMQPushConsumer pushConsumer)//添加消息public boolean putMessage(final List&lt;MessageExt&gt; msgs)//获取消息最大间隔public long getMaxSpan()//移除消息public long removeMessage(final List&lt;MessageExt&gt; msgs)//将consumingMsgOrderlyTreeMap中消息重新放在msgTreeMap,并清空consumingMsgOrderlyTreeMap public void rollback() //将consumingMsgOrderlyTreeMap消息清除,表示成功处理该批消息public long commit()//重新处理该批消息public void makeMessageToCosumeAgain(List&lt;MessageExt&gt; msgs) //从processQueue中取出batchSize条消息public List&lt;MessageExt&gt; takeMessags(final int batchSize) 消息拉取基本流程客户端发起拉取请求 代码：DefaultMQPushConsumerImpl#pullMessage 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public void pullMessage(final PullRequest pullRequest) &#123; //从pullRequest获得ProcessQueue final ProcessQueue processQueue = pullRequest.getProcessQueue(); //如果处理队列被丢弃,直接返回 if (processQueue.isDropped()) &#123; log.info("the pull request[&#123;&#125;] is dropped.", pullRequest.toString()); return; &#125; //如果处理队列未被丢弃,更新时间戳 pullRequest.getProcessQueue().setLastPullTimestamp(System.currentTimeMillis()); try &#123; this.makeSureStateOK(); &#125; catch (MQClientException e) &#123; log.warn("pullMessage exception, consumer state not ok", e); this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_EXCEPTION); return; &#125; //如果处理队列被挂起,延迟1s后再执行 if (this.isPause()) &#123; log.warn("consumer was paused, execute pull request later. instanceName=&#123;&#125;, group=&#123;&#125;", this.defaultMQPushConsumer.getInstanceName(), this.defaultMQPushConsumer.getConsumerGroup()); this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_SUSPEND); return; &#125; //获得最大待处理消息数量 long cachedMessageCount = processQueue.getMsgCount().get(); //获得最大待处理消息大小 long cachedMessageSizeInMiB = processQueue.getMsgSize().get() / (1024 * 1024); //从数量进行流控 if (cachedMessageCount &gt; this.defaultMQPushConsumer.getPullThresholdForQueue()) &#123; this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_FLOW_CONTROL); if ((queueFlowControlTimes++ % 1000) == 0) &#123; log.warn( "the cached message count exceeds the threshold &#123;&#125;, so do flow control, minOffset=&#123;&#125;, maxOffset=&#123;&#125;, count=&#123;&#125;, size=&#123;&#125; MiB, pullRequest=&#123;&#125;, flowControlTimes=&#123;&#125;", this.defaultMQPushConsumer.getPullThresholdForQueue(), processQueue.getMsgTreeMap().firstKey(), processQueue.getMsgTreeMap().lastKey(), cachedMessageCount, cachedMessageSizeInMiB, pullRequest, queueFlowControlTimes); &#125; return; &#125; //从消息大小进行流控 if (cachedMessageSizeInMiB &gt; this.defaultMQPushConsumer.getPullThresholdSizeForQueue()) &#123; this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_FLOW_CONTROL); if ((queueFlowControlTimes++ % 1000) == 0) &#123; log.warn( "the cached message size exceeds the threshold &#123;&#125; MiB, so do flow control, minOffset=&#123;&#125;, maxOffset=&#123;&#125;, count=&#123;&#125;, size=&#123;&#125; MiB, pullRequest=&#123;&#125;, flowControlTimes=&#123;&#125;", this.defaultMQPushConsumer.getPullThresholdSizeForQueue(), processQueue.getMsgTreeMap().firstKey(), processQueue.getMsgTreeMap().lastKey(), cachedMessageCount, cachedMessageSizeInMiB, pullRequest, queueFlowControlTimes); &#125; return; &#125; //获得订阅信息 final SubscriptionData subscriptionData = this.rebalanceImpl.getSubscriptionInner().get(pullRequest.getMessageQueue().getTopic()); if (null == subscriptionData) &#123; this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_EXCEPTION); log.warn("find the consumer's subscription failed, &#123;&#125;", pullRequest); return; //与服务端交互,获取消息 this.pullAPIWrapper.pullKernelImpl( pullRequest.getMessageQueue(), subExpression, subscriptionData.getExpressionType(), subscriptionData.getSubVersion(), pullRequest.getNextOffset(), this.defaultMQPushConsumer.getPullBatchSize(), sysFlag, commitOffsetValue, BROKER_SUSPEND_MAX_TIME_MILLIS, CONSUMER_TIMEOUT_MILLIS_WHEN_SUSPEND, CommunicationMode.ASYNC, pullCallback ); &#125; 消息服务端Broker组装消息 代码：PullMessageProcessor#processRequest 12345678910111213141516171819//构建消息过滤器MessageFilter messageFilter;if (this.brokerController.getBrokerConfig().isFilterSupportRetry()) &#123; messageFilter = new ExpressionForRetryMessageFilter(subscriptionData, consumerFilterData, this.brokerController.getConsumerFilterManager());&#125; else &#123; messageFilter = new ExpressionMessageFilter(subscriptionData, consumerFilterData, this.brokerController.getConsumerFilterManager());&#125;//调用MessageStore.getMessage查找消息final GetMessageResult getMessageResult = this.brokerController.getMessageStore().getMessage( requestHeader.getConsumerGroup(), //消费组名称 requestHeader.getTopic(), //主题名称 requestHeader.getQueueId(), //队列ID requestHeader.getQueueOffset(), //待拉取偏移量 requestHeader.getMaxMsgNums(), //最大拉取消息条数 messageFilter //消息过滤器 ); 代码：DefaultMessageStore#getMessage 123456789101112131415161718192021222324252627282930313233GetMessageStatus status = GetMessageStatus.NO_MESSAGE_IN_QUEUE;long nextBeginOffset = offset; //查找下一次队列偏移量long minOffset = 0; //当前消息队列最小偏移量long maxOffset = 0; //当前消息队列最大偏移量GetMessageResult getResult = new GetMessageResult();final long maxOffsetPy = this.commitLog.getMaxOffset(); //当前commitLog最大偏移量//根据主题名称和队列编号获取消息消费队列ConsumeQueue consumeQueue = findConsumeQueue(topic, queueId);...minOffset = consumeQueue.getMinOffsetInQueue();maxOffset = consumeQueue.getMaxOffsetInQueue();//消息偏移量异常情况校对下一次拉取偏移量if (maxOffset == 0) &#123; //表示当前消息队列中没有消息 status = GetMessageStatus.NO_MESSAGE_IN_QUEUE; nextBeginOffset = nextOffsetCorrection(offset, 0);&#125; else if (offset &lt; minOffset) &#123; //待拉取消息的偏移量小于队列的其实偏移量 status = GetMessageStatus.OFFSET_TOO_SMALL; nextBeginOffset = nextOffsetCorrection(offset, minOffset);&#125; else if (offset == maxOffset) &#123; //待拉取偏移量为队列最大偏移量 status = GetMessageStatus.OFFSET_OVERFLOW_ONE; nextBeginOffset = nextOffsetCorrection(offset, offset);&#125; else if (offset &gt; maxOffset) &#123; //偏移量越界 status = GetMessageStatus.OFFSET_OVERFLOW_BADLY; if (0 == minOffset) &#123; nextBeginOffset = nextOffsetCorrection(offset, minOffset); &#125; else &#123; nextBeginOffset = nextOffsetCorrection(offset, maxOffset); &#125;&#125;...//根据偏移量从CommitLog中拉取32条消息SelectMappedBufferResult selectResult = this.commitLog.getMessage(offsetPy, sizePy); 代码：PullMessageProcessor#processRequest 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677//根据拉取结果填充responseHeaderresponse.setRemark(getMessageResult.getStatus().name());responseHeader.setNextBeginOffset(getMessageResult.getNextBeginOffset());responseHeader.setMinOffset(getMessageResult.getMinOffset());responseHeader.setMaxOffset(getMessageResult.getMaxOffset());//判断如果存在主从同步慢,设置下一次拉取任务的ID为主节点switch (this.brokerController.getMessageStoreConfig().getBrokerRole()) &#123; case ASYNC_MASTER: case SYNC_MASTER: break; case SLAVE: if (!this.brokerController.getBrokerConfig().isSlaveReadEnable()) &#123; response.setCode(ResponseCode.PULL_RETRY_IMMEDIATELY); responseHeader.setSuggestWhichBrokerId(MixAll.MASTER_ID); &#125; break;&#125;...//GetMessageResult与Response的Code转换switch (getMessageResult.getStatus()) &#123; case FOUND: //成功 response.setCode(ResponseCode.SUCCESS); break; case MESSAGE_WAS_REMOVING: //消息存放在下一个commitLog中 response.setCode(ResponseCode.PULL_RETRY_IMMEDIATELY); //消息重试 break; case NO_MATCHED_LOGIC_QUEUE: //未找到队列 case NO_MESSAGE_IN_QUEUE: //队列中未包含消息 if (0 != requestHeader.getQueueOffset()) &#123; response.setCode(ResponseCode.PULL_OFFSET_MOVED); requestHeader.getQueueOffset(), getMessageResult.getNextBeginOffset(), requestHeader.getTopic(), requestHeader.getQueueId(), requestHeader.getConsumerGroup() ); &#125; else &#123; response.setCode(ResponseCode.PULL_NOT_FOUND); &#125; break; case NO_MATCHED_MESSAGE: //未找到消息 response.setCode(ResponseCode.PULL_RETRY_IMMEDIATELY); break; case OFFSET_FOUND_NULL: //消息物理偏移量为空 response.setCode(ResponseCode.PULL_NOT_FOUND); break; case OFFSET_OVERFLOW_BADLY: //offset越界 response.setCode(ResponseCode.PULL_OFFSET_MOVED); // XXX: warn and notify me log.info("the request offset: &#123;&#125; over flow badly, broker max offset: &#123;&#125;, consumer: &#123;&#125;", requestHeader.getQueueOffset(), getMessageResult.getMaxOffset(), channel.remoteAddress()); break; case OFFSET_OVERFLOW_ONE: //offset在队列中未找到 response.setCode(ResponseCode.PULL_NOT_FOUND); break; case OFFSET_TOO_SMALL: //offset未在队列中 response.setCode(ResponseCode.PULL_OFFSET_MOVED); requestHeader.getConsumerGroup(), requestHeader.getTopic(), requestHeader.getQueueOffset(), getMessageResult.getMinOffset(), channel.remoteAddress()); break; default: assert false; break;&#125;...//如果CommitLog标记可用,并且当前Broker为主节点,则更新消息消费进度boolean storeOffsetEnable = brokerAllowSuspend;storeOffsetEnable = storeOffsetEnable &amp;&amp; hasCommitOffsetFlag;storeOffsetEnable = storeOffsetEnable &amp;&amp; this.brokerController.getMessageStoreConfig().getBrokerRole() != BrokerRole.SLAVE;if (storeOffsetEnable) &#123; this.brokerController.getConsumerOffsetManager().commitOffset(RemotingHelper.parseChannelRemoteAddr(channel), requestHeader.getConsumerGroup(), requestHeader.getTopic(), requestHeader.getQueueId(), requestHeader.getCommitOffset());&#125; 消息拉取客户端处理消息 代码：MQClientAPIImpl#processPullResponse 12345678910111213141516171819202122232425262728private PullResult processPullResponse( final RemotingCommand response) throws MQBrokerException, RemotingCommandException &#123; PullStatus pullStatus = PullStatus.NO_NEW_MSG; //判断响应结果 switch (response.getCode()) &#123; case ResponseCode.SUCCESS: pullStatus = PullStatus.FOUND; break; case ResponseCode.PULL_NOT_FOUND: pullStatus = PullStatus.NO_NEW_MSG; break; case ResponseCode.PULL_RETRY_IMMEDIATELY: pullStatus = PullStatus.NO_MATCHED_MSG; break; case ResponseCode.PULL_OFFSET_MOVED: pullStatus = PullStatus.OFFSET_ILLEGAL; break; default: throw new MQBrokerException(response.getCode(), response.getRemark()); &#125; //解码响应头 PullMessageResponseHeader responseHeader = (PullMessageResponseHeader) response.decodeCommandCustomHeader(PullMessageResponseHeader.class); //封装PullResultExt返回 return new PullResultExt(pullStatus, responseHeader.getNextBeginOffset(), responseHeader.getMinOffset(), responseHeader.getMaxOffset(), null, responseHeader.getSuggestWhichBrokerId(), response.getBody());&#125; PullResult类 12345private final PullStatus pullStatus; //拉取结果private final long nextBeginOffset; //下次拉取偏移量private final long minOffset; //消息队列最小偏移量private final long maxOffset; //消息队列最大偏移量private List&lt;MessageExt&gt; msgFoundList; //拉取的消息列表 代码：DefaultMQPushConsumerImpl$PullCallback#OnSuccess 123456789101112131415//将拉取到的消息存入processQueueboolean dispatchToConsume = processQueue.putMessage(pullResult.getMsgFoundList());//将processQueue提交到consumeMessageService中供消费者消费DefaultMQPushConsumerImpl.this.consumeMessageService.submitConsumeRequest( pullResult.getMsgFoundList(), processQueue, pullRequest.getMessageQueue(), dispatchToConsume);//如果pullInterval大于0,则等待pullInterval毫秒后将pullRequest对象放入到PullMessageService中的pullRequestQueue队列中if (DefaultMQPushConsumerImpl.this.defaultMQPushConsumer.getPullInterval() &gt; 0) &#123; DefaultMQPushConsumerImpl.this.executePullRequestLater(pullRequest, DefaultMQPushConsumerImpl.this.defaultMQPushConsumer.getPullInterval());&#125; else &#123; DefaultMQPushConsumerImpl.this.executePullRequestImmediately(pullRequest);&#125; 消息拉取总结 消息拉取长轮询机制分析RocketMQ未真正实现消息推模式，而是消费者主动向消息服务器拉取消息，RocketMQ推模式是循环向消息服务端发起消息拉取请求，如果消息消费者向RocketMQ拉取消息时，消息未到达消费队列时，如果不启用长轮询机制，则会在服务端等待shortPollingTimeMills时间后（挂起）再去判断消息是否已经到达指定消息队列，如果消息仍未到达则提示拉取消息客户端PULL—NOT—FOUND（消息不存在）；如果开启长轮询模式，RocketMQ一方面会每隔5s轮询检查一次消息是否可达，同时一有消息达到后立马通知挂起线程再次验证消息是否是自己感兴趣的消息，如果是则从CommitLog文件中提取消息返回给消息拉取客户端，否则直到挂起超时，超时时间由消息拉取方在消息拉取是封装在请求参数中，PUSH模式为15s，PULL模式通过DefaultMQPullConsumer#setBrokerSuspendMaxTimeMillis设置。RocketMQ通过在Broker客户端配置longPollingEnable为true来开启长轮询模式。 代码：PullMessageProcessor#processRequest 12345678910111213141516171819//当没有拉取到消息时，通过长轮询方式继续拉取消息case ResponseCode.PULL_NOT_FOUND: if (brokerAllowSuspend &amp;&amp; hasSuspendFlag) &#123; long pollingTimeMills = suspendTimeoutMillisLong; if (!this.brokerController.getBrokerConfig().isLongPollingEnable()) &#123; pollingTimeMills = this.brokerController.getBrokerConfig().getShortPollingTimeMills(); &#125; String topic = requestHeader.getTopic(); long offset = requestHeader.getQueueOffset(); int queueId = requestHeader.getQueueId(); //构建拉取请求对象 PullRequest pullRequest = new PullRequest(request, channel, pollingTimeMills, this.brokerController.getMessageStore().now(), offset, subscriptionData, messageFilter); //处理拉取请求 this.brokerController.getPullRequestHoldService().suspendPullRequest(topic, queueId, pullRequest); response = null; break; &#125; PullRequestHoldService方式实现长轮询 代码：PullRequestHoldService#suspendPullRequest 1234567891011121314//将拉取消息请求，放置在ManyPullRequest集合中public void suspendPullRequest(final String topic, final int queueId, final PullRequest pullRequest) &#123; String key = this.buildKey(topic, queueId); ManyPullRequest mpr = this.pullRequestTable.get(key); if (null == mpr) &#123; mpr = new ManyPullRequest(); ManyPullRequest prev = this.pullRequestTable.putIfAbsent(key, mpr); if (prev != null) &#123; mpr = prev; &#125; &#125; mpr.addPullRequest(pullRequest);&#125; 代码：PullRequestHoldService#run 12345678910111213141516171819202122232425public void run() &#123; log.info("&#123;&#125; service started", this.getServiceName()); while (!this.isStopped()) &#123; try &#123; //如果开启长轮询每隔5秒判断消息是否到达 if (this.brokerController.getBrokerConfig().isLongPollingEnable()) &#123; this.waitForRunning(5 * 1000); &#125; else &#123; //没有开启长轮询,每隔1s再次尝试 this.waitForRunning(this.brokerController.getBrokerConfig().getShortPollingTimeMills()); &#125; long beginLockTimestamp = this.systemClock.now(); this.checkHoldRequest(); long costTime = this.systemClock.now() - beginLockTimestamp; if (costTime &gt; 5 * 1000) &#123; log.info("[NOTIFYME] check hold request cost &#123;&#125; ms.", costTime); &#125; &#125; catch (Throwable e) &#123; log.warn(this.getServiceName() + " service has exception. ", e); &#125; &#125; log.info("&#123;&#125; service end", this.getServiceName());&#125; 代码：PullRequestHoldService#checkHoldRequest 123456789101112131415161718//遍历拉取任务private void checkHoldRequest() &#123; for (String key : this.pullRequestTable.keySet()) &#123; String[] kArray = key.split(TOPIC_QUEUEID_SEPARATOR); if (2 == kArray.length) &#123; String topic = kArray[0]; int queueId = Integer.parseInt(kArray[1]); //获得消息偏移量 final long offset = this.brokerController.getMessageStore().getMaxOffsetInQueue(topic, queueId); try &#123; //通知有消息达到 this.notifyMessageArriving(topic, queueId, offset); &#125; catch (Throwable e) &#123; log.error("check hold request failed. topic=&#123;&#125;, queueId=&#123;&#125;", topic, queueId, e); &#125; &#125; &#125;&#125; 代码：PullRequestHoldService#notifyMessageArriving 1234567891011121314151617181920212223242526272829//如果拉取消息偏移大于请求偏移量,如果消息匹配调用executeRequestWhenWakeup处理消息if (newestOffset &gt; request.getPullFromThisOffset()) &#123; boolean match = request.getMessageFilter().isMatchedByConsumeQueue(tagsCode, new ConsumeQueueExt.CqExtUnit(tagsCode, msgStoreTime, filterBitMap)); // match by bit map, need eval again when properties is not null. if (match &amp;&amp; properties != null) &#123; match = request.getMessageFilter().isMatchedByCommitLog(null, properties); &#125; if (match) &#123; try &#123; this.brokerController.getPullMessageProcessor().executeRequestWhenWakeup(request.getClientChannel(), request.getRequestCommand()); &#125; catch (Throwable e) &#123; log.error("execute request when wakeup failed.", e); &#125; continue; &#125;&#125;//如果过期时间超时,则不继续等待将直接返回给客户端消息未找到if (System.currentTimeMillis() &gt;= (request.getSuspendTimestamp() + request.getTimeoutMillis())) &#123; try &#123; this.brokerController.getPullMessageProcessor().executeRequestWhenWakeup(request.getClientChannel(), request.getRequestCommand()); &#125; catch (Throwable e) &#123; log.error("execute request when wakeup failed.", e); &#125; continue;&#125; 如果开启了长轮询机制，PullRequestHoldService会每隔5s被唤醒去尝试检测是否有新的消息的到来才给客户端响应，或者直到超时才给客户端进行响应，消息实时性比较差，为了避免这种情况，RocketMQ引入另外一种机制：当消息到达时唤醒挂起线程触发一次检查。 DefaultMessageStore$ReputMessageService机制 代码：DefaultMessageStore#start 123//长轮询入口this.reputMessageService.setReputFromOffset(maxPhysicalPosInLogicQueue);this.reputMessageService.start(); 代码：DefaultMessageStore$ReputMessageService#run 123456789101112131415public void run() &#123; DefaultMessageStore.log.info(this.getServiceName() + " service started"); while (!this.isStopped()) &#123; try &#123; Thread.sleep(1); //长轮询核心逻辑代码入口 this.doReput(); &#125; catch (Exception e) &#123; DefaultMessageStore.log.warn(this.getServiceName() + " service has exception. ", e); &#125; &#125; DefaultMessageStore.log.info(this.getServiceName() + " service end");&#125; 代码：DefaultMessageStore$ReputMessageService#deReput 12345678//当新消息达到是,进行通知监听器进行处理if (BrokerRole.SLAVE != DefaultMessageStore.this.getMessageStoreConfig().getBrokerRole() &amp;&amp; DefaultMessageStore.this.brokerConfig.isLongPollingEnable()) &#123; DefaultMessageStore.this.messageArrivingListener.arriving(dispatchRequest.getTopic(), dispatchRequest.getQueueId(), dispatchRequest.getConsumeQueueOffset() + 1, dispatchRequest.getTagsCode(), dispatchRequest.getStoreTimestamp(), dispatchRequest.getBitMap(), dispatchRequest.getPropertiesMap());&#125; 代码：NotifyMessageArrivingListener#arriving 12345public void arriving(String topic, int queueId, long logicOffset, long tagsCode, long msgStoreTime, byte[] filterBitMap, Map&lt;String, String&gt; properties) &#123; this.pullRequestHoldService.notifyMessageArriving(topic, queueId, logicOffset, tagsCode, msgStoreTime, filterBitMap, properties);&#125; 消息队列负载与重新分布机制RocketMQ消息队列重新分配是由RebalanceService线程来实现。一个MQClientInstance持有一个RebalanceService实现，并随着MQClientInstance的启动而启动。 代码：RebalanceService#run 12345678910public void run() &#123; log.info(this.getServiceName() + " service started"); //RebalanceService线程默认每隔20s执行一次mqClientFactory.doRebalance方法 while (!this.isStopped()) &#123; this.waitForRunning(waitInterval); this.mqClientFactory.doRebalance(); &#125; log.info(this.getServiceName() + " service end");&#125; 代码：MQClientInstance#doRebalance 12345678910111213public void doRebalance() &#123; //MQClientInstance遍历以注册的消费者,对消费者执行doRebalance()方法 for (Map.Entry&lt;String, MQConsumerInner&gt; entry : this.consumerTable.entrySet()) &#123; MQConsumerInner impl = entry.getValue(); if (impl != null) &#123; try &#123; impl.doRebalance(); &#125; catch (Throwable e) &#123; log.error("doRebalance exception", e); &#125; &#125; &#125;&#125; 代码：RebalanceImpl#doRebalance 123456789101112131415161718//遍历订阅消息对每个主题的订阅的队列进行重新负载public void doRebalance(final boolean isOrder) &#123; Map&lt;String, SubscriptionData&gt; subTable = this.getSubscriptionInner(); if (subTable != null) &#123; for (final Map.Entry&lt;String, SubscriptionData&gt; entry : subTable.entrySet()) &#123; final String topic = entry.getKey(); try &#123; this.rebalanceByTopic(topic, isOrder); &#125; catch (Throwable e) &#123; if (!topic.startsWith(MixAll.RETRY_GROUP_TOPIC_PREFIX)) &#123; log.warn("rebalanceByTopic Exception", e); &#125; &#125; &#125; &#125; this.truncateMessageQueueNotMyTopic();&#125; 代码：RebalanceImpl#rebalanceByTopic 123456789101112131415161718192021222324252627//从主题订阅消息缓存表中获取主题的队列信息Set&lt;MessageQueue&gt; mqSet = this.topicSubscribeInfoTable.get(topic);//查找该主题订阅组所有的消费者IDList&lt;String&gt; cidAll = this.mQClientFactory.findConsumerIdList(topic, consumerGroup);//给消费者重新分配队列if (mqSet != null &amp;&amp; cidAll != null) &#123; List&lt;MessageQueue&gt; mqAll = new ArrayList&lt;MessageQueue&gt;(); mqAll.addAll(mqSet); Collections.sort(mqAll); Collections.sort(cidAll); AllocateMessageQueueStrategy strategy = this.allocateMessageQueueStrategy; List&lt;MessageQueue&gt; allocateResult = null; try &#123; allocateResult = strategy.allocate( this.consumerGroup, this.mQClientFactory.getClientId(), mqAll, cidAll); &#125; catch (Throwable e) &#123; log.error("AllocateMessageQueueStrategy.allocate Exception. allocateMessageQueueStrategyName=&#123;&#125;", strategy.getName(), e); return; &#125; RocketMQ默认提供5中负载均衡分配算法 123456789101112AllocateMessageQueueAveragely:平均分配举例:8个队列q1,q2,q3,q4,q5,a6,q7,q8,消费者3个:c1,c2,c3分配如下:c1:q1,q2,q3c2:q4,q5,a6c3:q7,q8AllocateMessageQueueAveragelyByCircle:平均轮询分配举例:8个队列q1,q2,q3,q4,q5,a6,q7,q8,消费者3个:c1,c2,c3分配如下:c1:q1,q4,q7c2:q2,q5,a8c3:q3,q6 注意：消息队列的分配遵循一个消费者可以分配到多个队列，但同一个消息队列只会分配给一个消费者，故如果出现消费者个数大于消息队列数量，则有些消费者无法消费消息。 消息消费过程PullMessageService负责对消息队列进行消息拉取，从远端服务器拉取消息后将消息存储ProcessQueue消息队列处理队列中，然后调用ConsumeMessageService#submitConsumeRequest方法进行消息消费，使用线程池来消费消息，确保了消息拉取与消息消费的解耦。ConsumeMessageService支持顺序消息和并发消息，核心类图如下： 并发消息消费 代码：ConsumeMessageConcurrentlyService#submitConsumeRequest 1234567891011121314151617181920212223242526272829303132//消息批次单次final int consumeBatchSize = this.defaultMQPushConsumer.getConsumeMessageBatchMaxSize();//msgs.size()默认最多为32条。//如果msgs.size()小于consumeBatchSize,则直接将拉取到的消息放入到consumeRequest,然后将consumeRequest提交到消费者线程池中if (msgs.size() &lt;= consumeBatchSize) &#123; ConsumeRequest consumeRequest = new ConsumeRequest(msgs, processQueue, messageQueue); try &#123; this.consumeExecutor.submit(consumeRequest); &#125; catch (RejectedExecutionException e) &#123; this.submitConsumeRequestLater(consumeRequest); &#125;&#125;else&#123; //如果拉取的消息条数大于consumeBatchSize,则对拉取消息进行分页 for (int total = 0; total &lt; msgs.size(); ) &#123; List&lt;MessageExt&gt; msgThis = new ArrayList&lt;MessageExt&gt;(consumeBatchSize); for (int i = 0; i &lt; consumeBatchSize; i++, total++) &#123; if (total &lt; msgs.size()) &#123; msgThis.add(msgs.get(total)); &#125; else &#123; break; &#125; ConsumeRequest consumeRequest = new ConsumeRequest(msgThis, processQueue, messageQueue); try &#123; this.consumeExecutor.submit(consumeRequest); &#125; catch (RejectedExecutionException e) &#123; for (; total &lt; msgs.size(); total++) &#123; msgThis.add(msgs.get(total)); this.submitConsumeRequestLater(consumeRequest); &#125; &#125;&#125; 代码：ConsumeMessageConcurrentlyService$ConsumeRequest#run 12345678910111213141516171819202122232425262728//检查processQueue的dropped,如果为true,则停止该队列消费。if (this.processQueue.isDropped()) &#123; log.info("the message queue not be able to consume, because it's dropped. group=&#123;&#125; &#123;&#125;", ConsumeMessageConcurrentlyService.this.consumerGroup, this.messageQueue); return;&#125;...//执行消息处理的钩子函数if (ConsumeMessageConcurrentlyService.this.defaultMQPushConsumerImpl.hasHook()) &#123; consumeMessageContext = new ConsumeMessageContext(); consumeMessageContext.setNamespace(defaultMQPushConsumer.getNamespace()); consumeMessageContext.setConsumerGroup(defaultMQPushConsumer.getConsumerGroup()); consumeMessageContext.setProps(new HashMap&lt;String, String&gt;()); consumeMessageContext.setMq(messageQueue); consumeMessageContext.setMsgList(msgs); consumeMessageContext.setSuccess(false); ConsumeMessageConcurrentlyService.this.defaultMQPushConsumerImpl.executeHookBefore(consumeMessageContext);&#125;...//调用应用程序消息监听器的consumeMessage方法,进入到具体的消息消费业务处理逻辑status = listener.consumeMessage(Collections.unmodifiableList(msgs), context);//执行消息处理后的钩子函数if (ConsumeMessageConcurrentlyService.this.defaultMQPushConsumerImpl.hasHook()) &#123; consumeMessageContext.setStatus(status.toString()); consumeMessageContext.setSuccess(ConsumeConcurrentlyStatus.CONSUME_SUCCESS == status); ConsumeMessageConcurrentlyService.this.defaultMQPushConsumerImpl.executeHookAfter(consumeMessageContext);&#125; 定时消息机制定时消息是消息发送到Broker后，并不立即被消费者消费而是要等到特定的时间后才能被消费，RocketMQ并不支持任意的时间精度，如果要支持任意时间精度定时调度，不可避免地需要在Broker层做消息排序，再加上持久化方面的考量，将不可避免的带来巨大的性能消耗，所以RocketMQ只支持特定级别的延迟消息。消息延迟级别在Broker端通过messageDelayLevel配置，默认为“1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h”，delayLevel=1表示延迟消息1s,delayLevel=2表示延迟5s,依次类推。 RocketMQ定时消息实现类为ScheduleMessageService，该类在DefaultMessageStore中创建。通过在DefaultMessageStore中调用load方法加载该类并调用start方法启动。 代码：ScheduleMessageService#load 123456//加载延迟消息消费进度的加载与delayLevelTable的构造。延迟消息的进度默认存储路径为/store/config/delayOffset.jsonpublic boolean load() &#123; boolean result = super.load(); result = result &amp;&amp; this.parseDelayLevel(); return result;&#125; 代码：ScheduleMessageService#start 1234567891011121314151617181920212223242526//遍历延迟队列创建定时任务,遍历延迟级别，根据延迟级别level从offsetTable中获取消费队列的消费进度。如果不存在，则使用0for (Map.Entry&lt;Integer, Long&gt; entry : this.delayLevelTable.entrySet()) &#123; Integer level = entry.getKey(); Long timeDelay = entry.getValue(); Long offset = this.offsetTable.get(level); if (null == offset) &#123; offset = 0L; &#125; if (timeDelay != null) &#123; this.timer.schedule(new DeliverDelayedMessageTimerTask(level, offset), FIRST_DELAY_TIME); &#125;&#125;//每隔10s持久化一次延迟队列的消息消费进度this.timer.scheduleAtFixedRate(new TimerTask() &#123; @Override public void run() &#123; try &#123; if (started.get()) ScheduleMessageService.this.persist(); &#125; catch (Throwable e) &#123; log.error("scheduleAtFixedRate flush exception", e); &#125; &#125;&#125;, 10000, this.defaultMessageStore.getMessageStoreConfig().getFlushDelayOffsetInterval()); 调度机制 ScheduleMessageService的start方法启动后，会为每一个延迟级别创建一个调度任务，每一个延迟级别对应SCHEDULE_TOPIC_XXXX主题下的一个消息消费队列。定时调度任务的实现类为DeliverDelayedMessageTimerTask，核心实现方法为executeOnTimeup 代码：ScheduleMessageService$DeliverDelayedMessageTimerTask#executeOnTimeup 123456789101112131415161718192021222324252627282930313233343536//根据队列ID与延迟主题查找消息消费队列ConsumeQueue cq = ScheduleMessageService.this.defaultMessageStore.findConsumeQueue(SCHEDULE_TOPIC, delayLevel2QueueId(delayLevel));...//根据偏移量从消息消费队列中获取当前队列中所有有效的消息SelectMappedBufferResult bufferCQ = cq.getIndexBuffer(this.offset);...//遍历ConsumeQueue,解析消息队列中消息for (; i &lt; bufferCQ.getSize(); i += ConsumeQueue.CQ_STORE_UNIT_SIZE) &#123; long offsetPy = bufferCQ.getByteBuffer().getLong(); int sizePy = bufferCQ.getByteBuffer().getInt(); long tagsCode = bufferCQ.getByteBuffer().getLong(); if (cq.isExtAddr(tagsCode)) &#123; if (cq.getExt(tagsCode, cqExtUnit)) &#123; tagsCode = cqExtUnit.getTagsCode(); &#125; else &#123; //can't find ext content.So re compute tags code. log.error("[BUG] can't find consume queue extend file content!addr=&#123;&#125;, offsetPy=&#123;&#125;, sizePy=&#123;&#125;", tagsCode, offsetPy, sizePy); long msgStoreTime = defaultMessageStore.getCommitLog().pickupStoreTimestamp(offsetPy, sizePy); tagsCode = computeDeliverTimestamp(delayLevel, msgStoreTime); &#125; &#125; long now = System.currentTimeMillis(); long deliverTimestamp = this.correctDeliverTimestamp(now, tagsCode); ... //根据消息偏移量与消息大小,从CommitLog中查找消息. MessageExt msgExt = ScheduleMessageService.this.defaultMessageStore.lookMessageByOffset( offsetPy, sizePy);&#125; 顺序消息顺序消息实现类是org.apache.rocketmq.client.impl.consumer.ConsumeMessageOrderlyService 代码：ConsumeMessageOrderlyService#start 1234567891011public void start() &#123; //如果消息模式为集群模式，启动定时任务，默认每隔20s执行一次锁定分配给自己的消息消费队列 if (MessageModel.CLUSTERING.equals(ConsumeMessageOrderlyService.this.defaultMQPushConsumerImpl.messageModel())) &#123; this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; ConsumeMessageOrderlyService.this.lockMQPeriodically(); &#125; &#125;, 1000 * 1, ProcessQueue.REBALANCE_LOCK_INTERVAL, TimeUnit.MILLISECONDS); &#125;&#125; 代码：ConsumeMessageOrderlyService#submitConsumeRequest 1234567891011//构建消息任务,并提交消费线程池中public void submitConsumeRequest( final List&lt;MessageExt&gt; msgs, final ProcessQueue processQueue, final MessageQueue messageQueue, final boolean dispathToConsume) &#123; if (dispathToConsume) &#123; ConsumeRequest consumeRequest = new ConsumeRequest(processQueue, messageQueue); this.consumeExecutor.submit(consumeRequest); &#125;&#125; 代码：ConsumeMessageOrderlyService$ConsumeRequest#run 12345678910//如果消息队列为丢弃,则停止本次消费任务if (this.processQueue.isDropped()) &#123; log.warn("run, the message queue not be able to consume, because it's dropped. &#123;&#125;", this.messageQueue); return;&#125;//从消息队列中获取一个对象。然后消费消息时先申请独占objLock锁。顺序消息一个消息消费队列同一时刻只会被一个消费线程池处理final Object objLock = messageQueueLock.fetchLockObject(this.messageQueue);synchronized (objLock) &#123; ...&#125; 小结RocketMQ消息消费方式分别为集群模式、广播模式。 消息队列负载由RebalanceService线程默认每隔20s进行一次消息队列负载，根据当前消费者组内消费者个数与主题队列数量按照某一种负载算法进行队列分配，分配原则为同一个消费者可以分配多个消息消费队列，同一个消息消费队列同一个时间只会分配给一个消费者。 消息拉取由PullMessageService线程根据RebalanceService线程创建的拉取任务进行拉取，默认每次拉取32条消息，提交给消费者消费线程后继续下一次消息拉取。如果消息消费过慢产生消息堆积会触发消息消费拉取流控。 并发消息消费指消费线程池中的线程可以并发对同一个消息队列的消息进行消费，消费成功后，取出消息队列中最小的消息偏移量作为消息消费进度偏移量存储在于消息消费进度存储文件中，集群模式消息消费进度存储在Broker（消息服务器），广播模式消息消费进度存储在消费者端。 RocketMQ不支持任意精度的定时调度消息，只支持自定义的消息延迟级别，例如1s、2s、5s等，可通过在broker配置文件中设置messageDelayLevel。 顺序消息一般使用集群模式，是指对消息消费者内的线程池中的线程对消息消费队列只能串行消费。并并发消息消费最本质的区别是消息消费时必须成功锁定消息消费队列，在Broker端会存储消息消费队列的锁占用情况。]]></content>
      <categories>
        <category>RocketMQ</category>
      </categories>
      <tags>
        <tag>RocketMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ 02 - 案例介绍]]></title>
    <url>%2FRocketMQ%2Fmiddleware%2Frocketmq-02%2F</url>
    <content type="text"><![CDATA[说明：本文档由《黑马程序员》整理，本人只是为了方便才整理到自己博客！！相应视频资料：RocketMQ系统精讲 案例介绍业务分析模拟电商网站购物场景中的【下单】和【支付】业务 下单 用户请求订单系统下单 订单系统通过RPC调用订单服务下单 订单服务调用优惠券服务，扣减优惠券 订单服务调用调用库存服务，校验并扣减库存 订单服务调用用户服务，扣减用户余额 订单服务完成确认订单 支付 用户请求支付系统 支付系统调用第三方支付平台API进行发起支付流程 用户通过第三方支付平台支付成功后，第三方支付平台回调通知支付系统 支付系统调用订单服务修改订单状态 支付系统调用积分服务添加积分 支付系统调用日志服务记录日志 问题分析问题1用户提交订单后，扣减库存成功、扣减优惠券成功、使用余额成功，但是在确认订单操作失败，需要对库存、库存、余额进行回退。 如何保证数据的完整性？ 使用MQ保证在下单失败后系统数据的完整性 问题2用户通过第三方支付平台（支付宝、微信）支付成功后，第三方支付平台要通过回调API异步通知商家支付系统用户支付结果，支付系统根据支付结果修改订单状态、记录支付日志和给用户增加积分。 商家支付系统如何保证在收到第三方支付平台的异步通知时，如何快速给第三方支付凭条做出回应？ 通过MQ进行数据分发，提高系统处理性能 技术分析技术选型 SpringBoot Dubbo Zookeeper RocketMQ Mysql SpringBoot整合RocketMQ下载rocketmq-spring项目 将rocketmq-spring安装到本地仓库 1mvn install -Dmaven.skip.test=true 消息生产者添加依赖12345678910111213141516171819202122232425262728&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;properties&gt; &lt;rocketmq-spring-boot-starter-version&gt;2.0.3&lt;/rocketmq-spring-boot-starter-version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt; &lt;artifactId&gt;rocketmq-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;$&#123;rocketmq-spring-boot-starter-version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 配置文件123# application.propertiesrocketmq.name-server=192.168.25.135:9876;192.168.25.138:9876rocketmq.producer.group=my-group 启动类123456@SpringBootApplicationpublic class MQProducerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(MQSpringBootApplication.class); &#125;&#125; 测试类123456789101112@RunWith(SpringRunner.class)@SpringBootTest(classes = &#123;MQSpringBootApplication.class&#125;)public class ProducerTest &#123; @Autowired private RocketMQTemplate rocketMQTemplate; @Test public void test1()&#123; rocketMQTemplate.convertAndSend("springboot-mq","hello springboot rocketmq"); &#125;&#125; 消息消费者添加依赖同消息生产者 配置文件同消息生产者 启动类123456@SpringBootApplicationpublic class MQConsumerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(MQSpringBootApplication.class); &#125;&#125; 消息监听器12345678910@Slf4j@Component@RocketMQMessageListener(topic = "springboot-mq",consumerGroup = "springboot-mq-consumer-1")public class Consumer implements RocketMQListener&lt;String&gt; &#123; @Override public void onMessage(String message) &#123; log.info("Receive message："+message); &#125;&#125; SpringBoot整合Dubbo下载dubbo-spring-boot-starter依赖包 将123```shellmvn install -Dmaven.skip.test=true 搭建Zookeeper集群准备工作 安装JDK 将Zookeeper上传到服务器 解压Zookeeper，并创建data目录，将conf下的zoo_sample.cfg文件改名为zoo.cfg 建立/user/local/zookeeper-cluster,将解压后的Zookeeper复制到以下三个目录 123/usr/local/zookeeper-cluster/zookeeper-1/usr/local/zookeeper-cluster/zookeeper-2/usr/local/zookeeper-cluster/zookeeper-3 配置每一个 Zookeeper 的 dataDir（zoo.cfg） clientPort 分别为 2181 2182 2183 修改/usr/local/zookeeper-cluster/zookeeper-1/conf/zoo.cfg 12clientPort=2181dataDir=/usr/local/zookeeper-cluster/zookeeper-1/data 修改/usr/local/zookeeper-cluster/zookeeper-2/conf/zoo.cfg 12clientPort=2182dataDir=/usr/local/zookeeper-cluster/zookeeper-2/data 修改/usr/local/zookeeper-cluster/zookeeper-3/conf/zoo.cfg 12clientPort=2183dataDir=/usr/local/zookeeper-cluster/zookeeper-3/data 配置集群 在每个 zookeeper 的 data 目录下创建一个 myid 文件，内容分别是 1、2、3 。这个文件就是记录每个服务器的 ID 在每一个 zookeeper 的 zoo.cfg 配置客户端访问端口（clientPort）和集群服务器 IP 列表。 集群服务器 IP 列表如下 123server.1=192.168.25.140:2881:3881server.2=192.168.25.140:2882:3882server.3=192.168.25.140:2883:3883 解释：server.服务器 ID=服务器 IP 地址：服务器之间通信端口：服务器之间投票选举端口 启动集群启动集群就是分别启动每个实例。 RPC服务接口123public interface IUserService &#123; public String sayHello(String name);&#125; 服务提供者添加依赖123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;!--dubbo--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--spring-boot-stater--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;log4j-to-slf4j&lt;/artifactId&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!--zookeeper--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.10&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.101tec&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.9&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!--API--&gt; &lt;dependency&gt; &lt;groupId&gt;com.itheima.demo&lt;/groupId&gt; &lt;artifactId&gt;dubbo-api&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 配置文件12345678# application.propertiesspring.application.name=dubbo-demo-providerspring.dubbo.application.id=dubbo-demo-providerspring.dubbo.application.name=dubbo-demo-providerspring.dubbo.registry.address=zookeeper://192.168.25.140:2181;zookeeper://192.168.25.140:2182;zookeeper://192.168.25.140:2183spring.dubbo.server=truespring.dubbo.protocol.name=dubbospring.dubbo.protocol.port=20880 启动类123456789@EnableDubboConfiguration@SpringBootApplicationpublic class ProviderBootstrap &#123; public static void main(String[] args) throws IOException &#123; SpringApplication.run(ProviderBootstrap.class,args); &#125;&#125; 服务实现12345678@Component@Service(interfaceClass = IUserService.class)public class UserServiceImpl implements IUserService&#123; @Override public String sayHello(String name) &#123; return "hello:"+name; &#125;&#125; 服务消费者添加依赖1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--dubbo--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;log4j-to-slf4j&lt;/artifactId&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!--zookeeper--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.10&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.101tec&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.9&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!--API--&gt; &lt;dependency&gt; &lt;groupId&gt;com.itheima.demo&lt;/groupId&gt; &lt;artifactId&gt;dubbo-api&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 配置文件12345# application.propertiesspring.application.name=dubbo-demo-consumerspring.dubbo.application.name=dubbo-demo-consumerspring.dubbo.application.id=dubbo-demo-consumer spring.dubbo.registry.address=zookeeper://192.168.25.140:2181;zookeeper://192.168.25.140:2182;zookeeper://192.168.25.140:2183 启动类1234567@EnableDubboConfiguration@SpringBootApplicationpublic class ConsumerBootstrap &#123; public static void main(String[] args) &#123; SpringApplication.run(ConsumerBootstrap.class); &#125;&#125; Controller12345678910111213@RestController@RequestMapping("/user")public class UserController &#123; @Reference private IUserService userService; @RequestMapping("/sayHello") public String sayHello(String name)&#123; return userService.sayHello(name); &#125;&#125; 环境搭建数据库优惠券表 Field Type Comment coupon_id bigint(50) NOT NULL 优惠券ID coupon_price decimal(10,2) NULL 优惠券金额 user_id bigint(50) NULL 用户ID order_id bigint(32) NULL 订单ID is_used int(1) NULL 是否使用 0未使用 1已使用 used_time timestamp NULL 使用时间 商品表 Field Type Comment goods_id bigint(50) NOT NULL 主键 goods_name varchar(255) NULL 商品名称 goods_number int(11) NULL 商品库存 goods_price decimal(10,2) NULL 商品价格 goods_desc varchar(255) NULL 商品描述 add_time timestamp NULL 添加时间 订单表 Field Type Comment order_id bigint(50) NOT NULL 订单ID user_id bigint(50) NULL 用户ID order_status int(1) NULL 订单状态 0未确认 1已确认 2已取消 3无效 4退款 pay_status int(1) NULL 支付状态 0未支付 1支付中 2已支付 shipping_status int(1) NULL 发货状态 0未发货 1已发货 2已退货 address varchar(255) NULL 收货地址 consignee varchar(255) NULL 收货人 goods_id bigint(50) NULL 商品ID goods_number int(11) NULL 商品数量 goods_price decimal(10,2) NULL 商品价格 goods_amount decimal(10,0) NULL 商品总价 shipping_fee decimal(10,2) NULL 运费 order_amount decimal(10,2) NULL 订单价格 coupon_id bigint(50) NULL 优惠券ID coupon_paid decimal(10,2) NULL 优惠券 money_paid decimal(10,2) NULL 已付金额 pay_amount decimal(10,2) NULL 支付金额 add_time timestamp NULL 创建时间 confirm_time timestamp NULL 订单确认时间 pay_time timestamp NULL 支付时间 订单商品日志表 Field Type Comment goods_id int(11) NOT NULL 商品ID order_id varchar(32) NOT NULL 订单ID goods_number int(11) NULL 库存数量 log_time datetime NULL 记录时间 用户表 Field Type Comment user_id bigint(50) NOT NULL 用户ID user_name varchar(255) NULL 用户姓名 user_password varchar(255) NULL 用户密码 user_mobile varchar(255) NULL 手机号 user_score int(11) NULL 积分 user_reg_time timestamp NULL 注册时间 user_money decimal(10,0) NULL 用户余额 用户余额日志表 Field Type Comment user_id bigint(50) NOT NULL 用户ID order_id bigint(50) NOT NULL 订单ID money_log_type int(1) NOT NULL 日志类型 1订单付款 2 订单退款 use_money decimal(10,2) NULL 操作金额 create_time timestamp NULL 日志时间 订单支付表 Field Type Comment pay_id bigint(50) NOT NULL 支付编号 order_id bigint(50) NULL 订单编号 pay_amount decimal(10,2) NULL 支付金额 is_paid int(1) NULL 是否已支付 1否 2是 MQ消息生产表 Field Type Comment id varchar(100) NOT NULL 主键 group_name varchar(100) NULL 生产者组名 msg_topic varchar(100) NULL 消息主题 msg_tag varchar(100) NULL Tag msg_key varchar(100) NULL Key msg_body varchar(500) NULL 消息内容 msg_status int(1) NULL 0:未处理;1:已经处理 create_time timestamp NOT NULL 记录时间 MQ消息消费表 Field Type Comment msg_id varchar(50) NULL 消息ID group_name varchar(100) NOT NULL 消费者组名 msg_tag varchar(100) NOT NULL Tag msg_key varchar(100) NOT NULL Key msg_body varchar(500) NULL 消息体 consumer_status int(1) NULL 0:正在处理;1:处理成功;2:处理失败 consumer_times int(1) NULL 消费次数 consumer_timestamp timestamp NULL 消费时间 remark varchar(500) NULL 备注 项目初始化shop系统基于Maven进行项目管理 工程浏览 父工程：shop-parent 订单系统：shop-order-web 支付系统：shop-pay-web 优惠券服务：shop-coupon-service 订单服务：shop-order-service 支付服务：shop-pay-service 商品服务：shop-goods-service 用户服务：shop-user-service 实体类：shop-pojo 持久层：shop-dao 接口层：shop-api 工具工程：shop-common 共12个系统 工程关系 Mybatis逆向工程使用代码生成使用Mybatis逆向工程针对数据表生成CURD持久层代码 代码导入 将实体类导入到shop-pojo工程 在服务层工程中导入对应的Mapper类和对应配置文件 公共类介绍 ID生成器 IDWorker：Twitter雪花算法 异常处理类 CustomerException：自定义异常类 CastException：异常抛出类 常量类 ShopCode：系统状态类 响应实体类 Result：封装响应状态和响应信息 下单业务 下单基本流程接口定义 IOrderService 12345678public interface IOrderService &#123; /** * 确认订单 * @param order * @return Result */ Result confirmOrder(TradeOrder order);&#125; 业务类实现123456789101112131415161718192021222324252627282930@Slf4j@Component@Service(interfaceClass = IOrderService.class)public class OrderServiceImpl implements IOrderService &#123; @Override public Result confirmOrder(TradeOrder order) &#123; //1.校验订单 //2.生成预订单 try &#123; //3.扣减库存 //4.扣减优惠券 //5.使用余额 //6.确认订单 //7.返回成功状态 &#125; catch (Exception e) &#123; //1.确认订单失败,发送消息 //2.返回失败状态 &#125; &#125;&#125; 校验订单 1234567891011121314151617181920212223242526private void checkOrder(TradeOrder order) &#123; //1.校验订单是否存在 if(order==null)&#123; CastException.cast(ShopCode.SHOP_ORDER_INVALID); &#125; //2.校验订单中的商品是否存在 TradeGoods goods = goodsService.findOne(order.getGoodsId()); if(goods==null)&#123; CastException.cast(ShopCode.SHOP_GOODS_NO_EXIST); &#125; //3.校验下单用户是否存在 TradeUser user = userService.findOne(order.getUserId()); if(user==null)&#123; CastException.cast(ShopCode.SHOP_USER_NO_EXIST); &#125; //4.校验商品单价是否合法 if(order.getGoodsPrice().compareTo(goods.getGoodsPrice())!=0)&#123; CastException.cast(ShopCode.SHOP_GOODS_PRICE_INVALID); &#125; //5.校验订单商品数量是否合法 if(order.getGoodsNumber()&gt;=goods.getGoodsNumber())&#123; CastException.cast(ShopCode.SHOP_GOODS_NUM_NOT_ENOUGH); &#125; log.info("校验订单通过");&#125; 生成预订单 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374private Long savePreOrder(TradeOrder order) &#123; //1.设置订单状态为不可见 order.setOrderStatus(ShopCode.SHOP_ORDER_NO_CONFIRM.getCode()); //2.订单ID order.setOrderId(idWorker.nextId()); //核算运费是否正确 BigDecimal shippingFee = calculateShippingFee(order.getOrderAmount()); if (order.getShippingFee().compareTo(shippingFee) != 0) &#123; CastException.cast(ShopCode.SHOP_ORDER_SHIPPINGFEE_INVALID); &#125; //3.计算订单总价格是否正确 BigDecimal orderAmount = order.getGoodsPrice().multiply(new BigDecimal(order.getGoodsNumber())); orderAmount.add(shippingFee); if (orderAmount.compareTo(order.getOrderAmount()) != 0) &#123; CastException.cast(ShopCode.SHOP_ORDERAMOUNT_INVALID); &#125; //4.判断优惠券信息是否合法 Long couponId = order.getCouponId(); if (couponId != null) &#123; TradeCoupon coupon = couponService.findOne(couponId); //优惠券不存在 if (coupon == null) &#123; CastException.cast(ShopCode.SHOP_COUPON_NO_EXIST); &#125; //优惠券已经使用 if ((ShopCode.SHOP_COUPON_ISUSED.getCode().toString()) .equals(coupon.getIsUsed().toString())) &#123; CastException.cast(ShopCode.SHOP_COUPON_INVALIED); &#125; order.setCouponPaid(coupon.getCouponPrice()); &#125; else &#123; order.setCouponPaid(BigDecimal.ZERO); &#125; //5.判断余额是否正确 BigDecimal moneyPaid = order.getMoneyPaid(); if (moneyPaid != null) &#123; //比较余额是否大于0 int r = order.getMoneyPaid().compareTo(BigDecimal.ZERO); //余额小于0 if (r == -1) &#123; CastException.cast(ShopCode.SHOP_MONEY_PAID_LESS_ZERO); &#125; //余额大于0 if (r == 1) &#123; //查询用户信息 TradeUser user = userService.findOne(order.getUserId()); if (user == null) &#123; CastException.cast(ShopCode.SHOP_USER_NO_EXIST); &#125; //比较余额是否大于用户账户余额 if (user.getUserMoney().compareTo(order.getMoneyPaid().longValue()) == -1) &#123; CastException.cast(ShopCode.SHOP_MONEY_PAID_INVALID); &#125; order.setMoneyPaid(order.getMoneyPaid()); &#125; &#125; else &#123; order.setMoneyPaid(BigDecimal.ZERO); &#125; //计算订单支付总价 order.setPayAmount(orderAmount.subtract(order.getCouponPaid()) .subtract(order.getMoneyPaid())); //设置订单添加时间 order.setAddTime(new Date()); //保存预订单 int r = orderMapper.insert(order); if (ShopCode.SHOP_SUCCESS.getCode() != r) &#123; CastException.cast(ShopCode.SHOP_ORDER_SAVE_ERROR); &#125; log.info("订单:["+order.getOrderId()+"]预订单生成成功"); return order.getOrderId();&#125; 扣减库存 通过dubbo调用商品服务完成扣减库存 1234567891011private void reduceGoodsNum(TradeOrder order) &#123; TradeGoodsNumberLog goodsNumberLog = new TradeGoodsNumberLog(); goodsNumberLog.setGoodsId(order.getGoodsId()); goodsNumberLog.setOrderId(order.getOrderId()); goodsNumberLog.setGoodsNumber(order.getGoodsNumber()); Result result = goodsService.reduceGoodsNum(goodsNumberLog); if (result.getSuccess().equals(ShopCode.SHOP_FAIL.getSuccess())) &#123; CastException.cast(ShopCode.SHOP_REDUCE_GOODS_NUM_FAIL); &#125; log.info("订单:["+order.getOrderId()+"]扣减库存["+order.getGoodsNumber()+"个]成功"); &#125; 商品服务GoodsService扣减库存 1234567891011121314151617181920212223242526@Overridepublic Result reduceGoodsNum(TradeGoodsNumberLog goodsNumberLog) &#123; if (goodsNumberLog == null || goodsNumberLog.getGoodsNumber() == null || goodsNumberLog.getOrderId() == null || goodsNumberLog.getGoodsNumber() == null || goodsNumberLog.getGoodsNumber().intValue() &lt;= 0) &#123; CastException.cast(ShopCode.SHOP_REQUEST_PARAMETER_VALID); &#125; TradeGoods goods = goodsMapper.selectByPrimaryKey(goodsNumberLog.getGoodsId()); if(goods.getGoodsNumber()&lt;goodsNumberLog.getGoodsNumber())&#123; //库存不足 CastException.cast(ShopCode.SHOP_GOODS_NUM_NOT_ENOUGH); &#125; //减库存 goods.setGoodsNumber(goods.getGoodsNumber()-goodsNumberLog.getGoodsNumber()); goodsMapper.updateByPrimaryKey(goods); //记录库存操作日志 goodsNumberLog.setGoodsNumber(-(goodsNumberLog.getGoodsNumber())); goodsNumberLog.setLogTime(new Date()); goodsNumberLogMapper.insert(goodsNumberLog); return new Result(ShopCode.SHOP_SUCCESS.getSuccess(),ShopCode.SHOP_SUCCESS.getMessage());&#125; 扣减优惠券 通过dubbo完成扣减优惠券 123456789101112131415161718private void changeCoponStatus(TradeOrder order) &#123; //判断用户是否使用优惠券 if (!StringUtils.isEmpty(order.getCouponId())) &#123; //封装优惠券对象 TradeCoupon coupon = couponService.findOne(order.getCouponId()); coupon.setIsUsed(ShopCode.SHOP_COUPON_ISUSED.getCode()); coupon.setUsedTime(new Date()); coupon.setOrderId(order.getOrderId()); Result result = couponService.changeCouponStatus(coupon); //判断执行结果 if (result.getSuccess().equals(ShopCode.SHOP_FAIL.getSuccess())) &#123; //优惠券使用失败 CastException.cast(ShopCode.SHOP_COUPON_USE_FAIL); &#125; log.info("订单:["+order.getOrderId()+"]使用扣减优惠券["+coupon.getCouponPrice()+"元]成功"); &#125;&#125; 优惠券服务CouponService更改优惠券状态 1234567891011121314@Overridepublic Result changeCouponStatus(TradeCoupon coupon) &#123; try &#123; //判断请求参数是否合法 if (coupon == null || StringUtils.isEmpty(coupon.getCouponId())) &#123; CastException.cast(ShopCode.SHOP_REQUEST_PARAMETER_VALID); &#125; //更新优惠券状态为已使用 couponMapper.updateByPrimaryKey(coupon); return new Result(ShopCode.SHOP_SUCCESS.getSuccess(), ShopCode.SHOP_SUCCESS.getMessage()); &#125; catch (Exception e) &#123; return new Result(ShopCode.SHOP_FAIL.getSuccess(), ShopCode.SHOP_FAIL.getMessage()); &#125;&#125; 扣减用户余额 通过用户服务完成扣减余额 12345678910111213141516private void reduceMoneyPaid(TradeOrder order) &#123; //判断订单中使用的余额是否合法 if (order.getMoneyPaid() != null &amp;&amp; order.getMoneyPaid().compareTo(BigDecimal.ZERO) == 1) &#123; TradeUserMoneyLog userMoneyLog = new TradeUserMoneyLog(); userMoneyLog.setOrderId(order.getOrderId()); userMoneyLog.setUserId(order.getUserId()); userMoneyLog.setUseMoney(order.getMoneyPaid()); userMoneyLog.setMoneyLogType(ShopCode.SHOP_USER_MONEY_PAID.getCode()); //扣减余额 Result result = userService.changeUserMoney(userMoneyLog); if (result.getSuccess().equals(ShopCode.SHOP_FAIL.getSuccess())) &#123; CastException.cast(ShopCode.SHOP_USER_MONEY_REDUCE_FAIL); &#125; log.info("订单:["+order.getOrderId()+"扣减余额["+order.getMoneyPaid()+"元]成功]"); &#125;&#125; 用户服务UserService,更新余额 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556@Overridepublic Result changeUserMoney(TradeUserMoneyLog userMoneyLog) &#123; //判断请求参数是否合法 if (userMoneyLog == null || userMoneyLog.getUserId() == null || userMoneyLog.getUseMoney() == null || userMoneyLog.getOrderId() == null || userMoneyLog.getUseMoney().compareTo(BigDecimal.ZERO) &lt;= 0) &#123; CastException.cast(ShopCode.SHOP_REQUEST_PARAMETER_VALID); &#125; //查询该订单是否存在付款记录 TradeUserMoneyLogExample userMoneyLogExample = new TradeUserMoneyLogExample(); userMoneyLogExample.createCriteria() .andUserIdEqualTo(userMoneyLog.getUserId()) .andOrderIdEqualTo(userMoneyLog.getOrderId()); int count = userMoneyLogMapper.countByExample(userMoneyLogExample); TradeUser tradeUser = new TradeUser(); tradeUser.setUserId(userMoneyLog.getUserId()); tradeUser.setUserMoney(userMoneyLog.getUseMoney().longValue()); //判断余额操作行为 //【付款操作】 if (userMoneyLog.getMoneyLogType().equals(ShopCode.SHOP_USER_MONEY_PAID.getCode())) &#123; //订单已经付款，则抛异常 if (count &gt; 0) &#123; CastException.cast(ShopCode.SHOP_ORDER_PAY_STATUS_IS_PAY); &#125; //用户账户扣减余额 userMapper.reduceUserMoney(tradeUser); &#125; //【退款操作】 if (userMoneyLog.getMoneyLogType().equals(ShopCode.SHOP_USER_MONEY_REFUND.getCode())) &#123; //如果订单未付款,则不能退款,抛异常 if (count == 0) &#123; CastException.cast(ShopCode.SHOP_ORDER_PAY_STATUS_NO_PAY); &#125; //防止多次退款 userMoneyLogExample = new TradeUserMoneyLogExample(); userMoneyLogExample.createCriteria() .andUserIdEqualTo(userMoneyLog.getUserId()) .andOrderIdEqualTo(userMoneyLog.getOrderId()) .andMoneyLogTypeEqualTo(ShopCode.SHOP_USER_MONEY_REFUND.getCode()); count = userMoneyLogMapper.countByExample(userMoneyLogExample); if (count &gt; 0) &#123; CastException.cast(ShopCode.SHOP_USER_MONEY_REFUND_ALREADY); &#125; //用户账户添加余额 userMapper.addUserMoney(tradeUser); &#125; //记录用户使用余额日志 userMoneyLog.setCreateTime(new Date()); userMoneyLogMapper.insert(userMoneyLog); return new Result(ShopCode.SHOP_SUCCESS.getSuccess(),ShopCode.SHOP_SUCCESS.getMessage());&#125; 确认订单12345678910private void updateOrderStatus(TradeOrder order) &#123; order.setOrderStatus(ShopCode.SHOP_ORDER_CONFIRM.getCode()); order.setPayStatus(ShopCode.SHOP_ORDER_PAY_STATUS_NO_PAY.getCode()); order.setConfirmTime(new Date()); int r = orderMapper.updateByPrimaryKey(order); if (r &lt;= 0) &#123; CastException.cast(ShopCode.SHOP_ORDER_CONFIRM_FAIL); &#125; log.info("订单:["+order.getOrderId()+"]状态修改成功");&#125; 小结123456789101112131415161718192021222324@Overridepublic Result confirmOrder(TradeOrder order) &#123; //1.校验订单 checkOrder(order); //2.生成预订单 Long orderId = savePreOrder(order); order.setOrderId(orderId); try &#123; //3.扣减库存 reduceGoodsNum(order); //4.扣减优惠券 changeCoponStatus(order); //5.使用余额 reduceMoneyPaid(order); //6.确认订单 updateOrderStatus(order); log.info("订单:["+orderId+"]确认成功"); return new Result(ShopCode.SHOP_SUCCESS.getSuccess(), ShopCode.SHOP_SUCCESS.getMessage()); &#125; catch (Exception e) &#123; //确认订单失败,发送消息 ... return new Result(ShopCode.SHOP_FAIL.getSuccess(), ShopCode.SHOP_FAIL.getMessage()); &#125;&#125; 失败补偿机制消息发送方 配置RocketMQ属性值 1234567rocketmq.name-server=192.168.25.135:9876;192.168.25.138:9876rocketmq.producer.group=orderProducerGroupmq.order.consumer.group.name=order_orderTopic_cancel_groupmq.order.topic=orderTopicmq.order.tag.confirm=order_confirmmq.order.tag.cancel=order_cancel 注入模板类和属性值信息 12345678@Autowiredprivate RocketMQTemplate rocketMQTemplate;@Value("$&#123;mq.order.topic&#125;")private String topic;@Value("$&#123;mq.order.tag.cancel&#125;")private String cancelTag; 发送下单失败消息 123456789101112131415161718192021222324252627282930@Overridepublic Result confirmOrder(TradeOrder order) &#123; //1.校验订单 //2.生成预订 try &#123; //3.扣减库存 //4.扣减优惠券 //5.使用余额 //6.确认订单 &#125; catch (Exception e) &#123; //确认订单失败,发送消息 CancelOrderMQ cancelOrderMQ = new CancelOrderMQ(); cancelOrderMQ.setOrderId(order.getOrderId()); cancelOrderMQ.setCouponId(order.getCouponId()); cancelOrderMQ.setGoodsId(order.getGoodsId()); cancelOrderMQ.setGoodsNumber(order.getGoodsNumber()); cancelOrderMQ.setUserId(order.getUserId()); cancelOrderMQ.setUserMoney(order.getMoneyPaid()); try &#123; sendMessage(topic, cancelTag, cancelOrderMQ.getOrderId().toString(), JSON.toJSONString(cancelOrderMQ)); &#125; catch (Exception e1) &#123; e1.printStackTrace(); CastException.cast(ShopCode.SHOP_MQ_SEND_MESSAGE_FAIL); &#125; return new Result(ShopCode.SHOP_FAIL.getSuccess(), ShopCode.SHOP_FAIL.getMessage()); &#125;&#125; 1234567891011121314private void sendMessage(String topic, String tags, String keys, String body) throws Exception &#123; //判断Topic是否为空 if (StringUtils.isEmpty(topic)) &#123; CastException.cast(ShopCode.SHOP_MQ_TOPIC_IS_EMPTY); &#125; //判断消息内容是否为空 if (StringUtils.isEmpty(body)) &#123; CastException.cast(ShopCode.SHOP_MQ_MESSAGE_BODY_IS_EMPTY); &#125; //消息体 Message message = new Message(topic, tags, keys, body.getBytes()); //发送消息 rocketMQTemplate.getProducer().send(message);&#125; 消费接收方 配置RocketMQ属性值 123rocketmq.name-server=192.168.25.135:9876;192.168.25.138:9876mq.order.consumer.group.name=order_orderTopic_cancel_groupmq.order.topic=orderTopic 创建监听类，消费消息 123456789101112@Slf4j@Component@RocketMQMessageListener(topic = "$&#123;mq.order.topic&#125;", consumerGroup = "$&#123;mq.order.consumer.group.name&#125;", messageModel = MessageModel.BROADCASTING)public class CancelOrderConsumer implements RocketMQListener&lt;MessageExt&gt;&#123; @Override public void onMessage(MessageExt messageExt) &#123; ... &#125;&#125; 回退库存 流程分析 消息消费者 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138@Slf4j@Component@RocketMQMessageListener(topic = "$&#123;mq.order.topic&#125;",consumerGroup = "$&#123;mq.order.consumer.group.name&#125;",messageModel = MessageModel.BROADCASTING )public class CancelMQListener implements RocketMQListener&lt;MessageExt&gt;&#123; @Value("$&#123;mq.order.consumer.group.name&#125;") private String groupName; @Autowired private TradeGoodsMapper goodsMapper; @Autowired private TradeMqConsumerLogMapper mqConsumerLogMapper; @Autowired private TradeGoodsNumberLogMapper goodsNumberLogMapper; @Override public void onMessage(MessageExt messageExt) &#123; String msgId=null; String tags=null; String keys=null; String body=null; try &#123; //1. 解析消息内容 msgId = messageExt.getMsgId(); tags= messageExt.getTags(); keys= messageExt.getKeys(); body= new String(messageExt.getBody(),"UTF-8"); log.info("接受消息成功"); //2. 查询消息消费记录 TradeMqConsumerLogKey primaryKey = new TradeMqConsumerLogKey(); primaryKey.setMsgTag(tags); primaryKey.setMsgKey(keys); primaryKey.setGroupName(groupName); TradeMqConsumerLog mqConsumerLog = mqConsumerLogMapper.selectByPrimaryKey(primaryKey); if(mqConsumerLog!=null)&#123; //3. 判断如果消费过... //3.1 获得消息处理状态 Integer status = mqConsumerLog.getConsumerStatus(); //处理过...返回 if(ShopCode.SHOP_MQ_MESSAGE_STATUS_SUCCESS.getCode().intValue()==status.intValue())&#123; log.info("消息:"+msgId+",已经处理过"); return; &#125; //正在处理...返回 if(ShopCode.SHOP_MQ_MESSAGE_STATUS_PROCESSING.getCode().intValue()==status.intValue())&#123; log.info("消息:"+msgId+",正在处理"); return; &#125; //处理失败 if(ShopCode.SHOP_MQ_MESSAGE_STATUS_FAIL.getCode().intValue()==status.intValue())&#123; //获得消息处理次数 Integer times = mqConsumerLog.getConsumerTimes(); if(times&gt;3)&#123; log.info("消息:"+msgId+",消息处理超过3次,不能再进行处理了"); return; &#125; mqConsumerLog.setConsumerStatus(ShopCode.SHOP_MQ_MESSAGE_STATUS_PROCESSING.getCode()); //使用数据库乐观锁更新 TradeMqConsumerLogExample example = new TradeMqConsumerLogExample(); TradeMqConsumerLogExample.Criteria criteria = example.createCriteria(); criteria.andMsgTagEqualTo(mqConsumerLog.getMsgTag()); criteria.andMsgKeyEqualTo(mqConsumerLog.getMsgKey()); criteria.andGroupNameEqualTo(groupName); criteria.andConsumerTimesEqualTo(mqConsumerLog.getConsumerTimes()); int r = mqConsumerLogMapper.updateByExampleSelective(mqConsumerLog, example); if(r&lt;=0)&#123; //未修改成功,其他线程并发修改 log.info("并发修改,稍后处理"); &#125; &#125; &#125;else&#123; //4. 判断如果没有消费过... mqConsumerLog = new TradeMqConsumerLog(); mqConsumerLog.setMsgTag(tags); mqConsumerLog.setMsgKey(keys); mqConsumerLog.setConsumerStatus(ShopCode.SHOP_MQ_MESSAGE_STATUS_PROCESSING.getCode()); mqConsumerLog.setMsgBody(body); mqConsumerLog.setMsgId(msgId); mqConsumerLog.setConsumerTimes(0); //将消息处理信息添加到数据库 mqConsumerLogMapper.insert(mqConsumerLog); &#125; //5. 回退库存 MQEntity mqEntity = JSON.parseObject(body, MQEntity.class); Long goodsId = mqEntity.getGoodsId(); TradeGoods goods = goodsMapper.selectByPrimaryKey(goodsId); goods.setGoodsNumber(goods.getGoodsNumber()+mqEntity.getGoodsNum()); goodsMapper.updateByPrimaryKey(goods); //记录库存操作日志 TradeGoodsNumberLog goodsNumberLog = new TradeGoodsNumberLog(); goodsNumberLog.setOrderId(mqEntity.getOrderId()); goodsNumberLog.setGoodsId(goodsId); goodsNumberLog.setGoodsNumber(mqEntity.getGoodsNum()); goodsNumberLog.setLogTime(new Date()); goodsNumberLogMapper.insert(goodsNumberLog); //6. 将消息的处理状态改为成功 mqConsumerLog.setConsumerStatus(ShopCode.SHOP_MQ_MESSAGE_STATUS_SUCCESS.getCode()); mqConsumerLog.setConsumerTimestamp(new Date()); mqConsumerLogMapper.updateByPrimaryKey(mqConsumerLog); log.info("回退库存成功"); &#125; catch (Exception e) &#123; e.printStackTrace(); TradeMqConsumerLogKey primaryKey = new TradeMqConsumerLogKey(); primaryKey.setMsgTag(tags); primaryKey.setMsgKey(keys); primaryKey.setGroupName(groupName); TradeMqConsumerLog mqConsumerLog = mqConsumerLogMapper.selectByPrimaryKey(primaryKey); if(mqConsumerLog==null)&#123; //数据库未有记录 mqConsumerLog = new TradeMqConsumerLog(); mqConsumerLog.setMsgTag(tags); mqConsumerLog.setMsgKey(keys); mqConsumerLog.setConsumerStatus(ShopCode.SHOP_MQ_MESSAGE_STATUS_FAIL.getCode()); mqConsumerLog.setMsgBody(body); mqConsumerLog.setMsgId(msgId); mqConsumerLog.setConsumerTimes(1); mqConsumerLogMapper.insert(mqConsumerLog); &#125;else&#123; mqConsumerLog.setConsumerTimes(mqConsumerLog.getConsumerTimes()+1); mqConsumerLogMapper.updateByPrimaryKeySelective(mqConsumerLog); &#125; &#125; &#125;&#125; 回退优惠券1234567891011121314151617181920212223242526272829303132@Slf4j@Component@RocketMQMessageListener(topic = "$&#123;mq.order.topic&#125;",consumerGroup = "$&#123;mq.order.consumer.group.name&#125;",messageModel = MessageModel.BROADCASTING )public class CancelMQListener implements RocketMQListener&lt;MessageExt&gt;&#123; @Autowired private TradeCouponMapper couponMapper; @Override public void onMessage(MessageExt message) &#123; try &#123; //1. 解析消息内容 String body = new String(message.getBody(), "UTF-8"); MQEntity mqEntity = JSON.parseObject(body, MQEntity.class); log.info("接收到消息"); //2. 查询优惠券信息 TradeCoupon coupon = couponMapper.selectByPrimaryKey(mqEntity.getCouponId()); //3.更改优惠券状态 coupon.setUsedTime(null); coupon.setIsUsed(ShopCode.SHOP_COUPON_UNUSED.getCode()); coupon.setOrderId(null); couponMapper.updateByPrimaryKey(coupon); log.info("回退优惠券成功"); &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); log.error("回退优惠券失败"); &#125; &#125;&#125; 回退余额12345678910111213141516171819202122232425262728293031323334@Slf4j@Component@RocketMQMessageListener(topic = "$&#123;mq.order.topic&#125;",consumerGroup = "$&#123;mq.order.consumer.group.name&#125;",messageModel = MessageModel.BROADCASTING )public class CancelMQListener implements RocketMQListener&lt;MessageExt&gt;&#123; @Autowired private IUserService userService; @Override public void onMessage(MessageExt messageExt) &#123; try &#123; //1.解析消息 String body = new String(messageExt.getBody(), "UTF-8"); MQEntity mqEntity = JSON.parseObject(body, MQEntity.class); log.info("接收到消息"); if(mqEntity.getUserMoney()!=null &amp;&amp; mqEntity.getUserMoney().compareTo(BigDecimal.ZERO)&gt;0)&#123; //2.调用业务层,进行余额修改 TradeUserMoneyLog userMoneyLog = new TradeUserMoneyLog(); userMoneyLog.setUseMoney(mqEntity.getUserMoney()); userMoneyLog.setMoneyLogType(ShopCode.SHOP_USER_MONEY_REFUND.getCode()); userMoneyLog.setUserId(mqEntity.getUserId()); userMoneyLog.setOrderId(mqEntity.getOrderId()); userService.updateMoneyPaid(userMoneyLog); log.info("余额回退成功"); &#125; &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); log.error("余额回退失败"); &#125; &#125;&#125; 取消订单1234567891011121314@Override public void onMessage(MessageExt messageExt) &#123; String body = new String(messageExt.getBody(), "UTF-8"); String msgId = messageExt.getMsgId(); String tags = messageExt.getTags(); String keys = messageExt.getKeys(); log.info("CancelOrderProcessor receive message:"+messageExt); CancelOrderMQ cancelOrderMQ = JSON.parseObject(body, CancelOrderMQ.class); TradeOrder order = orderService.findOne(cancelOrderMQ.getOrderId()); order.setOrderStatus(ShopCode.SHOP_ORDER_CANCEL.getCode()); orderService.changeOrderStatus(order); log.info("订单:["+order.getOrderId()+"]状态设置为取消"); return order; &#125; 测试准备测试环境1234567@RunWith(SpringRunner.class)@SpringBootTest(classes = ShopOrderServiceApplication.class)public class OrderTest &#123; @Autowired private IOrderService orderService;&#125; 准备测试数据 用户数据 商品数据 优惠券数据 测试下单成功流程123456789101112131415161718@Test public void add()&#123; Long goodsId=XXXL; Long userId=XXXL; Long couponId=XXXL; TradeOrder order = new TradeOrder(); order.setGoodsId(goodsId); order.setUserId(userId); order.setGoodsNumber(1); order.setAddress("北京"); order.setGoodsPrice(new BigDecimal("5000")); order.setOrderAmount(new BigDecimal("5000")); order.setMoneyPaid(new BigDecimal("100")); order.setCouponId(couponId); order.setShippingFee(new BigDecimal(0)); orderService.confirmOrder(order);&#125; 执行完毕后,查看数据库中用户的余额、优惠券数据，及订单的状态数据 测试下单失败流程代码同上。 执行完毕后，查看用户的余额、优惠券数据是否发生更改，订单的状态是否为取消。 支付业务创建支付订单 12345678910111213141516171819202122public Result createPayment(TradePay tradePay) &#123; //查询订单支付状态 try &#123; TradePayExample payExample = new TradePayExample(); TradePayExample.Criteria criteria = payExample.createCriteria(); criteria.andOrderIdEqualTo(tradePay.getOrderId()); criteria.andIsPaidEqualTo(ShopCode.SHOP_ORDER_PAY_STATUS_IS_PAY.getCode()); int count = tradePayMapper.countByExample(payExample); if (count &gt; 0) &#123; CastException.cast(ShopCode.SHOP_ORDER_PAY_STATUS_IS_PAY); &#125; long payId = idWorker.nextId(); tradePay.setPayId(payId); tradePay.setIsPaid(ShopCode.SHOP_ORDER_PAY_STATUS_NO_PAY.getCode()); tradePayMapper.insert(tradePay); log.info("创建支付订单成功:" + payId); &#125; catch (Exception e) &#123; return new Result(ShopCode.SHOP_FAIL.getSuccess(), ShopCode.SHOP_FAIL.getMessage()); &#125; return new Result(ShopCode.SHOP_SUCCESS.getSuccess(), ShopCode.SHOP_SUCCESS.getMessage());&#125; 支付回调流程分析 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344public Result callbackPayment(TradePay tradePay) &#123; if (tradePay.getIsPaid().equals(ShopCode.SHOP_ORDER_PAY_STATUS_IS_PAY.getCode())) &#123; tradePay = tradePayMapper.selectByPrimaryKey(tradePay.getPayId()); if (tradePay == null) &#123; CastException.cast(ShopCode.SHOP_PAYMENT_NOT_FOUND); &#125; tradePay.setIsPaid(ShopCode.SHOP_ORDER_PAY_STATUS_IS_PAY.getCode()); int i = tradePayMapper.updateByPrimaryKeySelective(tradePay); //更新成功代表支付成功 if (i == 1) &#123; TradeMqProducerTemp mqProducerTemp = new TradeMqProducerTemp(); mqProducerTemp.setId(String.valueOf(idWorker.nextId())); mqProducerTemp.setGroupName("payProducerGroup"); mqProducerTemp.setMsgKey(String.valueOf(tradePay.getPayId())); mqProducerTemp.setMsgTag(topic); mqProducerTemp.setMsgBody(JSON.toJSONString(tradePay)); mqProducerTemp.setCreateTime(new Date()); mqProducerTempMapper.insert(mqProducerTemp); TradePay finalTradePay = tradePay; executorService.submit(new Runnable() &#123; @Override public void run() &#123; try &#123; SendResult sendResult = sendMessage(topic, tag, finalTradePay.getPayId(), JSON.toJSONString(finalTradePay)); log.info(JSON.toJSONString(sendResult)); if (SendStatus.SEND_OK.equals(sendResult.getSendStatus())) &#123; mqProducerTempMapper.deleteByPrimaryKey(mqProducerTemp.getId()); System.out.println("删除消息表成功"); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; else &#123; CastException.cast(ShopCode.SHOP_PAYMENT_IS_PAID); &#125; &#125; return new Result(ShopCode.SHOP_SUCCESS.getSuccess(), ShopCode.SHOP_SUCCESS.getMessage());&#125; 线程池优化消息发送逻辑 创建线程池对象 12345678910111213141516171819202122@Beanpublic ThreadPoolTaskExecutor getThreadPool() &#123; ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(4); executor.setMaxPoolSize(8); executor.setQueueCapacity(100); executor.setKeepAliveSeconds(60); executor.setThreadNamePrefix("Pool-A"); executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy()); executor.initialize(); return executor;&#125; 使用线程池 123456789101112131415161718@Autowiredprivate ThreadPoolTaskExecutor executorService;executorService.submit(new Runnable() &#123; @Override public void run() &#123; try &#123; SendResult sendResult = sendMessage(topic, tag, finalTradePay.getPayId(), JSON.toJSONString(finalTradePay)); log.info(JSON.toJSONString(sendResult)); if (SendStatus.SEND_OK.equals(sendResult.getSendStatus())) &#123; mqProducerTempMapper.deleteByPrimaryKey(mqProducerTemp.getId()); System.out.println("删除消息表成功"); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125;); 处理消息支付成功后，支付服务payService发送MQ消息，订单服务、用户服务、日志服务需要订阅消息进行处理 订单服务修改订单状态为已支付 日志服务记录支付日志 用户服务负责给用户增加积分 以下用订单服务为例说明消息的处理情况 配置RocketMQ属性值12mq.pay.topic=payTopicmq.pay.consumer.group.name=pay_payTopic_group 消费消息 在订单服务中，配置公共的消息处理类 123456789101112131415161718192021222324252627public class BaseConsumer &#123; public TradeOrder handleMessage(IOrderService orderService, MessageExt messageExt,Integer code) throws Exception &#123; //解析消息内容 String body = new String(messageExt.getBody(), "UTF-8"); String msgId = messageExt.getMsgId(); String tags = messageExt.getTags(); String keys = messageExt.getKeys(); OrderMQ orderMq = JSON.parseObject(body, OrderMQ.class); //查询 TradeOrder order = orderService.findOne(orderMq.getOrderId()); if(ShopCode.SHOP_ORDER_MESSAGE_STATUS_CANCEL.getCode().equals(code))&#123; order.setOrderStatus(ShopCode.SHOP_ORDER_CANCEL.getCode()); &#125; if(ShopCode.SHOP_ORDER_MESSAGE_STATUS_ISPAID.getCode().equals(code))&#123; order.setPayStatus(ShopCode.SHOP_ORDER_PAY_STATUS_IS_PAY.getCode()); &#125; orderService.changeOrderStatus(order); return order; &#125;&#125; 接受订单支付成功消息 1234567891011121314151617181920212223@Slf4j@Component@RocketMQMessageListener(topic = "$&#123;mq.pay.topic&#125;", consumerGroup = "$&#123;mq.pay.consumer.group.name&#125;")public class PayConsumer extends BaseConsumer implements RocketMQListener&lt;MessageExt&gt; &#123; @Autowired private IOrderService orderService; @Override public void onMessage(MessageExt messageExt) &#123; try &#123; log.info("CancelOrderProcessor receive message:"+messageExt); TradeOrder order = handleMessage(orderService, messageExt, ShopCode.SHOP_ORDER_MESSAGE_STATUS_ISPAID.getCode()); log.info("订单:["+order.getOrderId()+"]支付成功"); &#125; catch (Exception e) &#123; e.printStackTrace(); log.error("订单支付失败"); &#125; &#125;&#125; 整体联调通过Rest客户端请求shop-order-web和shop-pay-web完成下单和支付操作 准备工作配置RestTemplate类12345678910111213141516171819202122232425262728293031323334@Configurationpublic class RestTemplateConfig &#123; @Bean @ConditionalOnMissingBean(&#123; RestOperations.class, RestTemplate.class &#125;) public RestTemplate restTemplate(ClientHttpRequestFactory factory) &#123; RestTemplate restTemplate = new RestTemplate(factory); // 使用 utf-8 编码集的 conver 替换默认的 conver（默认的 string conver 的编码集为"ISO-8859-1"） List&lt;HttpMessageConverter&lt;?&gt;&gt; messageConverters = restTemplate.getMessageConverters(); Iterator&lt;HttpMessageConverter&lt;?&gt;&gt; iterator = messageConverters.iterator(); while (iterator.hasNext()) &#123; HttpMessageConverter&lt;?&gt; converter = iterator.next(); if (converter instanceof StringHttpMessageConverter) &#123; iterator.remove(); &#125; &#125; messageConverters.add(new StringHttpMessageConverter(Charset.forName("UTF-8"))); return restTemplate; &#125; @Bean @ConditionalOnMissingBean(&#123;ClientHttpRequestFactory.class&#125;) public ClientHttpRequestFactory simpleClientHttpRequestFactory() &#123; SimpleClientHttpRequestFactory factory = new SimpleClientHttpRequestFactory(); // ms factory.setReadTimeout(15000); // ms factory.setConnectTimeout(15000); return factory; &#125;&#125; 配置请求地址 订单系统 12345server.host=http://localhostserver.servlet.path=/order-webserver.port=8080shop.order.baseURI=$&#123;server.host&#125;:$&#123;server.port&#125;$&#123;server.servlet.path&#125;shop.order.confirm=/order/confirm 支付系统 123456server.host=http://localhostserver.servlet.path=/pay-webserver.port=9090shop.pay.baseURI=$&#123;server.host&#125;:$&#123;server.port&#125;$&#123;server.servlet.path&#125;shop.pay.createPayment=/pay/createPaymentshop.pay.callbackPayment=/pay/callbackPayment 下单测试 123456789101112131415161718192021222324252627282930313233343536373839404142@RunWith(SpringRunner.class)@ContextConfiguration(classes = ShopOrderWebApplication.class)@TestPropertySource("classpath:application.properties")public class OrderTest &#123; @Autowired private RestTemplate restTemplate; @Value("$&#123;shop.order.baseURI&#125;") private String baseURI; @Value("$&#123;shop.order.confirm&#125;") private String confirmOrderPath; @Autowired private IDWorker idWorker; /** * 下单 */ @Test public void confirmOrder()&#123; Long goodsId=XXXL; Long userId=XXXL; Long couponId=XXXL; TradeOrder order = new TradeOrder(); order.setGoodsId(goodsId); order.setUserId(userId); order.setGoodsNumber(1); order.setAddress("北京"); order.setGoodsPrice(new BigDecimal("5000")); order.setOrderAmount(new BigDecimal("5000")); order.setMoneyPaid(new BigDecimal("100")); order.setCouponId(couponId); order.setShippingFee(new BigDecimal(0)); Result result = restTemplate.postForEntity(baseURI + confirmOrderPath, order, Result.class).getBody(); System.out.println(result); &#125;&#125; 支付测试1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@RunWith(SpringRunner.class)@ContextConfiguration(classes = ShopPayWebApplication.class)@TestPropertySource("classpath:application.properties")public class PayTest &#123; @Autowired private RestTemplate restTemplate; @Value("$&#123;shop.pay.baseURI&#125;") private String baseURI; @Value("$&#123;shop.pay.createPayment&#125;") private String createPaymentPath; @Value("$&#123;shop.pay.callbackPayment&#125;") private String callbackPaymentPath; @Autowired private IDWorker idWorker; /** * 创建支付订单 */ @Test public void createPayment()&#123; Long orderId = 346321587315814400L; TradePay pay = new TradePay(); pay.setOrderId(orderId); pay.setPayAmount(new BigDecimal(4800)); Result result = restTemplate.postForEntity(baseURI + createPaymentPath, pay, Result.class).getBody(); System.out.println(result); &#125; /** * 支付回调 */ @Test public void callbackPayment()&#123; Long payId = 346321891507720192L; TradePay pay = new TradePay(); pay.setPayId(payId); pay.setIsPaid(ShopCode.SHOP_ORDER_PAY_STATUS_IS_PAY.getCode()); Result result = restTemplate.postForEntity(baseURI + callbackPaymentPath, pay, Result.class).getBody(); System.out.println(result); &#125;&#125;]]></content>
      <categories>
        <category>RocketMQ</category>
      </categories>
      <tags>
        <tag>RocketMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ 01 - 基础介绍]]></title>
    <url>%2FRocketMQ%2Fmiddleware%2Frocketmq-01%2F</url>
    <content type="text"><![CDATA[说明：本文档由《黑马程序员》整理，本人只是为了方便才整理到自己博客！！相应视频资料：RocketMQ系统精讲 MQ介绍为什么要用MQ消息队列是一种“先进先出”的数据结构 其应用场景主要包含以下3个方面 应用解耦 系统的耦合性越高，容错性就越低。以电商应用为例，用户创建订单后，如果耦合调用库存系统、物流系统、支付系统，任何一个子系统出了故障或者因为升级等原因暂时不可用，都会造成下单操作异常，影响用户使用体验。 使用消息队列解耦合，系统的耦合性就会提高了。比如物流系统发生故障，需要几分钟才能来修复，在这段时间内，物流系统要处理的数据被缓存到消息队列中，用户的下单操作正常完成。当物流系统回复后，补充处理存在消息队列中的订单消息即可，终端系统感知不到物流系统发生过几分钟故障。 流量削峰 应用系统如果遇到系统请求流量的瞬间猛增，有可能会将系统压垮。有了消息队列可以将大量请求缓存起来，分散到很长一段时间处理，这样可以大大提到系统的稳定性和用户体验。 一般情况，为了保证系统的稳定性，如果系统负载超过阈值，就会阻止用户请求，这会影响用户体验，而如果使用消息队列将请求缓存起来，等待系统处理完毕后通知用户下单完毕，这样总不能下单体验要好。 处于经济考量目的： 业务系统正常时段的QPS如果是1000，流量最高峰是10000，为了应对流量高峰配置高性能的服务器显然不划算，这时可以使用消息队列对峰值流量削峰 数据分发 通过消息队列可以让数据在多个系统更加之间进行流通。数据的产生方不需要关心谁来使用数据，只需要将数据发送到消息队列，数据使用方直接在消息队列中直接获取数据即可 MQ的优点和缺点优点：解耦、削峰、数据分发 缺点包含以下几点： 系统可用性降低 系统引入的外部依赖越多，系统稳定性越差。一旦MQ宕机，就会对业务造成影响。 如何保证MQ的高可用？ 系统复杂度提高 MQ的加入大大增加了系统的复杂度，以前系统间是同步的远程调用，现在是通过MQ进行异步调用。 如何保证消息没有被重复消费？怎么处理消息丢失情况？那么保证消息传递的顺序性？ 一致性问题 A系统处理完业务，通过MQ给B、C、D三个系统发消息数据，如果B系统、C系统处理成功，D系统处理失败。 如何保证消息数据处理的一致性？ 各种MQ产品的比较常见的MQ产品包括Kafka、ActiveMQ、RabbitMQ、RocketMQ。 RocketMQ快速入门RocketMQ是阿里巴巴2016年MQ中间件，使用Java语言开发，在阿里内部，RocketMQ承接了例如“双11”等高并发场景的消息流转，能够处理万亿级别的消息。 准备工作下载RocketMQRocketMQ最新版本：4.5.1 下载地址 环境要求 Linux64位系统 JDK1.8(64位) 源码安装需要安装Maven 3.2.x 安装RocketMQ安装步骤本教程以二进制包方式安装 解压安装包 进入安装目录 目录介绍 bin：启动脚本，包括shell脚本和CMD脚本 conf：实例配置文件 ，包括broker配置文件、logback配置文件等 lib：依赖jar包，包括Netty、commons-lang、FastJSON等 启动RocketMQ 启动NameServer 1234# 1.启动NameServernohup sh bin/mqnamesrv &amp;# 2.查看启动日志tail -f ~/logs/rocketmqlogs/namesrv.log 启动Broker 1234# 1.启动Brokernohup sh bin/mqbroker -n localhost:9876 &amp;# 2.查看启动日志tail -f ~/logs/rocketmqlogs/broker.log 问题描述： RocketMQ默认的虚拟机内存较大，启动Broker如果因为内存不足失败，需要编辑如下两个配置文件，修改JVM内存大小 123# 编辑runbroker.sh和runserver.sh修改默认JVM大小vi runbroker.shvi runserver.sh 参考设置： 12345678910## 测试RocketMQ### 发送消息```sh# 1.设置环境变量export NAMESRV_ADDR=localhost:9876# 2.使用安装包的Demo发送消息sh bin/tools.sh org.apache.rocketmq.example.quickstart.Producer 接收消息1234# 1.设置环境变量export NAMESRV_ADDR=localhost:9876# 2.接收消息sh bin/tools.sh org.apache.rocketmq.example.quickstart.Consumer 关闭RocketMQ1234# 1.关闭NameServersh bin/mqshutdown namesrv# 2.关闭Brokersh bin/mqshutdown broker RocketMQ集群搭建各角色介绍 Producer：消息的发送者；举例：发信者 Consumer：消息接收者；举例：收信者 Broker：暂存和传输消息；举例：邮局 NameServer：管理Broker；举例：各个邮局的管理机构 Topic：区分消息的种类；一个发送者可以发送消息给一个或者多个Topic；一个消息的接收者可以订阅一个或者多个Topic消息 Message Queue：相当于是Topic的分区；用于并行发送和接收消息 集群搭建方式集群特点 NameServer是一个几乎无状态节点，可集群部署，节点之间无任何信息同步。 Broker部署相对复杂，Broker分为Master与Slave，一个Master可以对应多个Slave，但是一个Slave只能对应一个Master，Master与Slave的对应关系通过指定相同的BrokerName，不同的BrokerId来定义，BrokerId为0表示Master，非0表示Slave。Master也可以部署多个。每个Broker与NameServer集群中的所有节点建立长连接，定时注册Topic信息到所有NameServer。 Producer与NameServer集群中的其中一个节点（随机选择）建立长连接，定期从NameServer取Topic路由信息，并向提供Topic服务的Master建立长连接，且定时向Master发送心跳。Producer完全无状态，可集群部署。 Consumer与NameServer集群中的其中一个节点（随机选择）建立长连接，定期从NameServer取Topic路由信息，并向提供Topic服务的Master、Slave建立长连接，且定时向Master、Slave发送心跳。Consumer既可以从Master订阅消息，也可以从Slave订阅消息，订阅规则由Broker配置决定。 集群模式单Master模式这种方式风险较大，一旦Broker重启或者宕机时，会导致整个服务不可用。不建议线上环境使用,可以用于本地测试。 多Master模式一个集群无Slave，全是Master，例如2个Master或者3个Master，这种模式的优缺点如下： 优点：配置简单，单个Master宕机或重启维护对应用无影响，在磁盘配置为RAID10时，即使机器宕机不可恢复情况下，由于RAID10磁盘非常可靠，消息也不会丢（异步刷盘丢失少量消息，同步刷盘一条不丢），性能最高； 缺点：单台机器宕机期间，这台机器上未被消费的消息在机器恢复之前不可订阅，消息实时性会受到影响。 多Master多Slave模式（异步）每个Master配置一个Slave，有多对Master-Slave，HA采用异步复制方式，主备有短暂消息延迟（毫秒级），这种模式的优缺点如下： 优点：即使磁盘损坏，消息丢失的非常少，且消息实时性不会受影响，同时Master宕机后，消费者仍然可以从Slave消费，而且此过程对应用透明，不需要人工干预，性能同多Master模式几乎一样； 缺点：Master宕机，磁盘损坏情况下会丢失少量消息。 多Master多Slave模式（同步）每个Master配置一个Slave，有多对Master-Slave，HA采用同步双写方式，即只有主备都写成功，才向应用返回成功，这种模式的优缺点如下： 优点：数据与服务都无单点故障，Master宕机情况下，消息无延迟，服务可用性与数据可用性都非常高； 缺点：性能比异步复制模式略低（大约低10%左右），发送单个消息的RT会略高，且目前版本在主节点宕机后，备机不能自动切换为主机。 双主双从集群搭建总体架构消息高可用采用2m-2s（同步双写）方式 集群工作流程 启动NameServer，NameServer起来后监听端口，等待Broker、Producer、Consumer连上来，相当于一个路由控制中心。 Broker启动，跟所有的NameServer保持长连接，定时发送心跳包。心跳包中包含当前Broker信息(IP+端口等)以及存储所有Topic信息。注册成功后，NameServer集群中就有Topic跟Broker的映射关系。 收发消息前，先创建Topic，创建Topic时需要指定该Topic要存储在哪些Broker上，也可以在发送消息时自动创建Topic。 Producer发送消息，启动时先跟NameServer集群中的其中一台建立长连接，并从NameServer中获取当前发送的Topic存在哪些Broker上，轮询从队列列表中选择一个队列，然后与队列所在的Broker建立长连接从而向Broker发消息。 Consumer跟Producer类似，跟其中一台NameServer建立长连接，获取当前订阅Topic存在哪些Broker上，然后直接跟Broker建立连接通道，开始消费消息。 服务器环境 序号 IP 角色 架构模式 1 192.168.25.135 nameserver、brokerserver Master1、Slave2 2 192.168.25.138 nameserver、brokerserver Master2、Slave1 Host添加信息1vim /etc/hosts 配置如下: 12345678# nameserver192.168.25.135 rocketmq-nameserver1192.168.25.138 rocketmq-nameserver2# broker192.168.25.135 rocketmq-master1192.168.25.135 rocketmq-slave2192.168.25.138 rocketmq-master2192.168.25.138 rocketmq-slave1 配置完成后, 重启网卡 1systemctl restart network 防火墙配置宿主机需要远程访问虚拟机的rocketmq服务和web服务，需要开放相关的端口号，简单粗暴的方式是直接关闭防火墙 123456# 关闭防火墙systemctl stop firewalld.service # 查看防火墙的状态firewall-cmd --state # 禁止firewall开机启动systemctl disable firewalld.service 或者为了安全，只开放特定的端口号，RocketMQ默认使用3个端口：9876 、10911 、11011 。如果防火墙没有关闭的话，那么防火墙就必须开放这些端口： nameserver 默认使用 9876 端口 master 默认使用 10911 端口 slave 默认使用11011 端口 执行以下命令： 12345678# 开放name server默认端口firewall-cmd --remove-port=9876/tcp --permanent# 开放master默认端口firewall-cmd --remove-port=10911/tcp --permanent# 开放slave默认端口 (当前集群模式可不开启)firewall-cmd --remove-port=11011/tcp --permanent # 重启防火墙firewall-cmd --reload 环境变量配置1vim /etc/profile 在profile文件的末尾加入如下命令 1234#set rocketmqROCKETMQ_HOME=/usr/local/rocketmq/rocketmq-all-4.4.0-bin-releasePATH=$PATH:$ROCKETMQ_HOME/binexport ROCKETMQ_HOME PATH 输入:wq! 保存并退出， 并使得配置立刻生效： 1source /etc/profile 创建消息存储路径1234mkdir /usr/local/rocketmq/storemkdir /usr/local/rocketmq/store/commitlogmkdir /usr/local/rocketmq/store/consumequeuemkdir /usr/local/rocketmq/store/index broker配置文件master1服务器：192.168.25.135 1vi /usr/soft/rocketmq/conf/2m-2s-sync/broker-a.properties 修改配置如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#所属集群名字brokerClusterName=rocketmq-cluster#broker名字，注意此处不同的配置文件填写的不一样brokerName=broker-a#0 表示 Master，&gt;0 表示 SlavebrokerId=0#nameServer地址，分号分割namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876#在发送消息时，自动创建服务器不存在的topic，默认创建的队列数defaultTopicQueueNums=4#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭autoCreateTopicEnable=true#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭autoCreateSubscriptionGroup=true#Broker 对外服务的监听端口listenPort=10911#删除文件时间点，默认凌晨 4点deleteWhen=04#文件保留时间，默认 48 小时fileReservedTime=120#commitLog每个文件的大小默认1GmapedFileSizeCommitLog=1073741824#ConsumeQueue每个文件默认存30W条，根据业务情况调整mapedFileSizeConsumeQueue=300000#destroyMapedFileIntervalForcibly=120000#redeleteHangedFileInterval=120000#检测物理文件磁盘空间diskMaxUsedSpaceRatio=88#存储路径storePathRootDir=/usr/local/rocketmq/store#commitLog 存储路径storePathCommitLog=/usr/local/rocketmq/store/commitlog#消费队列存储路径存储路径storePathConsumeQueue=/usr/local/rocketmq/store/consumequeue#消息索引存储路径storePathIndex=/usr/local/rocketmq/store/index#checkpoint 文件存储路径storeCheckpoint=/usr/local/rocketmq/store/checkpoint#abort 文件存储路径abortFile=/usr/local/rocketmq/store/abort#限制的消息大小maxMessageSize=65536#flushCommitLogLeastPages=4#flushConsumeQueueLeastPages=2#flushCommitLogThoroughInterval=10000#flushConsumeQueueThoroughInterval=60000#Broker 的角色#- ASYNC_MASTER 异步复制Master#- SYNC_MASTER 同步双写Master#- SLAVEbrokerRole=SYNC_MASTER#刷盘方式#- ASYNC_FLUSH 异步刷盘#- SYNC_FLUSH 同步刷盘flushDiskType=SYNC_FLUSH#checkTransactionMessageEnable=false#发消息线程池数量#sendMessageThreadPoolNums=128#拉消息线程池数量#pullMessageThreadPoolNums=128 slave2服务器：192.168.25.135 1vi /usr/soft/rocketmq/conf/2m-2s-sync/broker-b-s.properties 修改配置如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#所属集群名字brokerClusterName=rocketmq-cluster#broker名字，注意此处不同的配置文件填写的不一样brokerName=broker-b#0 表示 Master，&gt;0 表示 SlavebrokerId=1#nameServer地址，分号分割namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876#在发送消息时，自动创建服务器不存在的topic，默认创建的队列数defaultTopicQueueNums=4#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭autoCreateTopicEnable=true#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭autoCreateSubscriptionGroup=true#Broker 对外服务的监听端口listenPort=11011#删除文件时间点，默认凌晨 4点deleteWhen=04#文件保留时间，默认 48 小时fileReservedTime=120#commitLog每个文件的大小默认1GmapedFileSizeCommitLog=1073741824#ConsumeQueue每个文件默认存30W条，根据业务情况调整mapedFileSizeConsumeQueue=300000#destroyMapedFileIntervalForcibly=120000#redeleteHangedFileInterval=120000#检测物理文件磁盘空间diskMaxUsedSpaceRatio=88#存储路径storePathRootDir=/usr/local/rocketmq/store#commitLog 存储路径storePathCommitLog=/usr/local/rocketmq/store/commitlog#消费队列存储路径存储路径storePathConsumeQueue=/usr/local/rocketmq/store/consumequeue#消息索引存储路径storePathIndex=/usr/local/rocketmq/store/index#checkpoint 文件存储路径storeCheckpoint=/usr/local/rocketmq/store/checkpoint#abort 文件存储路径abortFile=/usr/local/rocketmq/store/abort#限制的消息大小maxMessageSize=65536#flushCommitLogLeastPages=4#flushConsumeQueueLeastPages=2#flushCommitLogThoroughInterval=10000#flushConsumeQueueThoroughInterval=60000#Broker 的角色#- ASYNC_MASTER 异步复制Master#- SYNC_MASTER 同步双写Master#- SLAVEbrokerRole=SLAVE#刷盘方式#- ASYNC_FLUSH 异步刷盘#- SYNC_FLUSH 同步刷盘flushDiskType=ASYNC_FLUSH#checkTransactionMessageEnable=false#发消息线程池数量#sendMessageThreadPoolNums=128#拉消息线程池数量#pullMessageThreadPoolNums=128 master2服务器：192.168.25.138 1vi /usr/soft/rocketmq/conf/2m-2s-sync/broker-b.properties 修改配置如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#所属集群名字brokerClusterName=rocketmq-cluster#broker名字，注意此处不同的配置文件填写的不一样brokerName=broker-b#0 表示 Master，&gt;0 表示 SlavebrokerId=0#nameServer地址，分号分割namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876#在发送消息时，自动创建服务器不存在的topic，默认创建的队列数defaultTopicQueueNums=4#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭autoCreateTopicEnable=true#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭autoCreateSubscriptionGroup=true#Broker 对外服务的监听端口listenPort=10911#删除文件时间点，默认凌晨 4点deleteWhen=04#文件保留时间，默认 48 小时fileReservedTime=120#commitLog每个文件的大小默认1GmapedFileSizeCommitLog=1073741824#ConsumeQueue每个文件默认存30W条，根据业务情况调整mapedFileSizeConsumeQueue=300000#destroyMapedFileIntervalForcibly=120000#redeleteHangedFileInterval=120000#检测物理文件磁盘空间diskMaxUsedSpaceRatio=88#存储路径storePathRootDir=/usr/local/rocketmq/store#commitLog 存储路径storePathCommitLog=/usr/local/rocketmq/store/commitlog#消费队列存储路径存储路径storePathConsumeQueue=/usr/local/rocketmq/store/consumequeue#消息索引存储路径storePathIndex=/usr/local/rocketmq/store/index#checkpoint 文件存储路径storeCheckpoint=/usr/local/rocketmq/store/checkpoint#abort 文件存储路径abortFile=/usr/local/rocketmq/store/abort#限制的消息大小maxMessageSize=65536#flushCommitLogLeastPages=4#flushConsumeQueueLeastPages=2#flushCommitLogThoroughInterval=10000#flushConsumeQueueThoroughInterval=60000#Broker 的角色#- ASYNC_MASTER 异步复制Master#- SYNC_MASTER 同步双写Master#- SLAVEbrokerRole=SYNC_MASTER#刷盘方式#- ASYNC_FLUSH 异步刷盘#- SYNC_FLUSH 同步刷盘flushDiskType=SYNC_FLUSH#checkTransactionMessageEnable=false#发消息线程池数量#sendMessageThreadPoolNums=128#拉消息线程池数量#pullMessageThreadPoolNums=128 slave1服务器：192.168.25.138 1vi /usr/soft/rocketmq/conf/2m-2s-sync/broker-a-s.properties 修改配置如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#所属集群名字brokerClusterName=rocketmq-cluster#broker名字，注意此处不同的配置文件填写的不一样brokerName=broker-a#0 表示 Master，&gt;0 表示 SlavebrokerId=1#nameServer地址，分号分割namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876#在发送消息时，自动创建服务器不存在的topic，默认创建的队列数defaultTopicQueueNums=4#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭autoCreateTopicEnable=true#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭autoCreateSubscriptionGroup=true#Broker 对外服务的监听端口listenPort=11011#删除文件时间点，默认凌晨 4点deleteWhen=04#文件保留时间，默认 48 小时fileReservedTime=120#commitLog每个文件的大小默认1GmapedFileSizeCommitLog=1073741824#ConsumeQueue每个文件默认存30W条，根据业务情况调整mapedFileSizeConsumeQueue=300000#destroyMapedFileIntervalForcibly=120000#redeleteHangedFileInterval=120000#检测物理文件磁盘空间diskMaxUsedSpaceRatio=88#存储路径storePathRootDir=/usr/local/rocketmq/store#commitLog 存储路径storePathCommitLog=/usr/local/rocketmq/store/commitlog#消费队列存储路径存储路径storePathConsumeQueue=/usr/local/rocketmq/store/consumequeue#消息索引存储路径storePathIndex=/usr/local/rocketmq/store/index#checkpoint 文件存储路径storeCheckpoint=/usr/local/rocketmq/store/checkpoint#abort 文件存储路径abortFile=/usr/local/rocketmq/store/abort#限制的消息大小maxMessageSize=65536#flushCommitLogLeastPages=4#flushConsumeQueueLeastPages=2#flushCommitLogThoroughInterval=10000#flushConsumeQueueThoroughInterval=60000#Broker 的角色#- ASYNC_MASTER 异步复制Master#- SYNC_MASTER 同步双写Master#- SLAVEbrokerRole=SLAVE#刷盘方式#- ASYNC_FLUSH 异步刷盘#- SYNC_FLUSH 同步刷盘flushDiskType=ASYNC_FLUSH#checkTransactionMessageEnable=false#发消息线程池数量#sendMessageThreadPoolNums=128#拉消息线程池数量#pullMessageThreadPoolNums=128 修改启动脚本文件runbroker.sh1vi /usr/local/rocketmq/bin/runbroker.sh 需要根据内存大小进行适当的对JVM参数进行调整： 123#===================================================# 开发环境配置 JVM ConfigurationJAVA_OPT="$&#123;JAVA_OPT&#125; -server -Xms256m -Xmx256m -Xmn128m" runserver.sh1vim /usr/local/rocketmq/bin/runserver.sh 1JAVA_OPT="$&#123;JAVA_OPT&#125; -server -Xms256m -Xmx256m -Xmn128m -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m" 服务启动启动NameServe集群分别在192.168.25.135和192.168.25.138启动NameServer 12cd /usr/local/rocketmq/binnohup sh mqnamesrv &amp; 启动Broker集群 在192.168.25.135上启动master1和slave2 master1： 12cd /usr/local/rocketmq/binnohup sh mqbroker -c /usr/local/rocketmq/conf/2m-2s-syncbroker-a.properties &amp; slave2： 12cd /usr/local/rocketmq/binnohup sh mqbroker -c /usr/local/rocketmq/conf/2m-2s-sync/broker-b-s.properties &amp; 在192.168.25.138上启动master2和slave2 master2 12cd /usr/local/rocketmq/binnohup sh mqbroker -c /usr/local/rocketmq/conf/2m-2s-sync/broker-b.properties &amp; slave1 12cd /usr/local/rocketmq/binnohup sh mqbroker -c /usr/local/rocketmq/conf/2m-2s-sync/broker-a-s.properties &amp; 查看进程状态启动后通过JPS查看启动进程 查看日志1234# 查看nameServer日志tail -500f ~/logs/rocketmqlogs/namesrv.log# 查看broker日志tail -500f ~/logs/rocketmqlogs/broker.log 注意事项 几乎所有命令都需要配置-n表示NameServer地址，格式为ip:port 几乎所有命令都可以通过-h获取帮助 如果既有Broker地址（-b）配置项又有clusterName（-c）配置项，则优先以Broker地址执行命令；如果不配置Broker地址，则对集群中所有主机执行命令 集群监控平台搭建概述RocketMQ有一个对其扩展的开源项目incubator-rocketmq-externals，这个项目中有一个子模块叫rocketmq-console，这个便是管理控制台项目了，先将incubator-rocketmq-externals拉到本地，因为我们需要自己对rocketmq-console进行编译打包运行。 下载并编译打包123git clone https://github.com/apache/rocketmq-externalscd rocketmq-consolemvn clean package -Dmaven.test.skip=true 注意：打包前在123```shrocketmq.config.namesrvAddr=192.168.25.135:9876;192.168.25.138:9876 启动rocketmq-console： 1java -jar rocketmq-console-ng-1.0.0.jar 启动成功后，我们就可以通过浏览器访问http://localhost:8080进入控制台界面了，如下图： 集群状态： 消息发送样例 导入MQ客户端依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt; &lt;artifactId&gt;rocketmq-client&lt;/artifactId&gt; &lt;version&gt;4.4.0&lt;/version&gt;&lt;/dependency&gt; 消息发送者步骤分析r 1234561.创建消息生产者producer，并制定生产者组名2.指定Nameserver地址3.启动producer4.创建消息对象，指定主题Topic、Tag和消息体5.发送消息6.关闭生产者producer 消息消费者步骤分析 123451.创建消费者Consumer，制定消费者组名2.指定Nameserver地址3.订阅主题Topic和Tag4.设置回调函数，处理消息5.启动消费者consumer 基本样例消息发送发送同步消息这种可靠性同步地发送方式使用的比较广泛，比如：重要的消息通知，短信通知。 1234567891011121314151617181920212223public class SyncProducer &#123; public static void main(String[] args) throws Exception &#123; // 实例化消息生产者Producer DefaultMQProducer producer = new DefaultMQProducer("please_rename_unique_group_name"); // 设置NameServer的地址 producer.setNamesrvAddr("localhost:9876"); // 启动Producer实例 producer.start(); for (int i = 0; i &lt; 100; i++) &#123; // 创建消息，并指定Topic，Tag和消息体 Message msg = new Message("TopicTest" /* Topic */, "TagA" /* Tag */, ("Hello RocketMQ " + i).getBytes(RemotingHelper.DEFAULT_CHARSET) /* Message body */ ); // 发送消息到一个Broker SendResult sendResult = producer.send(msg); // 通过sendResult返回消息是否成功送达 System.out.printf("%s%n", sendResult); &#125; // 如果不再发送消息，关闭Producer实例。 producer.shutdown(); &#125;&#125; 发送异步消息异步消息通常用在对响应时间敏感的业务场景，即发送端不能容忍长时间地等待Broker的响应。 12345678910111213141516171819202122232425262728293031323334public class AsyncProducer &#123; public static void main(String[] args) throws Exception &#123; // 实例化消息生产者Producer DefaultMQProducer producer = new DefaultMQProducer("please_rename_unique_group_name"); // 设置NameServer的地址 producer.setNamesrvAddr("localhost:9876"); // 启动Producer实例 producer.start(); producer.setRetryTimesWhenSendAsyncFailed(0); for (int i = 0; i &lt; 100; i++) &#123; final int index = i; // 创建消息，并指定Topic，Tag和消息体 Message msg = new Message("TopicTest", "TagA", "OrderID188", "Hello world".getBytes(RemotingHelper.DEFAULT_CHARSET)); // SendCallback接收异步返回结果的回调 producer.send(msg, new SendCallback() &#123; @Override public void onSuccess(SendResult sendResult) &#123; System.out.printf("%-10d OK %s %n", index, sendResult.getMsgId()); &#125; @Override public void onException(Throwable e) &#123; System.out.printf("%-10d Exception %s %n", index, e); e.printStackTrace(); &#125; &#125;); &#125; // 如果不再发送消息，关闭Producer实例。 producer.shutdown(); &#125;&#125; 单向发送消息这种方式主要用在不特别关心发送结果的场景，例如日志发送。 12345678910111213141516171819202122public class OnewayProducer &#123; public static void main(String[] args) throws Exception&#123; // 实例化消息生产者Producer DefaultMQProducer producer = new DefaultMQProducer("please_rename_unique_group_name"); // 设置NameServer的地址 producer.setNamesrvAddr("localhost:9876"); // 启动Producer实例 producer.start(); for (int i = 0; i &lt; 100; i++) &#123; // 创建消息，并指定Topic，Tag和消息体 Message msg = new Message("TopicTest" /* Topic */, "TagA" /* Tag */, ("Hello RocketMQ " + i).getBytes(RemotingHelper.DEFAULT_CHARSET) /* Message body */ ); // 发送单向消息，没有任何返回结果 producer.sendOneway(msg); &#125; // 如果不再发送消息，关闭Producer实例。 producer.shutdown(); &#125;&#125; 消费消息负载均衡模式消费者采用负载均衡方式消费消息，多个消费者共同消费队列消息，每个消费者处理的消息不同 1234567891011121314151617181920212223public static void main(String[] args) throws Exception &#123; // 实例化消息生产者,指定组名 DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("group1"); // 指定Namesrv地址信息. consumer.setNamesrvAddr("localhost:9876"); // 订阅Topic consumer.subscribe("Test", "*"); //负载均衡模式消费 consumer.setMessageModel(MessageModel.CLUSTERING); // 注册回调函数，处理消息 consumer.registerMessageListener(new MessageListenerConcurrently() &#123; @Override public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123; System.out.printf("%s Receive New Messages: %s %n", Thread.currentThread().getName(), msgs); return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125; &#125;); //启动消息者 consumer.start(); System.out.printf("Consumer Started.%n");&#125; 广播模式消费者采用广播的方式消费消息，每个消费者消费的消息都是相同的 1234567891011121314151617181920212223public static void main(String[] args) throws Exception &#123; // 实例化消息生产者,指定组名 DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("group1"); // 指定Namesrv地址信息. consumer.setNamesrvAddr("localhost:9876"); // 订阅Topic consumer.subscribe("Test", "*"); //广播模式消费 consumer.setMessageModel(MessageModel.BROADCASTING); // 注册回调函数，处理消息 consumer.registerMessageListener(new MessageListenerConcurrently() &#123; @Override public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123; System.out.printf("%s Receive New Messages: %s %n", Thread.currentThread().getName(), msgs); return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125; &#125;); //启动消息者 consumer.start(); System.out.printf("Consumer Started.%n");&#125; 顺序消息消息有序指的是可以按照消息的发送顺序来消费(FIFO)。RocketMQ可以严格的保证消息有序，可以分为分区有序或者全局有序。 顺序消费的原理解析，在默认的情况下消息发送会采取Round Robin轮询方式把消息发送到不同的queue(分区队列)；而消费消息的时候从多个queue上拉取消息，这种情况发送和消费是不能保证顺序。但是如果控制发送的顺序消息只依次发送到同一个queue中，消费的时候只从这个queue上依次拉取，则就保证了顺序。当发送和消费参与的queue只有一个，则是全局有序；如果多个queue参与，则为分区有序，即相对每个queue，消息都是有序的。 下面用订单进行分区有序的示例。一个订单的顺序流程是：创建、付款、推送、完成。订单号相同的消息会被先后发送到同一个队列中，消费时，同一个OrderId获取到的肯定是同一个队列。 顺序消息生产123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134/*** Producer，发送顺序消息*/public class Producer &#123; public static void main(String[] args) throws Exception &#123; DefaultMQProducer producer = new DefaultMQProducer("please_rename_unique_group_name"); producer.setNamesrvAddr("127.0.0.1:9876"); producer.start(); String[] tags = new String[]&#123;"TagA", "TagC", "TagD"&#125;; // 订单列表 List&lt;OrderStep&gt; orderList = new Producer().buildOrders(); Date date = new Date(); SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); String dateStr = sdf.format(date); for (int i = 0; i &lt; 10; i++) &#123; // 加个时间前缀 String body = dateStr + " Hello RocketMQ " + orderList.get(i); Message msg = new Message("TopicTest", tags[i % tags.length], "KEY" + i, body.getBytes()); SendResult sendResult = producer.send(msg, new MessageQueueSelector() &#123; @Override public MessageQueue select(List&lt;MessageQueue&gt; mqs, Message msg, Object arg) &#123; Long id = (Long) arg; //根据订单id选择发送queue long index = id % mqs.size(); return mqs.get((int) index); &#125; &#125;, orderList.get(i).getOrderId());//订单id System.out.println(String.format("SendResult status:%s, queueId:%d, body:%s", sendResult.getSendStatus(), sendResult.getMessageQueue().getQueueId(), body)); &#125; producer.shutdown(); &#125; /** * 订单的步骤 */ private static class OrderStep &#123; private long orderId; private String desc; public long getOrderId() &#123; return orderId; &#125; public void setOrderId(long orderId) &#123; this.orderId = orderId; &#125; public String getDesc() &#123; return desc; &#125; public void setDesc(String desc) &#123; this.desc = desc; &#125; @Override public String toString() &#123; return "OrderStep&#123;" + "orderId=" + orderId + ", desc='" + desc + '\'' + '&#125;'; &#125; &#125; /** * 生成模拟订单数据 */ private List&lt;OrderStep&gt; buildOrders() &#123; List&lt;OrderStep&gt; orderList = new ArrayList&lt;OrderStep&gt;(); OrderStep orderDemo = new OrderStep(); orderDemo.setOrderId(15103111039L); orderDemo.setDesc("创建"); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103111065L); orderDemo.setDesc("创建"); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103111039L); orderDemo.setDesc("付款"); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103117235L); orderDemo.setDesc("创建"); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103111065L); orderDemo.setDesc("付款"); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103117235L); orderDemo.setDesc("付款"); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103111065L); orderDemo.setDesc("完成"); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103111039L); orderDemo.setDesc("推送"); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103117235L); orderDemo.setDesc("完成"); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103111039L); orderDemo.setDesc("完成"); orderList.add(orderDemo); return orderList; &#125;&#125; 顺序消费消息1234567891011121314151617181920212223242526272829303132333435363738394041424344/*** 顺序消息消费，带事务方式（应用可控制Offset什么时候提交）*/public class ConsumerInOrder &#123; public static void main(String[] args) throws Exception &#123; DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("please_rename_unique_group_name_3"); consumer.setNamesrvAddr("127.0.0.1:9876"); /** * 设置Consumer第一次启动是从队列头部开始消费还是队列尾部开始消费&lt;br&gt; * 如果非第一次启动，那么按照上次消费的位置继续消费 */ consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET); consumer.subscribe("TopicTest", "TagA || TagC || TagD"); consumer.registerMessageListener(new MessageListenerOrderly() &#123; Random random = new Random(); @Override public ConsumeOrderlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeOrderlyContext context) &#123; context.setAutoCommit(true); for (MessageExt msg : msgs) &#123; // 可以看到每个queue有唯一的consume线程来消费, 订单对每个queue(分区)有序 System.out.println("consumeThread=" + Thread.currentThread().getName() + "queueId=" + msg.getQueueId() + ", content:" + new String(msg.getBody())); &#125; try &#123; //模拟业务逻辑处理中... TimeUnit.SECONDS.sleep(random.nextInt(10)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return ConsumeOrderlyStatus.SUCCESS; &#125; &#125;); consumer.start(); System.out.println("Consumer Started."); &#125;&#125; 延时消息比如电商里，提交了一个订单就可以发送一个延时消息，1h后去检查这个订单的状态，如果还是未付款就取消订单释放库存。 启动消息消费者123456789101112131415161718192021public class ScheduledMessageConsumer &#123; public static void main(String[] args) throws Exception &#123; // 实例化消费者 DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("ExampleConsumer"); // 订阅Topics consumer.subscribe("TestTopic", "*"); // 注册消息监听者 consumer.registerMessageListener(new MessageListenerConcurrently() &#123; @Override public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; messages, ConsumeConcurrentlyContext context) &#123; for (MessageExt message : messages) &#123; // Print approximate delay time period System.out.println("Receive message[msgId=" + message.getMsgId() + "] " + (System.currentTimeMillis() - message.getStoreTimestamp()) + "ms later"); &#125; return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125; &#125;); // 启动消费者 consumer.start(); &#125;&#125; 发送延时消息123456789101112131415161718public class ScheduledMessageProducer &#123; public static void main(String[] args) throws Exception &#123; // 实例化一个生产者来产生延时消息 DefaultMQProducer producer = new DefaultMQProducer("ExampleProducerGroup"); // 启动生产者 producer.start(); int totalMessagesToSend = 100; for (int i = 0; i &lt; totalMessagesToSend; i++) &#123; Message message = new Message("TestTopic", ("Hello scheduled message " + i).getBytes()); // 设置延时等级3,这个消息将在10s之后发送(现在只支持固定的几个时间,详看delayTimeLevel) message.setDelayTimeLevel(3); // 发送消息 producer.send(message); &#125; // 关闭生产者 producer.shutdown(); &#125;&#125; 验证您将会看到消息的消费比存储时间晚10秒 使用限制12// org/apache/rocketmq/store/config/MessageStoreConfig.javaprivate String messageDelayLevel = "1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h"; 现在RocketMq并不支持任意时间的延时，需要设置几个固定的延时等级，从1s到2h分别对应着等级1到18 批量消息批量发送消息能显著提高传递小消息的性能。限制是这些批量消息应该有相同的topic，相同的waitStoreMsgOK，而且不能是延时消息。此外，这一批消息的总大小不应超过4MB。 发送批量消息如果您每次只发送不超过4MB的消息，则很容易使用批处理，样例如下： 1234567891011String topic = "BatchTest";List&lt;Message&gt; messages = new ArrayList&lt;&gt;();messages.add(new Message(topic, "TagA", "OrderID001", "Hello world 0".getBytes()));messages.add(new Message(topic, "TagA", "OrderID002", "Hello world 1".getBytes()));messages.add(new Message(topic, "TagA", "OrderID003", "Hello world 2".getBytes()));try &#123; producer.send(messages);&#125; catch (Exception e) &#123; e.printStackTrace(); //处理error&#125; 如果消息的总长度可能大于4MB时，这时候最好把消息进行分割 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class ListSplitter implements Iterator&lt;List&lt;Message&gt;&gt; &#123; private final int SIZE_LIMIT = 1024 * 1024 * 4; private final List&lt;Message&gt; messages; private int currIndex; public ListSplitter(List&lt;Message&gt; messages) &#123; this.messages = messages; &#125; @Override public boolean hasNext() &#123; return currIndex &lt; messages.size(); &#125; @Override public List&lt;Message&gt; next() &#123; int nextIndex = currIndex; int totalSize = 0; for (; nextIndex &lt; messages.size(); nextIndex++) &#123; Message message = messages.get(nextIndex); int tmpSize = message.getTopic().length() + message.getBody().length; Map&lt;String, String&gt; properties = message.getProperties(); for (Map.Entry&lt;String, String&gt; entry : properties.entrySet()) &#123; tmpSize += entry.getKey().length() + entry.getValue().length(); &#125; tmpSize = tmpSize + 20; // 增加日志的开销20字节 if (tmpSize &gt; SIZE_LIMIT) &#123; //单个消息超过了最大的限制 //忽略,否则会阻塞分裂的进程 if (nextIndex - currIndex == 0) &#123; //假如下一个子列表没有元素,则添加这个子列表然后退出循环,否则只是退出循环 nextIndex++; &#125; break; &#125; if (tmpSize + totalSize &gt; SIZE_LIMIT) &#123; break; &#125; else &#123; totalSize += tmpSize; &#125; &#125; List&lt;Message&gt; subList = messages.subList(currIndex, nextIndex); currIndex = nextIndex; return subList; &#125;&#125;//把大的消息分裂成若干个小的消息ListSplitter splitter = new ListSplitter(messages);while (splitter.hasNext()) &#123; try &#123; List&lt;Message&gt; listItem = splitter.next(); producer.send(listItem); &#125; catch (Exception e) &#123; e.printStackTrace(); //处理error &#125;&#125; 过滤消息在大多数情况下，TAG是一个简单而有用的设计，其可以来选择您想要的消息。例如： 12DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("CID_EXAMPLE");consumer.subscribe("TOPIC", "TAGA || TAGB || TAGC"); 消费者将接收包含TAGA或TAGB或TAGC的消息。但是限制是一个消息只能有一个标签，这对于复杂的场景可能不起作用。在这种情况下，可以使用SQL表达式筛选消息。SQL特性可以通过发送消息时的属性来进行计算。在RocketMQ定义的语法下，可以实现一些简单的逻辑。下面是一个例子： 1234567891011121314------------| message ||----------| a &gt; 5 AND b = &apos;abc&apos;| a = 10 | --------------------&gt; Gotten| b = &apos;abc&apos;|| c = true |------------------------| message ||----------| a &gt; 5 AND b = &apos;abc&apos;| a = 1 | --------------------&gt; Missed| b = &apos;abc&apos;|| c = true |------------ SQL基本语法RocketMQ只定义了一些基本语法来支持这个特性。你也可以很容易地扩展它。 数值比较，比如：&gt;，&gt;=，&lt;，&lt;=，BETWEEN，=； 字符比较，比如：=，&lt;&gt;，IN； IS NULL 或者 IS NOT NULL； 逻辑符号 AND，OR，NOT； 常量支持类型为： 数值，比如：123，3.1415； 字符，比如：‘abc’，必须用单引号包裹起来； NULL，特殊的常量 布尔值，TRUE 或 FALSE 只有使用push模式的消费者才能用使用SQL92标准的sql语句，接口如下： 1public void subscribe(finalString topic, final MessageSelector messageSelector) 消息生产者发送消息时，你能通过putUserProperty来设置消息的属性 1234567891011DefaultMQProducer producer = new DefaultMQProducer("please_rename_unique_group_name");producer.start();Message msg = new Message("TopicTest", tag, ("Hello RocketMQ " + i).getBytes(RemotingHelper.DEFAULT_CHARSET));// 设置一些属性msg.putUserProperty("a", String.valueOf(i));SendResult sendResult = producer.send(msg);producer.shutdown(); 消息消费者用MessageSelector.bySql来使用sql筛选消息 12345678910DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("please_rename_unique_group_name_4");// 只有订阅的消息有这个属性a, a &gt;=0 and a &lt;= 3consumer.subscribe("TopicTest", MessageSelector.bySql("a between 0 and 3");consumer.registerMessageListener(new MessageListenerConcurrently() &#123; @Override public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123; return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125;&#125;);consumer.start(); 事务消息流程分析 上图说明了事务消息的大致方案，其中分为两个流程：正常事务消息的发送及提交、事务消息的补偿流程。 事务消息发送及提交(1) 发送消息（half消息）。 (2) 服务端响应消息写入结果。 (3) 根据发送结果执行本地事务（如果写入失败，此时half消息对业务不可见，本地逻辑不执行）。 (4) 根据本地事务状态执行Commit或者Rollback（Commit操作生成消息索引，消息对消费者可见） 事务补偿(1) 对没有Commit/Rollback的事务消息（pending状态的消息），从服务端发起一次“回查” (2) Producer收到回查消息，检查回查消息对应的本地事务的状态 (3) 根据本地事务状态，重新Commit或者Rollback 其中，补偿阶段用于解决消息Commit或者Rollback发生超时或者失败的情况。 事务消息状态事务消息共有三种状态，提交状态、回滚状态、中间状态： TransactionStatus.CommitTransaction: 提交事务，它允许消费者消费此消息。 TransactionStatus.RollbackTransaction: 回滚事务，它代表该消息将被删除，不允许被消费。 TransactionStatus.Unknown: 中间状态，它代表需要检查消息队列来确定状态。 发送事务消息创建事务性生产者使用 TransactionMQProducer类创建生产者，并指定唯一的 ProducerGroup，就可以设置自定义线程池来处理这些检查请求。执行本地事务后、需要根据执行结果对消息队列进行回复。回传的事务状态在请参考前一节。 1234567891011121314151617181920212223242526public class Producer &#123; public static void main(String[] args) throws MQClientException, InterruptedException &#123; //创建事务监听器 TransactionListener transactionListener = new TransactionListenerImpl(); //创建消息生产者 TransactionMQProducer producer = new TransactionMQProducer("group6"); producer.setNamesrvAddr("192.168.25.135:9876;192.168.25.138:9876"); //生产者这是监听器 producer.setTransactionListener(transactionListener); //启动消息生产者 producer.start(); String[] tags = new String[]&#123;"TagA", "TagB", "TagC"&#125;; for (int i = 0; i &lt; 3; i++) &#123; try &#123; Message msg = new Message("TransactionTopic", tags[i % tags.length], "KEY" + i, ("Hello RocketMQ " + i).getBytes(RemotingHelper.DEFAULT_CHARSET)); SendResult sendResult = producer.sendMessageInTransaction(msg, null); System.out.printf("%s%n", sendResult); TimeUnit.SECONDS.sleep(1); &#125; catch (MQClientException | UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; &#125; //producer.shutdown(); &#125;&#125; 实现事务的监听接口当发送半消息成功时，我们使用 executeLocalTransaction 方法来执行本地事务。它返回前一节中提到的三个事务状态之一。checkLocalTranscation 方法用于检查本地事务状态，并回应消息队列的检查请求。它也是返回前一节中提到的三个事务状态之一。 123456789101112131415161718192021public class TransactionListenerImpl implements TransactionListener &#123; @Override public LocalTransactionState executeLocalTransaction(Message msg, Object arg) &#123; System.out.println("执行本地事务"); if (StringUtils.equals("TagA", msg.getTags())) &#123; return LocalTransactionState.COMMIT_MESSAGE; &#125; else if (StringUtils.equals("TagB", msg.getTags())) &#123; return LocalTransactionState.ROLLBACK_MESSAGE; &#125; else &#123; return LocalTransactionState.UNKNOW; &#125; &#125; @Override public LocalTransactionState checkLocalTransaction(MessageExt msg) &#123; System.out.println("MQ检查消息Tag【"+msg.getTags()+"】的本地事务执行结果"); return LocalTransactionState.COMMIT_MESSAGE; &#125;&#125; 使用限制 事务消息不支持延时消息和批量消息。 为了避免单个消息被检查太多次而导致半队列消息累积，我们默认将单个消息的检查次数限制为 15 次，但是用户可以通过 Broker 配置文件的 transactionCheckMax参数来修改此限制。如果已经检查某条消息超过 N 次的话（ N = transactionCheckMax ） 则 Broker 将丢弃此消息，并在默认情况下同时打印错误日志。用户可以通过重写 AbstractTransactionCheckListener 类来修改这个行为。 事务消息将在 Broker 配置文件中的参数 transactionMsgTimeout 这样的特定时间长度之后被检查。当发送事务消息时，用户还可以通过设置用户属性 CHECK_IMMUNITY_TIME_IN_SECONDS 来改变这个限制，该参数优先于 transactionMsgTimeout 参数。 事务性消息可能不止一次被检查或消费。 提交给用户的目标主题消息可能会失败，目前这依日志的记录而定。它的高可用性通过 RocketMQ 本身的高可用性机制来保证，如果希望确保事务消息不丢失、并且事务完整性得到保证，建议使用同步的双重写入机制。 事务消息的生产者 ID 不能与其他类型消息的生产者 ID 共享。与其他类型的消息不同，事务消息允许反向查询、MQ服务器能通过它们的生产者 ID 查询到消费者。]]></content>
      <categories>
        <category>RocketMQ</category>
      </categories>
      <tags>
        <tag>RocketMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一 Before start kernel]]></title>
    <url>%2FLinuxKernel4-0-arm32%2Flinuxkernel4.0%2Fbefore-start-kernel%2F</url>
    <content type="text"><![CDATA[Qemu 调试 详见 《奔跑吧 Linux内核》，有详细的 Qemu 调试内核的过程。 Qemu 调试配置123456789101112131415161718192021222324# 准备环境$ sudo apt-get install qemu libncurses5-dev gcc-arm-linux-gnueabi build-essential git gdb-arm-none-eabi# 下载仓库$ git clone git@github.com:figozhang/runninglinuxkernel_4.0.git$ git checkout rlk_basic# 编译内核$ cd runninglinuxkernel-4.0$ export ARCH=arm$ export CROSS_COMPILE=arm-linux-gnueabi-$ make vexpress_defconfig$ make menuconfig# 在 _install_arm32/dev 下创建设备节点$ cd _install_arm32$ mkdir dev$ cd dev$ sudo mknod console c 5 1# 开始编译$ make bzImage -j4$ make dtbs# 一个 session 打开 ， 退出 ctrl+a+x, killall qemu-system-arm$ ./run.sh arm32 debug # 另一个 session 调试 Qemu 调试 mmu 之前在 Qemu 调试过程中分为两个阶段: mmu 启动前; mmu启动后。因此，必须要了解链接地址与装载地址之间的差别。对于vexpress板子而言，其dts文件在arch/arm/boot/dts/vexpress-v2p-ca9.dts，其物理内存从0x6000_0000开始。 因此，为了能够调试 mmu 启动前的代码，需要将符号表加载到能够访问的物理地址。 123456789// arm-none-eabi-gdb --tui 启动调试$ target remote localhost:1234$ load vmlinux -0x60000000$ add-symbol-file vmlinux 0x60008280 -s .head.text 0x60008000 -s .rodata 0x60b18000 // 通过 arm-none-eabi-readelf -S vmlinux 查看对应的 .text off: 008280; .head.text off: 008000; .rodata off: b18000。$ set $pc=0x60008000$ set $r2=0x63200000 // 调试 dtb 时使用$ b stext$ c 概述 为什么需要关闭 MMU? 内核需要自己创建页表，在创建页表前，内核实际运行在物理地址，必须关闭 MMU。为什么 D-cache 关闭？因为内核运行到 stext 时，若 D-cache 开始，则会从 cache 中获取数据，可能缓存 uboot 或 解压时留下的数据，这部分数据是无效的，可能导致数据异常。 此时系统满足的条件 MMU=off, D-cache=off, C-cache=don’t care r0=0, r1=machine nr, r2=atags or dtb pointer 内核装载后，pc 指向 stext 所在物理地址 0x6000_8000，执行内核代码。此时，内核页表、c程序执行的堆栈、必要硬件配置还未设置完成，因此无法使用 c 语言程序，且只能使用地址无关的汇编代码(bl等)。 开启 SVC 关闭中断cpsr 寄存器 __hyp_stub_install12345678// arch/arm/kernel/hyp-stup.S:40.macro store_primary_cpu_mode reg1, reg2, reg3mrs \reg1, cpsrand \reg1, \reg1, #MODE_MASK @ MODE_MASK=1f, 获取 cpsr低5位数据adr \reg2, .L__boot_cpu_mode_offset @ 读取 .L__boot_cpu_mode_offset 所在内存地址ldr \reg3, [\reg2] @ __boot_cpu_mode 存放地址str \reg1, [\reg2, \reg3] @ 将当前运行模式保存到 __boot_cpu_mode 之中.endm 由上面代码分析可知，store_primary_cpu_mode所要做的事情便是从 cpsr 状态寄存器中读取 boot 启动后的运行模式，然后将其存储到 __boot_cpu_mode 变量中。 123456789// arch/arm/kernel/hyp-stup.S:54.macro compare_cpu_mode_with_primary mode, reg1, reg2, reg3adr \reg2, .L__boot_cpu_mode_offsetldr \reg3, [\reg2]ldr \reg1, [\reg2, \reg3]cmp \mode, \reg1 @ matches primary CPU boot mode?orrne \reg1, \reg1, #BOOT_CPU_MODE_MISMATCH @ 不等strne \reg1, [\reg2, \reg3] @ record what happened and give up.endm 比较运行模式是否与 __boot_cpu_mode 一致 1234567891011// arch/arm/kernel/hyp-stup.S:87ENTRY(__hyp_stub_install) store_primary_cpu_mode r4, r5, r6ENDPROC(__hyp_stub_install)ENTRY(__hyp_stub_install_secondary) mrs r4, cpsr and r4, r4, #MODE_MASK compare_cpu_mode_with_primary r4, r5, r6, r7 @ 比较运行模式是否与 __boot_cpu_mode 一致 retne lr cmp r4, #HYP_MODE retne lr @ give up if the CPU is not in HYP mode, 默认是 svc 模式，若当前为 HYP_MODE 则继续运行，但是在本案例中，默认启动为 svc 综上，该函数做了两件事情： 通过 store_primary_cpu_mode 设置 __boot_cpu_mode 变量 对比当前运行模式是否为 __boot_cpu_mode safe_svcmode_maskall1234567891011121314151617181920// arch/arm/include/asm/assembler.h.macro safe_svcmode_maskall reg:req#if __LINUX_ARM_ARCH__ &gt;= 6 &amp;&amp; !defined(CONFIG_CPU_V7M) mrs \reg , cpsr eor \reg, \reg, #HYP_MODE tst \reg, #MODE_MASK bic \reg , \reg , #MODE_MASK @ 清0位 orr \reg , \reg , #PSR_I_BIT | PSR_F_BIT | SVC_MODE @ 设置 svc 模式以及关闭中断THUMB( orr \reg , \reg , #PSR_T_BIT ) bne 1f orr \reg, \reg, #PSR_A_BIT adr lr, BSYM(2f) msr spsr_cxsf, \reg __MSR_ELR_HYP(14) __ERET1: msr cpsr_c, \reg @ 写入状态寄存器2:#else#endif.endm 1234// arch/arm/include/uapi/asm/ptrace.h#define SVC_MODE 0x00000013#define PSR_F_BIT 0x00000040#define PSR_I_BIT 0x00000080 总结最开始的两个函数，确保了程序运行在 svc 模式，并且关闭了中断。 其核心代码：1234mrs \reg , cpsrbic \reg , \reg , #MODE_MASK @ 清0位orr \reg , \reg , #PSR_I_BIT | PSR_F_BIT | SVC_MODE @ 设置 svc 模式以及关闭中断msr cpsr_c, \reg @ 写入状态寄 获取 proc info123456// arch/arm/kernel/head.S: 98 mrc p15, 0, r9, c0, c0 @ get processor id, r9 记录 cpu 的信息 bl __lookup_processor_type @ r5 = procinfo cpuid arch/arm/include/asm/procinfo.h struct proc_info_list 描述 cpu 信息, 定义在 arch/arm/mm/proc-v7.S movs r10, r5 @ invalid processor (r5=0)? THUMB( it eq ) @ force fixup-able long branch encoding beq __error_p @ yes, error 'p' procinfo 前提知识 说明procinfo 使用proc_info_list结构体，用于说明一个 cpu 的信息，包括 cpu 的 ID 号，对应的内核数据映射区的 MMU 标志等。 数据结构定义arch/arm/include/asm/procinfo.h 123456789101112131415struct proc_info_list &#123; unsigned int cpu_val; // cpu id unsigned int cpu_mask; // cpu id 掩码 unsigned long __cpu_mm_mmu_flags; /* used by head.S 临时页表映射的内核空间 MMU 标识 */ unsigned long __cpu_io_mmu_flags; /* used by head.S IO 映射区的 MMU 标识 */ unsigned long __cpu_flush; /* used by head.S CPU setup 函数的地址，后续在打开 MMU 过程中使用 */ const char *arch_name; const char *elf_name; unsigned int elf_hwcap; const char *cpu_name; struct processor *proc; struct cpu_tlb_fns *tlb; struct cpu_user_fns *user; struct cpu_cache_fns *cache;&#125;; 存放地址对于本案例而言数据存储在 arch/arm/mm/proc-v7.S:503文件中，为 Cortex A9 处理器。 12345678910111213141516171819202122232425262728293031323334465: .section ".proc.info.init", #alloc, #execinstr467: /*468: * Standard v7 proc info content469: */470: .macro __v7_proc initfunc, mm_mmuflags = 0, io_mmuflags = 0, hwcaps = 0, proc_fns = v7_processor_functions471: ALT_SMP(.long PMD_TYPE_SECT | PMD_SECT_AP_WRITE | PMD_SECT_AP_READ | \472: PMD_SECT_AF | PMD_FLAGS_SMP | \mm_mmuflags)473: ALT_UP(.long PMD_TYPE_SECT | PMD_SECT_AP_WRITE | PMD_SECT_AP_READ | \474: PMD_SECT_AF | PMD_FLAGS_UP | \mm_mmuflags)475: .long PMD_TYPE_SECT | PMD_SECT_AP_WRITE | \476: PMD_SECT_AP_READ | PMD_SECT_AF | \io_mmuflags477: W(b) \initfunc478: .long cpu_arch_name479: .long cpu_elf_name480: .long HWCAP_SWP | HWCAP_HALF | HWCAP_THUMB | HWCAP_FAST_MULT | \481: HWCAP_EDSP | HWCAP_TLS | \hwcaps482: .long cpu_v7_name483: .long \proc_fns484: .long v7wbi_tlb_fns485: .long v6_user_fns486: .long v7_cache_fns487: .endm500: /*501: * ARM Ltd. Cortex A9 processor.502: */503: .type __v7_ca9mp_proc_info, #object504: __v7_ca9mp_proc_info:505: .long 0x410fc090506: .long 0xff0ffff0507: __v7_proc __v7_ca9mp_setup, proc_fns = ca9mp_processor_functions508: .size __v7_ca9mp_proc_info, . - __v7_ca9mp_proc_info 链接arch/arm/kernel/vmlinux.lds.S会将arch/arm/mm/proc-*.S中所有相关数据都编译到同一段中。 12345678925: #define PROC_INFO \ 26: . = ALIGN(4); \27: VMLINUX_SYMBOL(__proc_info_begin) = .; \28: *(.proc.info.init) \29: VMLINUX_SYMBOL(__proc_info_end) = .;203: .init.proc.info : &#123;204: ARM_CPU_DISCARD(PROC_INFO)205: &#125; 链接后结构通过查看 System.map 文件查看链接地址，可以看到 .init.proc.info 段放了所有相关 cpu 的 procinfo。 123456789101112c0b17c04 T __proc_info_beginc0b17c04 t __v7_ca5mp_proc_infoc0b17c38 t __v7_ca9mp_proc_infoc0b17c6c t __v7_cr7mp_proc_infoc0b17ca0 t __v7_ca7mp_proc_infoc0b17cd4 t __v7_ca12mp_proc_infoc0b17d08 t __v7_ca15mp_proc_infoc0b17d3c t __v7_b15mp_proc_infoc0b17d70 t __v7_ca17mp_proc_infoc0b17da4 t __krait_proc_infoc0b17dd8 t __v7_proc_infoc0b17e0c T __proc_info_end 获取 CPU IDarm 中将 CPU ID 存放在 cp15 的 c0 寄存器 1mrc p15(cp15协处理器), 0(cp15必须为0), r9(写入寄存器), c0(cp15上c0寄存器), c0(默认使用), &#123;,opcode_2(默认为0)&#125; opcode_2=0时访问处理器标识符寄存器。opcode_2=1时访问cache类型标识符寄存器。 运行完成后获取 r9=0x410fc090 获取procinfo12345678910111213141516171819202122232425262728293031323334353637// arch/arm/kernel/head-common.S:152/* * * r9 = cpuid * Returns: * r3, r4, r6 corrupted * r5 = proc_info pointer in physical address space * r9 = cpuid (preserved) */__lookup_processor_type: adr r3, __lookup_processor_type_data @ r3 = __lookup_processor_type_data 所在的物理地址 ldmia r3, &#123;r4 - r6&#125; @ r4 = __lookup_processor_type_data 所在虚拟地址 r5 = __proc_info_begin 虚拟地址, r6 = __proc_info_end 虚拟地址 sub r3, r3, r4 @ get offset between virt&amp;phys add r5, r5, r3 @ convert virt addresses to, r5 = __proc_info_begin 物理地址 add r6, r6, r3 @ physical address space, r6 = __proc_info_end 物理地址1: ldmia r5, &#123;r3, r4&#125; @ value, mask and r4, r4, r9 @ mask wanted bits teq r3, r4 beq 2f add r5, r5, #PROC_INFO_SZ @ sizeof(proc_info_list) cmp r5, r6 blo 1b mov r5, #0 @ unknown processor2: ret lrENDPROC(__lookup_processor_type)/* * Look in &lt;asm/procinfo.h&gt; for information about the __proc_info structure. */ .align 2 .type __lookup_processor_type_data, %object__lookup_processor_type_data: .long . .long __proc_info_begin .long __proc_info_end .size __lookup_processor_type_data, . - __lookup_processor_type_data 总结 proc info 是如何获取的？通过读取 cp15 中的 cpu id 信息，然后与 __proc_info_begin 段中的所有 cpu_val 进行比较，如果与当前 cpu 一致，则找到对应的 procinfo 数据。 为何需要提前获取？有初始化必要的参数，譬如临时页表所需的 mmu 标识，在打开 mmu 之前需要配置临时内核页表。 DTB 校验 以及 fixupDTB 校验DTB 的校验工作其实很简单就是从 DTB 文件读取 magic 进行比较，如果一致则认为 DTB 有效。 DTB 这部分数据的话会在 start_kernel 的时候使用，所以必须保证是一个有效的数据块。 DTB 数据会被加载到 0x6320_0000 这一地址。DTB 文件在编译后的 arch/arm/boot/dts。 1234567// arch/arm/boot/dts$ hexdump -C vexpress-v2p-ca9.dtb | head00000000 d0 0d fe ed 00 00 36 e0 00 00 00 38 00 00 33 3c |......6....8..3&lt;|00000010 00 00 00 28 00 00 00 11 00 00 00 10 00 00 00 00 |...(............|00000020 00 00 03 a4 00 00 33 04 00 00 00 00 00 00 00 00 |......3.........|00000030 00 00 00 00 00 00 00 00 00 00 00 01 00 00 00 00 |................| 1234567891011121314151617// arch/arm/kernel/head-common.S:47__vet_atags: tst r2, #0x3 @ aligned? 保证 dtb 地址是4字节对齐的 bne 1f ldr r5, [r2, #0] @ 获取 dtb 前四个字节，存放在 r5 寄存器中#ifdef CONFIG_OF_FLATTREE ldr r6, =OF_DT_MAGIC @ is it a DTB? cmp r5, r6 beq 2f#endif2: ret lr @ atag/dtb pointer is ok1: mov r2, #0 ret lr fixup 该部分感觉并非特别重要，且与具体 cpu 相关，放弃阅读。 创建页表 页表创建是在 start_kernel 最关键也是比较难以理解的一个部分，尤其是对计算机硬件了解不深，对 MMU、TLB 没有认识的人而言更加难以理解整个过程。本节将简单的介绍一下 MMU，并通过一个简单的例子来说明虚拟地址是如何转化到物理地址上的。对 MMU 有一个比较深刻的认识。了解完 MMU 后，才能够继续讲解 create_page_tables，因为 create_page_tables 函数就是基于 MMU 特性所做的一个操作。 MMU 介绍MMU 是 Memory Management Unit（内存管理单元），开启 MMU 后就可以通过虚拟地址管理内存，完成硬件进行虚拟地址到物理地址的转化，提供硬件的内存访问授权。 Arm32 MMU 一级描述符的格式 最后两位用于表示当前使用的是哪一种类型： 0b00: Fault, 表示无效或错误。 0b01: Page table, 表示使用页表。 0b10: Section or Supersection, 表示当前使用段管理，或者超级段（在32位中访问大于4G内存）。若是 Supersection 则 Bit[18] 必须为 1, Section 为 0。 Section 段管理 因为在 __create_page_tables 中使用的是最简单的段管理，因此这里详细讲解下 Section。参考 ARMv7 手册 p.1320。 Bit[1:0] 描述类型，必须为 10 TEX[2:0] + CB 用于表示 cache 的运行模式，例如：001 + 11 表示 Outer and Inner Write-Back，Write-Allocate。这部分需要有 cpu cache 前置知识，需要知道 cache 有直写和写回两种方法，Write-Allocate 是要求在写 cache 中没有的数据时，不能直接写内存，而是先把内存数据加载到 cache，然后写 cache。 Domain 控制域。 AP[2:0] 访问权限控制，若是 011 则表示 Full access。 NS 表示 Non-secure bit. 表示转化 PA 是否在 secure address map。 nG 表示 not global bit. S 表示 shareable bit. MMU 如何工作 MMU 的工作实质就是将虚拟地址转化为物理地址，其步骤如下： 从 TTBR0(translation table base register) 获取基础地址。 通过虚拟地址的 Table Index 查找到相应的页表所在地址。 从页表所在地址中读取出相应物理页表数据，完成虚拟页表到物理页表的转化。 在获取的物理页表上加上对应的指令偏移则获取到最后的物理地址。 下面举一个实际的例子： 该例子中，TTBR0 保存的基地址是 0x6000_4000，并且从 0x6000_4000~0x6000_8000 这一个 16k 的内存中建立了相应的页表数据，此时需要获取虚拟地址 0xc000_8130 的 物理地址。 步骤： 根据从 TTBR0 中获取的 0x6000_4000 以及对应虚拟地址 0xc000_8130 计算得到 对应的描述符物理地址为 0x6000_7000。 从描述符锁在物理地址 0x6000_7000 可以得到 section 信息为 0x6001_1c0e。 通过 0x6001_1c0e 的 Section Base address + Section Index 即获取最后的物理地址为 0x8000_8130。 为何说内核是线性映射的，因为内核虚拟地址在 0xc0000000 向上位置，其对于的物理地址刚好是 0x6000_0000 向上，这部分映射的话直接映射即可。0xc00 -&gt; 0x600_ MMU 做的工作如上所述，但是这一工作的前提是必须要由我们来创建页表信息。页表是需要人为维护的，这也是 __create_page_tables 所做的工作。 代码解读123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596/* * 创建一个物理地址与虚拟地址一致的映射 * 为了打开 MMU 时候需要一个临时的页表。 * __create_page_tables 做了两件事情: * 1. 打开 mmu 把打开 mmu 代码对应到段映射到 1m 大小的地方。创建一个恒等的映射。虚拟地址=物理地址 * 2. 把内核的 image 代码映射到内核到虚拟空间中。 * * Setup the initial page tables. We only setup the barest * amount which are required to get the kernel running, which * generally means mapping in the kernel code. * * r8 = phys_offset, r9 = cpuid, r10 = procinfo * * Returns: * r0, r3, r5-r7 corrupted * r4 = page table (see ARCH_PGD_SHIFT in asm/memory.h) */__create_page_tables: pgtbl r4, r8 @ page table address, r4 设置成页表的基地址, phys_offset + TEXT_OFFSET(0x8000) - PG_DIR_SIZE(0x4000), 即为内核位置下方的 16k @ r4 = 0x6000_4000 r8 = 0x6000_8000 /* * Clear the swapper page table * 清除 0x6000_4000 - 0x6000_8000 这段内存数据 */ mov r0, r4 @ r0 = r4 mov r3, #0 @ r3 = 0 add r6, r0, #PG_DIR_SIZE @ r6 = 页表基地址 + 16k，即页表的尾地址1: str r3, [r0], #4 @ str 移动的是 32 位的数据, 将 r3 的值写入 r0 为地址的内存, 并设置 r0 = r0 + 4 str r3, [r0], #4 str r3, [r0], #4 str r3, [r0], #4 teq r0, r6 @ 判断是否到页表到尾地址 bne 1b ldr r7, [r10, #PROCINFO_MM_MMUFLAGS] @ mm_mmuflags 设置 mmu 标志位 /* * Create identity mapping to cater for __enable_mmu. * This identity mapping will be removed by paging_init(). */ adr r0, __turn_mmu_on_loc @ r0 = __turn_mmu_on_loc 地址 ldmia r0, &#123;r3, r5, r6&#125; @ 将 r0 中的数据 弹出到 r3, r5, r6 sub r0, r0, r3 @ virt-&gt;phys offset, r0 为物理地址, r3, r5, r6 为虚拟地址，此时需要将虚拟地址转化为物理地址, r0 与 r3 是 __turn_mmu_on_loc 对应的物理与虚拟地址 add r5, r5, r0 @ phys __turn_mmu_on add r6, r6, r0 @ phys __turn_mmu_on_end mov r5, r5, lsr #SECTION_SHIFT @ r5 = r5 &gt;&gt; 20, 取高12位 mov r6, r6, lsr #SECTION_SHIFT @ r6 = r6 &gt;&gt; 20, 取高12位1: orr r3, r7, r5, lsl #SECTION_SHIFT @ flags + kernel base r3 = r7 | (r5 &lt;&lt; 20) 得到页表需要设置的数据 str r3, [r4, r5, lsl #PMD_ORDER] @ identity mapping [r4 + r5 &lt;&lt; 2] = r3, 将 __turn_mmu_on 设置到 对应的一级页表中 cmp r5, r6 @ 是否在同一段，若不是则继续映射 addlo r5, r5, #1 @ next section blo 1b /* * Map our RAM from the start to the end of the kernel .bss section. * 下面这部分其实是为了将内核部分数据映射 * 内核虚拟地址为 0xc000_0000 - 0xc112_23d7 * 内核物理地址为 0x6000_0000 - 0x6112_23d7 * 进行计算后对应段为 0x6000_7000 - 0x6000_7044 * 0x6000_7000 如何计算: L1 基地址为 0x6000_4000 + 0xc000_0000 &gt;&gt; 18 * 0x6000_7044 如何计算: L1 基地址为 0x6000_4000 + 0xc112_23d7 &gt;&gt; 18 */ add r0, r4, #PAGE_OFFSET &gt;&gt; (SECTION_SHIFT - PMD_ORDER) @ r0 = r4 + 0xc000_0000 &gt;&gt; 18, r0 = 0x6000_7000 ldr r6, =(_end - 1) @ ldr 伪指令, r6 = kernel_end - 1 r6 = 0xc112_23d7 orr r3, r8, r7 @ r3 = r8 | r7, r8 为 phys_offset 0x6000_0000 add r6, r4, r6, lsr #(SECTION_SHIFT - PMD_ORDER) @ r6 = r4 + r6 &gt;&gt; 18, 与之前设置 __turn_mmu_on 段过程一样1: str r3, [r0], #1 &lt;&lt; PMD_ORDER add r3, r3, #1 &lt;&lt; SECTION_SHIFT cmp r0, r6 bls 1b /* * dtb 所在两个段的映射 * Then map boot params address in r2 if specified. * We map 2 sections in case the ATAGs/DTB crosses a section boundary. */ mov r0, r2, lsr #SECTION_SHIFT @ 将 r2=dtb dtb 左移，存放到 r0 movs r0, r0, lsl #SECTION_SHIFT @ 再将 r0 右移获取物理内存段地址 subne r3, r0, r8 @ 计算 r0 对应DARM起始地址r8偏移 addne r3, r3, #PAGE_OFFSET addne r3, r4, r3, lsr #(SECTION_SHIFT - PMD_ORDER) orrne r6, r7, r0 strne r6, [r3], #1 &lt;&lt; PMD_ORDER addne r6, r6, #1 &lt;&lt; SECTION_SHIFT strne r6, [r3] ret lrENDPROC(__create_page_tables) .ltorg .align__turn_mmu_on_loc: .long . .long __turn_mmu_on .long __turn_mmu_on_end 运行完成后内存结构 重点 为何 __turn_mmu_on 这部分需要做一个直接映射？因为在开启 MMU 完成后 pc 指针指向的还是物理地址，但此时对 MMU 而言该地址是个虚拟地址，需要查页表，若不建立这一块直接映射，则无法继续执行。 MMU flags 03x11C0E 表示什么？是从arch/arm/mm/proc-v7.S:503这里获取的。AP=011可读性，TEXCB=00111表示Outer and Inner Write-Back，Write-Allocate。 TEXT_OFFSET 在那个文件指定？arch/arm/Makefile, 0x8000 PG_DIR_SIZE 在哪个文件指定？arch/arm/kernel/head.S, 0x4000 PAGE_OFFSET 表示内核的开始虚拟地址, 0xc000_000ARM 汇编指令说明 指令 说明 mrs r4, cpsr 状态寄存器到通用寄存器的传送指令 msr cpsr, r4 通用寄存器到状态寄存器的传送指令 adr r0, .test 相当于 add r0, pc, #4 小范围地址读取伪指令 ldr 大范围地址读取伪指令 str r0, [r1, #8] 将r0中的字写入r1+8为地址的存储中 cmp r0, r1 相当于 r0-r1 eor 异或指令 MCR{&lt;cond&gt;} &lt;p&gt;,&lt;opcode_1&gt;,&lt;Rd&gt;,&lt;CRn&gt;,&lt;CRm&gt;{,&lt;opcode_2&gt;} 将arm寄存器数据传递到协处理器 MRC{&lt;cond&gt;} &lt;p&gt;,&lt;opcode_1&gt;,&lt;Rd&gt;,&lt;CRn&gt;,&lt;CRm&gt;{,&lt;opcode_2&gt;} 将协处理器数据传递到arm寄存器 LDMIA R1!,{R0,R4-R6} 从左到右加载,相当于 LDR R0,10000000 LDR R4,10000004… …，!表示最后地址是否写回到R1 STMIB R1,{R4-R6} 从左到右加载,相当于STR [R4],0X10000004 STR [R5],0X10000008 ….. 参考本文在大量参考已有知识，并通过自己调试总结后完成。 ARMv7 手册奔跑吧 Linux 内核[kernel 启动流程] 前篇——vmlinux.lds分析[kernel 启动流程] （第一章）概述[kernel 启动流程] （第二章）第一阶段之——设置SVC、关闭中断[kernel 启动流程] （第三章）第一阶段之——proc info的获取[kernel 启动流程] （第四章）第一阶段之——dtb的验证[kernel 启动流程] （第五章）第一阶段之——临时内核页表的创建[kernel 启动流程] （第六章）第一阶段之——打开MMU[kernel 启动流程] （第七章）第一阶段之——跳转到start_kernel]]></content>
      <categories>
        <category>LinuxKernel4.0-arm32</category>
      </categories>
      <tags>
        <tag>LinuxKernel4.0-arm32</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入浅出计算机组成原理]]></title>
    <url>%2FLinux%2Flinux%2Fcpu-info%2F</url>
    <content type="text"><![CDATA[深入浅出计算机组成原理[TOC] 本博客主要用于整理深入浅出计算机组成原理学到的相关知识，本人深觉笔记的重要性，读书百遍其义自现，不如认真将精华记录。 入门篇 计算机组成，向下可以学习数字电路，向上可以学习编译原理、操作系统、计算机网络。 冯·诺依曼体系结构，也叫存储程序计算机，一个是”可编程”计算机，一个是”存储”计算机。主要由 CPU（控制器，运算器）、存储器、输入输出设备组成。 图灵机相较于冯·诺依曼机两者差别在于，图灵机侧重于计算抽象，冯·诺依曼机侧重于硬件抽象。 性能与功耗性能响应时间（执行时间），“跑得快”。吞吐率（带宽），“搬的多”。 性能指标由以下公式决定： CPU Time = InstructionNum * CPI(Cycles Per Instruction) * Clock Cycle Time InstructionNum，程序指令数，由编译器以及程序员决定。 CPI，Cycles Per Instruction，使用 Pipeline，超流水线、SIMD 等技术使一条指令需要的 CPU Cycle 尽可能少。对 CPI 优化是计算机体系结构最重要一环。 Clock Cycle Time，由计算机主频决定。 功耗我们的 CPU，一般都被叫作超大规模集成电路（Very-Large-Scale Integration，VLSI） 由于功耗墙问题，通过提升主频实现性能提升越来越难，因此通过引入多核 CPU 设计的方案通过提升“吞吐率”而非“响应时间”来提升性能。即通过并行提升性能。 阿姆达尔定律（Amdahl’s Law）： 由于阿姆达尔定律的存在，优化后的执行时间也是有上限的，主要是不受影响的执行时间无法继续优化。 提升性能方法 摩尔定律，提升主频。 并行计算，提升吞吐率。 加大大概率事件。 流水线提高性能，降低 CPI。 预测提升性能，分支与保险、局部性原理。 指令与计算指令 不同的 CPU 有不同的指令集，以 MIPS 指令集为例。 MIPS 指令是一个32位整数，高6位叫操作码，剩下26位有 R、I、J 三种格式。 内联函数能够减少，函数调用过程中，压栈出栈的操作。 为什么程序无法同时在Linux和Windows下运行？ 链接器会扫描所有的目标文件，把符号表所有信息收集构成全局符号表，再根据重定位表，把所有不确定要跳转地址的代码，根据符号表地址进行修正，最后把所有目标文件对应段进行合并，形成最终的可执行代码。 程序装载采用内存分页的方式，getconf PAGE_SIZE 默认为4k。 若全部采用静态链接，标准库需要不停地拷贝很多份。 此时，选择采用动态链接的方式，内存中只保存一份，虚拟内存地址与之进行映射。 通过查找 PLT 与 GOT（Global Offset Table） 找到动态库地址。 计算 乘法可以用加法进行代替。 电路在物理上是并行的，因此可以加速。 处理器数据通路 CPU 指令周期可以分为 Fetch、Decode、Execute。数据通路连接操作元件（ALU）和存储元件（寄存器），完成数据存储、处理、传输的处理器单元。 通过反向器实现时钟信号。 通过 D 触发器实现寄存器。 PC 寄存器。 CPU 优化流水线 首先考虑的是单指令周期的处理器，由于不同指令所需时钟周期不同，因此都需要等满一个时钟周期，产生大量浪费，同时时钟⏰频率也很难提升。 通过将指令，按照不同的小步骤拆分，即形成流水线工作，我们不需要确保最复杂的那条指令在时钟周期里执行完成，而只要保障一个最复杂的流水线级的操作，在一个时钟周期内完成即可。 虽然，没有提升“延时”，但是提升了“吞吐量”。5条流水线，即提升5倍。像我们现代的 ARM 或者 Intel 的 CPU，流水线级数已经到了14级。 超长流水线瓶颈，增加流水线深度，其实是有性能成本，在读写流水线寄存器时，会有开销。同时还会带来结构冒险、控制冒险等其他依赖问题。为了应付各类冒险问题，采用乱序执行、分支预测等解决方案。流水线越长，这一类冒险问题就更难解决。 12345678910int a = 10 + 5; // 指令 1int b = a * 2; // 指令 2float c = b * 1.0f; // 指令 3int x = 10 + 5; // 指令 4int y = a * 2; // 指令 5float z = b * 1.0f; // 指令 6int o = 10 + 5; // 指令 7int p = a * 2; // 指令 8float q = b * 1.0f; // 指令 9// 由于1，2，3有依赖关系，可以先执行1，4，7 冒险与预测 流水线设计必须解决三大冒险，分别是结构冒险, 数据冒险, 控制冒险。 结构冒险 访存与取指令都需要操作程序内存，但是我们只有一个地址译码器，会产生资源冲突，为了解决该冲突，将内存分为两部分，一个用于存放指令，一个用于存放数据。把内存拆成两部分的解决方案，被称为哈佛架构，我们使用的冯·诺依曼体系结构又叫普林斯顿架构，该体系结构没有拆分内存，而是在高速缓存部分分成指令缓存和数据缓存。 数据冒险 先写后读，数据依赖 1234567int main() &#123; int a = 1; int b = 2; // 先写1 再读1 a = a + 2; b = a + 3;&#125; 先读后写，反依赖 1234567int main() &#123; int a = 1; int b = 2; a = b + a; // 若下面早于上面完成 b = a + b;&#125; 写后再写，输出依赖 12345int main() &#123; int a = 1; // 先写1 再写1 a = 2;&#125; 利用 nop 解决 操作数前推（操作数转发），减少 nop 由于不同指令不存在依赖关系，可以通过乱序执行，将没有依赖关系的指令提前，减少 nop 时间。乱序执行，有点类似于 线程池。 控制冒险 以上假设都是基于顺序执行进行的，取指令 和 指令译码不会遇到任何停顿。当遇到 jmp 等跳转指令时，为了保证争取，不得不等待延迟。 分支预测，便是假装分支不发生，即静态预测成功率在50%，当发现错误时，就将不要的数据丢弃。 动态分支预测，即今天下雨，就猜明天也下雨。即用1比特保存当前预测结果，并用该结果作为下一次分支比较。还可以用2比特预测，叫双模态预测，状态机如下。 注意在不改变代码基础上，将循环少的放外面。 Superscalar和VLIW使CPU吞吐率超过1 多发射与超标量 由于取指令一次只能提取一条，CPU吞吐率只能限定在1。 通过指令多发射设计，可以得到超标量，即并行执行多条指令。 VLIW（Very Long Instruction Word） VLIW 无法兼容，最后失败了。 超线程超长流水线，意味着在流水线中指令越多，相互依赖关系越多，不得不面临冒险。2002 年底，Intel 在的 3.06GHz 主频的 Pentium 4 CPU上，第一次引入了超线程技术。超线程技术，就是在CPU中同时执行多个程序的指令。 增加一份PC寄存器、指令寄存器、条件码寄存器。 SIMD, 加速矩阵乘法SIMD，Single Instruction Multiple Data。SIMD 在获取数据和执行指令时，都做到了并行，一方面，再从内存读取数据的时候，SIMD是一次性读取多个数据，在SSE指令集中，CPU添加了8个16Bytes的寄存器，一个寄存器一次性可以加载4个整数。 Numpy，使用 SIMD 技术。 基于 SIMD 的向量计算指令，被称为 MMX（Matrix Math eXtensions） 指令集，即矩阵数学扩展。 异常和中断CISC和RISC GPUMMX + SIMD =&gt; GPU SIMT, Single Instruction Multiple Threads。 GPU, 超线程。 FPGA和ASICFPGA，现场可编程门阵列（Field-Programmable Gate Array）。 ASIC，（Application-Specific Integrated Circuit），专用集成电路。 存储与IOCPU Cache 使用 SRAM（Static Random-Access Memory）的芯片，只要处于通电，数据可以持续保存。在CPU中有L1、L2、L3三层缓存。L1分为指令缓存和数据缓存。 内存采用 DRAM（Dynamic Random Access Memory），需要不断地“刷新”。 局部性原理：时间局部性和空间局部性。时间局部性：如果一个数据被访问了，它在短时间内还会被再次访问。空间局部性：如果一个数据被访问了，相邻数据也会很快被访问。 高速缓存L1、L2、L3 由于CPU与内存性能差距越来越大，因此需要缓存。 直接映射 全关联 组关联 MESI 协议保持多核CPU缓存一致。 内存利用虚拟内存技术，采用多级页表。 利用 TLB 进行加速。 总线 12iotopiostat 参考书籍 《计算机是怎样跑起来的》 《程序是怎样跑起来的 《Computer Organization》 《计算机组成与设计：硬件 / 软件接口》 《深入理解计算机系统》 《深入理解计算机系统bilibili》 《深入理解计算机系统YouTube》 《计算机组成：结构化方法》 《计算机体系结构：量化研究方法》 《编码：隐匿在计算机软硬件背后的语言》 《程序员的自我修养：链接、装载和库》 Modern Microprocessors Teach Yourself Programming in Ten Years Building Software Systems at Google andLessons Learned What Every Programmer Should Know About Memory 参考论文hwViewForSwHackers.pdfAvailable-Instruction-Level-Parallelism-for-Superscalar-and-Superpipelined-Machines.pdf]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[travelling salesman problem]]></title>
    <url>%2FAlgorithm%2Falgorithm%2Ftravelling-salesman-problem%2F</url>
    <content type="text"><![CDATA[后面更新 Leetcode 943]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>TSP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二分搜索模版]]></title>
    <url>%2FAlgorithm%2Falgorithm%2Fbinary-search-template%2F</url>
    <content type="text"><![CDATA[二分查找、二分边界查找算法的模板代码总结 基础模版形式Java 基础版本12345678910public int binarySearch(int[] nums, int target) &#123; int left = 0, right = nums.length-1; while (left &lt;= right) &#123; int mid = (right - left) / 2 + left; if (nums[mid] == target) return mid; else if (nums[mid] &gt; target) right = mid-1; else left = mid+1; &#125; return -1;&#125; First 版本123456789101112private int findFirst(int[] nums, int target) &#123; int start = 0, end = nums.length - 1; while (start + 1 &lt; end) &#123; int mid = (end - start) / 2 + start; if (nums[mid] &gt;= target) end = mid; else start = mid; &#125; if (nums[start] == target) return start; if (nums[end] == target) return end; return -1;&#125; Last 版本1234567891011private int findLast(int[] nums, int target) &#123; int start = 0, end = nums.length - 1; while (start + 1 &lt; end) &#123; int mid = (end - start) / 2 + start; if (nums[mid] &gt; target) end = mid; else start = mid; &#125; if (nums[end] == target) return end; if (nums[start] == target) return start; return -1;&#125; 花花模版 (*)花花酱 LeetCode Binary Search II - 刷题找工作 SP 17 123456789// [l, r)public int binarySearch(int l, int r) &#123; while (l &lt; r) &#123; m = (r - l) / 2 + l; if (g(m)) r = m; // [l, m) else l = m+1; // [m+1, r) &#125; return l;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Binary Search</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spinlock]]></title>
    <url>%2Funcategorized%2Flinux%2Fspinlock%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Trie]]></title>
    <url>%2FAlgorithm%2Falgorithm%2Ftrie%2F</url>
    <content type="text"></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Trie</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[奔跑吧 Linux 内核(入门篇)学习笔记]]></title>
    <url>%2FLinux%2Flinux%2Frunning-linux-primer%2F</url>
    <content type="text"><![CDATA[奔跑吧 Linux 内核（入门篇）是由笨叔叔出版的一本学习 Linux 内核的基础数据，本人于 2019-03-01 购入，期间阅读过一遍，但再回首发现自己学习到的并不多，决定讲学习过程记录下来，一方面用于学习巩固，另一方面也方便自己后期整理学习。 实验前准备使用定制的 runninglinuxkernel Linux 内核编译Ctrl+a，再输入 x 退出 QEMU。 安装工具 1sudo apt-get install qemu libncurses5-dev gcc-arm-linux-gnueabi build-essential gcc-5-arm-linux-gnueabi git 确保 gcc 版本为 5.+ 1arm-linux-gnueabi-gcc -v 下载 runninglinuxkernel 12git clone https://github.com/figozhang/runninglinuxkernel_4.0.gitgit checkout rlk_basic 编译内核 12345cd runninglinuxkernel-4.0export ARCH=armexport CROSS_COMPILE=arm-linux-gnueabi-make vexpress_defconfigmake menuconfig 创建设备节点 1234cd _install_arm32mkdir devcd devsudo mknod console c 5 1 编译 12make bzImage -j4make dtbs 运行 1./run.sh arm32 测试主机与虚拟机共享文件，只需将共享文件复制到 kmodules 即可 12cd runninglinuxkernel-4.0/kmodulestouch test.c GCC1234gcc -E test.c // 预编译 test.igcc -S test.c // 编译 test.sgcc -c test.c // 汇编 test.ogcc test.c -o test // 链接 test 交叉编译命名规范：[arch]-[os]-[(gnu)eabi]arch: 表示体系结构, ARM、MIPSos: 表示目标操作系统eabi: 嵌入式应用二进制接口 arm-linux-gnueabi: 用于 ARM32 的 Liunx 系统编译。 aarch-linux-gnueabi: 用于 ARM64 的 Linux 系统编译。 CUN C 语法技巧 语法表达 12345678// #define max(a, b) ((a) &gt; (b) ? (a) : (b))// 若传入 i++, j++ 会导致 a 和 b 计算两次// 因此需要使用更安全的写法 #define min(x, y) (&#123; \ typeof(x) _min1 = (x); \ typeof(y) -min2 = (y); \ (void) (&amp;_min1 == &amp;_min2); \ _min1 &lt; _min2 ? _min1 : _min2; &#125;) 零长数组 123456struct line &#123; int length; char contents[0];&#125;;struct line *thisline = malloc(sizeof(struct line) + this_length);thisline-&gt;length = this_length; 标号元素 123456789101112// GUN C 语言可以通过指定索引或结构体名来初始化，不必按照原来固定顺序进行初始化&lt;drivers/char/mem.c&gt;static const struct file_operations zero_fops = &#123; .llseek = zero_lseek, .read = new_sync_read, .write = write_zero, .read_iter = read_iter_zero, .aio_write = aio_write_zero, .mmap = mmap_zero,&#125;; 第二章实验2: 内核链表内核链表 点击下载 第四章简单模块Linux 内核通过内核模块来实现动态添加和删除某个功能。 编写一个简单的内核模块 1234567891011121314151617181920212223242526272829303132333435363738// ###### c 代码 结束 ######## include &lt;linux/init.h&gt; // 内核源代码，包含了 module_init module_exit # include &lt;linux/module.h&gt; // 包含最后的声明static int __init my_test_init(void) &#123; // 函数入口 printk("my first kernel module init\n"); return 0;&#125;static void __exit my_test_exit(void) &#123; // 退出 printk("goodbye\n");&#125;module_init(my_test_init);module_exit(my_test_exit);MODULE_LICENSE("GPL"); // 许可证MODULE_AUTHOR("Klutzoder"); // 作者MODULE_DESCRIPTION("my test kernel module"); // 描述MODULE_ALIAS("mytest"); // 别名// ####### c 代码 结束 #######// ####### Makefile 开始 #######BASEINCLUDE ?= /lib/modules/`uname -r`/buildmytest-objs := my_test.oobj-m := mytest.o# 一定要注意是 taball: $(MAKE) -C $(BASEINCLUDE) M=$(PWD) modules;clean: $(MAKE) -C $(BASEINCLUDE) SUBDIRS=$(PWD) clean; rm -f *.ko // ####### Makefile 结束 ####### 编译后加载 1234567file mytest.komodinfo mytest.ko sudo insmod mytest.ko // 加载dmesg // 查看日志lsmod // 查看所有的 module// /sys/module 包含所有的模块sudo rmmod mytest // 卸载模块 模块参数使用]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dijkstra]]></title>
    <url>%2FAlgorithm%2Falgorithm%2Fdijkstra%2F</url>
    <content type="text"><![CDATA[本文基于 [Python] BFS和DFS算法（第3讲）—— 从BFS到Dijkstra算法 学习整理后所得。对于 BFS 及 DFS 有所不了解的可以看 BFS(Breadth First Search) 与 DFS (Depth First Search) DijkstraDijkstra 是在 BFS 基础上进行修改而来的，对于 BFS 遍历而言，由于其边的长度只为1，因此只需使用 Queue 即可。但是对于边有权重的图而言，需要考虑每一条边的权重。 因此，对于 Dijkstra 可以考虑使用 PriorityQueue 来代替 Queue，如此每次便可取出距离最短的点。 过程解析第一步：将 A:0 放入 PQ第二步：从 PQ 中获取一个节点 A:0， A:0 没有被使用过，且其连接的节点为 B, C。此时将 B:5, C:1 放入 PQ 中，将 A 标记为已使用。第三步：从 PQ 中获取一个节点 C:1 (此时使用优先队列，所以取最近的)，此时与 C 连接的节点为 A, B, D，且 A 被标记使用，需要将 B:3, D:5, E:9 放入 PQ。 这里最关键的一步是，取出 C 点后，需要将 B:3 再放入 PQ 中，PQ 中的数据将变为 [B:5, B:3, D:5, E:9] 代码123456789101112131415161718192021222324252627282930313233343536// 无向图，注意 u, v 可以相互到达public class Dijkstra &#123; public void solution(int[][] uvw, int N) &#123; Map&lt;Integer, List&lt;int[]&gt;&gt; map = new HashMap&lt;&gt;(); for (int[] c : uvw) &#123; map.putIfAbsent(c[0], new ArrayList&lt;&gt;()); map.putIfAbsent(c[1], new ArrayList&lt;&gt;()); map.get(c[0]).add(new int[] &#123; c[2], c[1] &#125;); map.get(c[1]).add(new int[] &#123; c[2], c[0] &#125;); &#125; int[] parents = new int[N]; parents[0] = -1; boolean[] used = new boolean[N]; // 0: distance, 1: node PriorityQueue&lt;int[]&gt; pq = new PriorityQueue&lt;&gt;((a, b) -&gt; a[0] - b[0]); pq.offer(new int[] &#123; 0, 0 &#125;); while (!pq.isEmpty()) &#123; int[] cur = pq.poll(); if (used[cur[1]] || !map.containsKey(cur[1])) continue; used[cur[1]] = true; for (int[] next : map.get(cur[1])) &#123; if (used[next[1]]) continue; parents[next[1]] = cur[1]; pq.offer(new int[]&#123;cur[0]+next[0], next[1]&#125;); &#125; &#125; System.out.println(Arrays.toString(parents)); &#125; public static void main(String[] args) &#123; new Dijkstra().solution(new int[][] &#123; &#123; 0, 1, 5 &#125;, &#123; 0, 2, 1 &#125;, &#123; 1, 2, 2 &#125;, &#123; 1, 3, 1 &#125;, &#123; 2, 3, 4 &#125;, &#123; 2, 4, 8 &#125;, &#123; 3, 4, 3 &#125;, &#123; 3, 5, 6 &#125; &#125;, 6); &#125;&#125; LeetCode 743123456789101112131415161718192021222324252627282930class Solution &#123; public int networkDelayTime(int[][] times, int N, int K) &#123; Map&lt;Integer, List&lt;int[]&gt;&gt; graph = new HashMap&lt;&gt;(); for (int[] t : times) &#123; graph.putIfAbsent(t[0], new ArrayList&lt;&gt;()); graph.get(t[0]).add(new int[]&#123;t[1], t[2]&#125;); &#125; boolean[] used = new boolean[N+1]; int res = 0; // 0: distance 1: Node PriorityQueue&lt;int[]&gt; pq = new PriorityQueue&lt;&gt;((a,b)-&gt;a[0]-b[0]); pq.offer(new int[]&#123;0, K&#125;); while (!pq.isEmpty()) &#123; int[] cur = pq.poll(); if (used[cur[1]]) continue; used[cur[1]] = true; res = Math.max(res, cur[0]); if (!graph.containsKey(cur[1])) continue; for (int[] next : graph.get(cur[1])) &#123; if (used[next[0]]) continue; pq.offer(new int[]&#123;cur[0]+next[1], next[0]&#125;); &#125; &#125; for (int i = 1; i &lt;= N; i++) &#123; if (!used[i]) return -1; &#125; return res; &#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>BFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Segment Tree And Fenwick Tree]]></title>
    <url>%2FAlgorithm%2Falgorithm%2Fsegment-tree-and-fenwick-tree%2F</url>
    <content type="text"><![CDATA[本文讲解的 FenwickTree 以及 Segment Tree 主要用于解决可变数组中，高效求解范围和的算法。以 Leetcode 307. Range Sum Query - Mutable 为例进行分析。学习资料来源于：花花酱 Fenwick Tree / Binary Indexed Tree - 刷题找工作 SP3、花花酱 Segment Tree 线段树 - 刷题找工作 SP14、线段树 (segment tree)，并结合自身理解进行整理。 不可变数组的范围和求解对于不可变数组的范围和，我们可以通过预先处理前缀和，然后通过前缀和之间的关系求解。 303. Range Sum Query - Immutable 1234567891011121314151617class NumArray &#123; private int[] prefix; public NumArray(int[] nums) &#123; prefix = new int[nums.length]; if (nums == null || nums.length == 0) return; prefix[0] = nums[0]; for (int i = 1; i &lt; nums.length; i++) &#123; prefix[i] = prefix[i-1] + nums[i]; &#125; &#125; public int sumRange(int i, int j) &#123; if (prefix == null || prefix.length == 0) return 0; return prefix[j] - (i == 0 ? 0 : prefix[i-1]); &#125;&#125; 但是对于一个可变的数组如何求解前缀和呢？ 譬如有一个数组 [2, 5, -1, 3, 6] 需要实现 update 与 sumRange 函数，此时最简单的实现便是 update 时直接修改 O(1) 时间复杂度， sumRange 时直接遍历计算 O(n) 时间复杂度，这一 naive 的解题思路看着很不错，但实际上在求和时，会有很多的重复解，这便是可以优化的一部分。 一般常见的优化手段有 FenwickTree 以及 SegmentTree，以下就分别对这两种算法进行详细介绍。 FenwickTreeFenwickTree 又称为 Binary Indexed Tree 这是一种特殊的数据结构，能够高效地修改元素并计算数组中的前缀和。 FenwickTree 一共包含两个函数 update(i, val) 和 query(i)。 可通过点击FenwickTree可视化过程链接，查看整个过程。 lowBit在代码中如何寻找数字中的最低位，这里有一个特别方便的方法，就是通过位运算求解。 123private int lowBit(int x) &#123; return x &amp; (-x);&#125; 对于 x = 5 而言，其二进制位 0101，-x = ~x + 1 = 1010 + 1 = 1011；对 x &amp; (-x) = 0101 &amp; 1011 = 1即所求的最低位。 update O(log(n)) 在 update 过程中，每一个节点的父节点均为 i+lowBit(i)，每次更新 i 节点时候，均要更新该节点的所有父节点。 123456public void update(int i, int val) &#123; while (i &lt; sums.length) &#123; sums[i] += val; i += lowBit(i); &#125;&#125; query O(log(n)) 在 query 过程中，每一个节点的父节点均为 i-lowBit(i)，若要求得 [0, i] 的和必须添加对应父节点上所有的解。 1234567public int query(int i) &#123; int sum = 0; while (i &gt; 0) &#123; sum += sums[i]; i -= lowBit(i); &#125;&#125; LeetCode30712345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class NumArray &#123; private class FenwickTree &#123; private int[] sums; public FenwickTree(int n) &#123; sums = new int[n+1]; &#125; private int lowBit(int x) &#123; return x &amp; (-x); &#125; public void update(int i, int val) &#123; while (i &lt; sums.length) &#123; sums[i] += val; i += lowBit(i); &#125; &#125; public int query(int i) &#123; int sum = 0; while (i &gt; 0) &#123; sum += sums[i]; i -= lowBit(i); &#125; return sum; &#125; &#125; private FenwickTree ft; private int[] nums; public NumArray(int[] nums) &#123; this.nums = nums; ft = new FenwickTree(nums.length); for (int i = 0; i &lt; nums.length; i++) &#123; ft.update(i+1, nums[i]); &#125; &#125; public void update(int i, int val) &#123; ft.update(i+1, val - nums[i]); nums[i] = val; &#125; public int sumRange(int i, int j) &#123; return ft.query(j+1) - ft.query(i); &#125;&#125; SegmentTreeSegment Tree 是一个完全二叉树，用于存储区间内对应信息的一种数据结构，该数据结构相较于 FenwickTree 而言，除了处理范围和这一问题，还能解决区间最大、区间最小、区间异或值的处理。 一般 SegmentTree 有两种实现方法，第一种是 Tree 实现，第二种类似堆使用数组。 Tree build1234567private Node buildSegmentTree(int i, int j, int[] nums) &#123; if (i == j) return new Node(i, j, nums[i], null, null); int mid = (j-i)/2 + i; Node left = buildSegmentTree(i, mid, nums); Node right = buildSegmentTree(mid+1, j, nums); return new Node(i, j, left.sum+right.sum, left, right);&#125; update12345678910private void update(Node node, int i, int val) &#123; if (node.start == i &amp;&amp; node.end == i) &#123; node.sum = val; return; &#125; int mid = (node.end - node.start) / 2 + node.start; if (i &lt;= mid) update(node.left, i, val); else update(node.right, i, val); node.sum = node.left.sum + node.right.sum;&#125; queryRange1234567private int queryRange(Node node, int i, int j) &#123; if (node.start == i &amp;&amp; node.end == j) return node.sum; int mid = (node.end - node.start) / 2 + node.start; if (j &lt;= mid) return queryRange(node.left, i, j); else if (i &gt; mid) return queryRange(node.right, i, j); else return queryRange(node.left, i, mid) + queryRange(node.right, mid+1, j);&#125; LeetCode3071234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class NumArray &#123; private class Node &#123; private Node left, right; private int sum, start, end; public Node (int start, int end, int sum, Node left, Node right) &#123; this.start = start; this.end = end; this.sum = sum; this.left = left; this.right = right; &#125; &#125; private Node root; public NumArray(int[] nums) &#123; if (nums == null || nums.length == 0) return; root = buildSegmentTree(0, nums.length-1, nums); &#125; private Node buildSegmentTree(int i, int j, int[] nums) &#123; if (i == j) return new Node(i, j, nums[i], null, null); int mid = (j-i)/2 + i; Node left = buildSegmentTree(i, mid, nums); Node right = buildSegmentTree(mid+1, j, nums); return new Node(i, j, left.sum+right.sum, left, right); &#125; public void update(int i, int val) &#123; update(root, i, val); &#125; private void update(Node node, int i, int val) &#123; if (node.start == i &amp;&amp; node.end == i) &#123; node.sum = val; return; &#125; int mid = (node.end - node.start) / 2 + node.start; if (i &lt;= mid) update(node.left, i, val); else update(node.right, i, val); node.sum = node.left.sum + node.right.sum; &#125; public int sumRange(int i, int j) &#123; return queryRange(root, i, j); &#125; private int queryRange(Node node, int i, int j) &#123; if (node.start == i &amp;&amp; node.end == j) return node.sum; int mid = (node.end - node.start) / 2 + node.start; if (j &lt;= mid) return queryRange(node.left, i, j); else if (i &gt; mid) return queryRange(node.right, i, j); else return queryRange(node.left, i, mid) + queryRange(node.right, mid+1, j); &#125;&#125; Array以数组为蓝本的线段树，相对于上面而言有一个比较不直观的地方，若通过画图显示则可以表示成以下。以 [1,2,3,4,5,6,7,8,9] 作为基础数据。 build 新建数组，tree = new int[2*n]; 将 nums[i] 数据写到 tree[i+n] 更新每一个数据节点 123456789public int[] buildTree(int[] nums) &#123; int[] tree = new int[n*2]; for (int i = n, j = 0; i &lt; n*2; i++, j++) &#123; tree[i] = nums[j]; &#125; for (int i = n-1; i &gt;= 0; i--) &#123; tree[i] = tree[i*2] + tree[i*2+1]; &#125;&#125; update 1234567891011121314public void update(int pos, int val) &#123; pos += n; tree[pos] = val; while (pos &gt; 0) &#123; int left = pos, right = pos; if ((pos % 2) == 0) &#123; // pos is left right++; &#125; else &#123; // pos is right left--; &#125; tree[pos / 2] = tree[left] + tree[right]; pos /= 2; &#125;&#125; sumRange 在求和时候，若 left 为偶数，则可以直接考虑其父节点，若 left 为奇数则需要单独加上该节点。同理，若 right 为奇数，则可以直接考虑其父节点，若 right 为偶数则需要单独加上该节点。 123456789101112131415public int sumRange(int left, int right) &#123; left += n; right += n; int sum = 0; while (left &lt;= right) &#123; if ((left % 2) == 1) &#123; sum += tree[left++]; &#125; if ((right % 2) == 0) &#123; sum += tree[right--]; &#125; left /= 2; right /= 2; &#125; return sum;&#125; LeetCode 3071234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class NumArray &#123; private int[] tree; private int n; public NumArray(int[] nums) &#123; if (nums == null || nums.length == 0) return; n = nums.length; tree = buildTree(nums); &#125; private int[] buildTree(int[] nums) &#123; int[] tree = new int[2*n]; for (int i = n, j = 0; i &lt; 2*n; i++, j++) &#123; tree[i] = nums[j]; &#125; for (int i = n-1; i &gt;= 0; i--) &#123; tree[i] = tree[i*2] + tree[2*i+1]; &#125; return tree; &#125; public void update(int i, int val) &#123; i += n; tree[i] = val; while (i &gt; 0) &#123; int left = i, right = i; if ((i % 2) == 0) right++; else left--; tree[i / 2] = tree[left] + tree[right]; i /= 2; &#125; &#125; public int sumRange(int i, int j) &#123; i += n; j+= n; int sum = 0; while (i &lt;= j) &#123; if ((i % 2) == 1) &#123; sum += tree[i++]; &#125; if ((j % 2) == 0) &#123; sum += tree[j--]; &#125; i /= 2; j /= 2; &#125; return sum; &#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>SegmentTree</tag>
        <tag>FenwickTree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Topological Sort]]></title>
    <url>%2FAlgorithm%2Falgorithm%2Ftopological-sort%2F</url>
    <content type="text"><![CDATA[Topological SortTopological Sort 算法用于解决有向无环图 (DAG, directed acyclic graph) 中的依赖解析问题。在一系列有先后顺序的任务中，运用 Topological Sort 可以得到满足执行顺序限制条件的一系列任务所需执行的先后顺序。 参考文档: Topological Sort(DFS)Topological Sort(Kahn’s)All Topological Sorts 参考代码 DAG 说明 上图是一个典型的 DAG，可以使用 Graph 类进行存储。 12345678910111213141516171819202122class Graph &#123; private int V; // vertex，顶点数 private List&lt;Integer&gt; adj[]; // edges, 边 public Graph(int v) &#123; this.V = v; this.adj = new List&lt;Integer&gt;[v]; for (int i = 0; i &lt; v; i++) adj[i] = new LinkedList&lt;&gt;(); &#125; public void addEdge(int v, int w) &#123; this.adj[v].add(w); &#125;&#125;Graph g = new Graph(6);g.addEdge(5, 0);g.addEdge(4, 0);g.addEdge(5, 2);g.addEdge(4, 1);g.addEdge(2, 3);g.addEdge(3, 1); 基于 DFS 的拓扑排序12345678910111213141516171819List&lt;Integer&gt; topoSortByDFS(Graph g) &#123; boolean visited[] = new boolean[g.V]; Arrays.fill(visited, false); Deque&lt;Integer&gt; stack = new ArrayDeque&lt;&gt;(g.V); for (int i = 0; i &lt; g.V; i++) &#123; if (!visited[i]) dfs(i, g, visited, stack); &#125; List&lt;Integer&gt; res = new ArrayList&lt;&gt;(g.V); while (!stack.isEmpty()) res.add(stack.pop()); return res;&#125;private void dfs(int v, Graph g, boolean[] visited, Deque&lt;Integer&gt; stack) &#123; visited[v] = true; for (Integer i : g.ajd[v]) &#123; if (!visited[i]) dfs(i, g, visited, stack); &#125; stack.push(v);&#125; Kahn’s Topological Sort 入度: 设 DAG 有一个节点 v, 入度即为当前所有从其他节点出发，终点为 v 的边的数目。 出度: 设 DAG 有一个节点 v, 出度即为从 v 出发，到所有其他节点的边的数。 Kahn 算法核心思想为不断寻找入度为 0 的节点，入度为 0 表示当前节点不依赖其他节点或者当前节点依赖的节点已完成相应操作。 12345678910111213141516171819202122List&lt;Integer&gt; topoSortByKahn(Graph g) &#123; int[] inDegree = new int[g.V]; for (int v = 0; v &lt; g.V; v++) &#123; for (int w : g.adj[v]) &#123; inDegree[w]++; // 每个节点的入度 &#125; &#125; Queue&lt;Integer&gt; queue = new LinkedList&lt;&gt;(); for (int v = 0; v &lt; g.V; v++) &#123; if (inDegree[v] == 0) queue.offer(v); &#125; List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); while (!queue.isEmpty()) &#123; int v = queue.poll(); res.add(v); for (int w : g.adj[v]) &#123; inDegree[w]--; if (inDegree[w] == 0) queue.offer(w); &#125; &#125; return res;&#125; Find all TopoSort of DAG123456789101112131415161718192021222324252627List&lt;List&lt;Integer&gt;&gt; findAllTopoSort(Graph g) &#123; int[] inDegree = new int[g.V]; for (int u = 0; u &lt; g.V; u++) &#123; for (int w : g.adj[u]) inDegree[w]++; &#125; boolean[] visited = new boolean[g.V]; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); findAllTopoSortOrder(res, new ArrayList&lt;&gt;(), g, inDegree, visited); return res;&#125;void findAllTopoSortOrder(List&lt;List&lt;Integer&gt;&gt; res, List&lt;Integer&gt; temp, Graph g, int[] inDegree, boolean[] visited) &#123; for (int u = 0; u &lt; g.V; u++) &#123; if (inDegree[u] != 0 || visited[u]) continue; for (int w : g.adj[u]) inDegree[w]--; temp.add(u); visited[u] = true; findAllTopoSortOrder(res, temp, g, inDegree, visited); for (int w : g.adj[u]) inDegree[w]++; visited[u] = false; temp.remove(temp.size()-1); &#125; if (temp.size() == g.V) res.add(new ArrayList&lt;&gt;(temp));&#125; 应用LeetCode 2071234567891011121314151617181920212223242526class Solution &#123; public boolean canFinish(int numCourses, int[][] prerequisites) &#123; int[] inDegree = new int[numCourses]; Map&lt;Integer, List&lt;Integer&gt;&gt; map = new HashMap&lt;&gt;(); for (int[] pre : prerequisites) &#123; map.putIfAbsent(pre[1], new ArrayList&lt;&gt;()); map.get(pre[1]).add(pre[0]); inDegree[pre[0]]++; &#125; Queue&lt;Integer&gt; queue = new LinkedList&lt;&gt;(); for (int i = 0; i &lt; numCourses; i++) &#123; if (inDegree[i] == 0) queue.offer(i); &#125; while (!queue.isEmpty()) &#123; int i = queue.poll(); numCourses--; for (int j : map.getOrDefault(i, new ArrayList&lt;&gt;())) &#123; inDegree[j]--; if (inDegree[j] == 0) queue.offer(j); &#125; &#125; return numCourses == 0; &#125;&#125; LeetCode 210123456789101112131415161718192021222324252627282930class Solution &#123; public int[] findOrder(int numCourses, int[][] prerequisites) &#123; int[] inDegree = new int[numCourses]; Map&lt;Integer, List&lt;Integer&gt;&gt; map = new HashMap&lt;&gt;(); for (int[] pre : prerequisites) &#123; map.putIfAbsent(pre[1], new ArrayList&lt;&gt;()); map.get(pre[1]).add(pre[0]); inDegree[pre[0]]++; &#125; Queue&lt;Integer&gt; queue = new LinkedList&lt;&gt;(); for (int i = 0; i &lt; numCourses; i++) &#123; if (inDegree[i] == 0) queue.offer(i); &#125; int size = 0; int[] res = new int[numCourses]; while (!queue.isEmpty()) &#123; int i = queue.poll(); numCourses--; res[size++] = i; for (int j : map.getOrDefault(i, new ArrayList&lt;&gt;())) &#123; inDegree[j]--; if (inDegree[j] == 0) queue.offer(j); &#125; &#125; return numCourses == 0 ? res : new int[0]; &#125;&#125; LeetCode 269该题目看懂题目很重要！！！！！ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class Solution &#123; public String alienOrder(String[] words) &#123; Map&lt;Character, Set&lt;Character&gt;&gt; graph = new HashMap&lt;&gt;(); for (int i = 0; i &lt; words.length - 1; i++) &#123; for (int j = 0; j &lt; words[i].length() &amp;&amp; j &lt; words[i + 1].length(); j++) &#123; //如果字符相同，比较下一个 char u = words[i].charAt(j), w = words[i + 1].charAt(j); if (u == w) continue; //保存第一个不同的字符顺序 graph.putIfAbsent(u, new HashSet&lt;&gt;()); graph.get(u).add(w); break; &#125; &#125; int[] inDegree = new int[26]; Arrays.fill(inDegree, -1); int count = 0; for (String word : words) &#123; for (char c : word.toCharArray()) &#123; if (inDegree[c-'a'] == -1) count++; inDegree[c-'a'] = 0; &#125; &#125; for (char c : graph.keySet()) &#123; for (char next : graph.get(c)) &#123; inDegree[next-'a']++; &#125; &#125; Queue&lt;Character&gt; queue = new LinkedList&lt;&gt;(); StringBuilder sb = new StringBuilder(); for (char c = 'a'; c &lt;= 'z'; c++) &#123; if (inDegree[c-'a'] == 0) queue.offer(c); &#125; while (!queue.isEmpty()) &#123; Character c = queue.poll(); sb.append(c); for (Character next : graph.getOrDefault(c, new HashSet&lt;&gt;())) &#123; inDegree[next-'a']--; if (inDegree[next-'a'] == 0) queue.offer(next); &#125; &#125; // System.out.println(count); // System.out.println(sb.toString()); return sb.length() == count ? sb.toString() : ""; &#125;&#125; LeetCode 120312345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class Solution &#123; public int[] sortItems(int n, int m, int[] group, List&lt;List&lt;Integer&gt;&gt; beforeItems) &#123; Map&lt;Integer, Set&lt;Integer&gt;&gt; itemGraph = new HashMap&lt;&gt;(); Map&lt;Integer, Integer&gt; itemInDegree = new HashMap&lt;&gt;(); for (int i = 0; i &lt; n; i++) itemGraph.putIfAbsent(i, new HashSet&lt;&gt;()); Map&lt;Integer, Set&lt;Integer&gt;&gt; groupGraph = new HashMap&lt;&gt;(); Map&lt;Integer, Integer&gt; groupInDegree = new HashMap&lt;&gt;(); for (int i : group) groupGraph.putIfAbsent(i, new HashSet&lt;&gt;()); for (int after = 0; after &lt; n; after++) &#123; for (int before : beforeItems.get(after)) &#123; itemGraph.get(before).add(after); itemInDegree.put(after, itemInDegree.getOrDefault(after, 0)+1); int gbefore = group[before]; int gafter = group[after]; if (gbefore != gafter &amp;&amp; groupGraph.get(gbefore).add(gafter)) &#123; groupInDegree.put(gafter, groupInDegree.getOrDefault(gafter, 0)+1); &#125; &#125; &#125; List&lt;Integer&gt; itemList = topologicalSort(itemGraph, itemInDegree); List&lt;Integer&gt; groupList = topologicalSort(groupGraph, groupInDegree); // System.out.println(Arrays.toString(itemList.toArray())); // System.out.println(Arrays.toString(groupList.toArray())); if (itemList.size() == 0 || groupList.size() == 0) return new int[0]; int[] res = new int[n]; Map&lt;Integer, List&lt;Integer&gt;&gt; map = new HashMap&lt;&gt;(); for (int item : itemList) &#123; int g = group[item]; map.putIfAbsent(g, new ArrayList&lt;&gt;()); map.get(g).add(item); &#125; int i = 0; for (int g : groupList) &#123; for (int item : map.get(g)) &#123; res[i++] = item; &#125; &#125; return res; &#125; private List&lt;Integer&gt; topologicalSort(Map&lt;Integer, Set&lt;Integer&gt;&gt; graph, Map&lt;Integer, Integer&gt; inDegree) &#123; List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); Queue&lt;Integer&gt; queue = new LinkedList&lt;&gt;(); for (int i : graph.keySet()) &#123; if (inDegree.getOrDefault(i, 0) == 0) queue.offer(i); &#125; while (!queue.isEmpty()) &#123; int item = queue.poll(); res.add(item); for (int next : graph.get(item)) &#123; inDegree.put(next, inDegree.getOrDefault(next, 0)-1); if (inDegree.get(next) == 0) queue.offer(next); &#125; &#125; return res.size() == graph.size() ? res : new ArrayList&lt;&gt;(); &#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>TopoSort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Union Find]]></title>
    <url>%2FAlgorithm%2Falgorithm%2Funion-find%2F</url>
    <content type="text"><![CDATA[Union Find并查集原理，主要参考：并查集（Disjoint-set union）第1讲并查集（Disjoint-set union）第2讲花花酱 Disjoint-set/Union-find Forest 学习整理后完成。 原理讲解 对于一个无向图，我们要如何才能找到是否存在环。 由图可知，我们通过构建 parent 数组，将各个节点关系使用有向图的形式进行构建。当遇到无法 union 的时候，表示两个节点具有相同的父节点，即寻找到环。 1234567891011121314151617181920212223class UnionFind &#123; private int[] parents; public UnionFind(int size) &#123; this.parents = new int[size]; Arrays.fill(this.parents, -1); &#125; public int findRoot(int x) &#123; int root = x; while (parents[root] != -1) &#123; root = parents[root]; &#125; return root; &#125; public boolean union(int x, int y) &#123; int xRoot = findRoot(x); int yRoot = findRoot(y); if (xRoot == yRoot) return false; parents[xRoot] = yRoot; return true; &#125;&#125; 优化以上不难看出，当数据量较大时候， findRoot 函数，将执行许多遍，并且都是无效的查找。如果我们在查找过程中，直接将其指向根节点，使得树深度变低，能够很好的加快查询效率。 此时对上面代码作出修改，如下： 1234567891011121314151617181920212223242526class UnionFind &#123; private int[] parents; public UnionFind(int size) &#123; this.parents = new int[size]; for (int i = 0; i &lt; size; i++) &#123; parents[i] = i; &#125; &#125; public int findRoot(int x) &#123; int root = x; while (parents[root] != root) &#123; parents[root] = parents[parents[root]]; root = parents[root]; &#125; return root; &#125; public boolean union(int x, int y) &#123; int xRoot = findRoot(x); int yRoot = findRoot(y); if (xRoot == yRoot) return false; parents[xRoot] = yRoot; return true; &#125;&#125; 应用LeetCode 547 Friend Circles12345678910111213public int findCircleNum(int[][] M) &#123; int N = M.length; UnionFind uf = new UnionFind(N); for (int i = 0; i &lt; N; i++) &#123; for (int j = 0; j &lt; N; j++) &#123; if (M[i][j] == 1) uf.union(i, j); &#125; &#125; Set&lt;Integer&gt; set = new HashSet&lt;&gt;(); for (int i = 0; i &lt; N; i++) set.add(uf.findRoot(i)); return set.size();&#125; LeetCode 684 Redundant Connection12345678910public int[] findRedundantConnection(int[][] edges) &#123; int n = edges.length; UnionFind uf = new UnionFind(n+1); for (int[] edge : edges) &#123; if (!uf.union(edge[0], edge[1])) &#123; return edge; &#125; &#125; return null;&#125; LeetCode 733 Sentence Similarity II123456789101112131415161718192021public boolean areSentencesSimilarTwo(String[] words1, String[] words2, List&lt;List&lt;String&gt;&gt; pairs) &#123; if (words1 == null || words2 == null) return false; if (words1.length != words2.length) return false; Map&lt;String, Integer&gt; map = new HashMap&lt;&gt;(); int size = 0; for (String s : words1) map.putIfAbsent(s, size++); for (String s : words2) map.putIfAbsent(s, size++); for (List&lt;String&gt; ls : pairs) &#123; for (String s : ls) map.putIfAbsent(s, size++); &#125; UnionFind uf = new UnionFind(size); for (List&lt;String&gt; ls : pairs) &#123; uf.union(map.get(ls.get(0)), map.get(ls.get(1))); &#125; for (int i = 0; i &lt; words1.length; i++) &#123; int root1 = uf.findRoot(map.get(words1[i])); int root2 = uf.findRoot(map.get(words2[i])); if (root1 != root2) return false; &#125; return true;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Union Find</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode Minimax (486, 877)]]></title>
    <url>%2FLeetcode%2Fleetcode%2Fleetcode-Minimax%2F</url>
    <content type="text"><![CDATA[Leetcode 486, 877题目描述给定一个表示分数的非负整数数组。 玩家1从数组任意一端拿取一个分数，随后玩家2继续从剩余数组任意一端拿取分数，然后玩家1拿，……。每次一个玩家只能拿取一个分数，分数被拿取之后不再可取。直到没有剩余分数可取时游戏结束。最终获得分数总和最多的玩家获胜。 给定一个表示分数的数组，预测玩家1是否会成为赢家。你可以假设每个玩家的玩法都会使他的分数最大化。 1234输入: [1, 5, 233, 7]输出: True解释: 玩家1一开始选择1。然后玩家2必须从5和7中进行选择。无论玩家2选择了哪个，玩家1都可以选择233。最终，玩家1（234分）比玩家2（12分）获得更多的分数，所以返回 True，表示玩家1可以成为赢家。 解题思路 Recursive1234567return score(s, 0, n-1) &gt; 0def score(s, l, r): if l &gt; r : return 0 if l == r : return s[l] return max(s[l] - score(s,l+1,r), s[r] - score(s,l,r-1)) 以上函数是最直观的认识，每一次都取最大的分数。该算法时间复杂度 $O(2^n)$，空间复杂度为$O(n)$。 Recursive + Memory12345678return score(s, 0, n-1) &gt; 0def score(s, l, r, m): if l &gt; r : return 0 if l == r : return s[l] if m[l][r] : return m[l][r] return max(s[l] - score(s,l+1,r), s[r] - score(s,l,r-1)) DPdp[i][j] 表示 i~j 最好的分数。 dp[i][j] = max\begin{cases} nums[i]-dp[i+1][j]\\ nums[j]-dp[i][j-1]\\ \end{cases} ,i = n\to 0, j = i+1 \to n123456789101112public boolean stoneGame(int[] nums) &#123; if (nums == null || nums.length == 0) return false; int len = nums.length; int[][] dp = new int[len][len]; for (int i = len; i &gt;= 0; i--) &#123; for (int j = i+1; j &lt; len; j++) &#123; dp[i][j] = Math.max(nums[i] - dp[i+1][j], nums[j] - dp[i][j-1]); &#125; &#125; return dp[0][len-1] &gt;= 0;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Dynamic Programming</tag>
        <tag>Minimax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Stack And PC]]></title>
    <url>%2FLinux%2Flinux%2Fstack-and-pc%2F</url>
    <content type="text"><![CDATA[知识得来终觉浅，因此还是应该将收获的知识记录下来比较有用。本文主要在 X86 体系结构下，函数栈调用的过程以及 PC 变化过程，以此对 C/C++、Java 等高级语言中的函数有一个深刻的认识。本文使用 64 位编译器进行编译。主要讲函数栈调用及 PC 变化过程，其他基础知识仅提及而不深入讲解。 参考资料： C程序的汇编 Introduction_to_x64_Assembly 基础知识寄存器介绍X64 提供了 16 个通用寄存器，分别为 %rax，%rbx，%rcx，%rdx，%rsi，%rdi，%rbp，%rsp，%r8，%r9，%r10，%r11，%r12，%r13，%r14，%r15。在 C 语言调用时输入参数前6位依次使用 rdi, rsi, rdx, rcx, r8, r9 寄存器，后面的参数压栈使用。在 C 语言中返回结果一律记在 rax 中进行返回。 caller register: rax, rcx, rdx, r8, r9, r10, r11callee register: rbx, rbp, rdi, rsi, rsp, r12, r13, r14 关键寄存器说明，以下三个指针在程序运行过程中起着关键的作用。 寄存器 说明 rbp 基指针 rsp 栈指针 rip 程序计数器 GDB 基础命令以下只给出部分实验中需要使用到的命令，具体命令可查看相应的手册。 命令 作用 i r a info register all，查看所有寄存器中数据 i r rax info register rax， 查看指定寄存器数据 n next 执行下一条指令，函数直接执行而不进入 s step 进入函数内部调试 b func/34 breakpoint 断点 r run 运行 disas disassem 反汇编，将机器码转为汇编便于阅读 si，ni 针对汇编 GCC 编译，将 C 语言编译为汇编代码。 1gcc -S add.c -o add.s 链接 1gcc -g add.c -o add 相关参数 -o：指定生成的输出文件； -E：仅执行编译预处理； -S：将C代码转换为汇编代码； -Wall：显示警告信息； -c：仅执行编译操作，不进行连接操作。 -g：表示在生成的目标文件中带调试信息 Linux X64 内存空间结构在 X64 系统架构中，只使用 48 根地址线，虚拟内存空间划分时将 0xFFFF0000~00000000——0xFFFFFFFF~FFFFFFFF 划分给内核使用，0x00000000~00000000——0x0000FFFF~FFFFFFFF 划分给进程使用。 进程分布图如下所示： 实验准备 实验均在 Linux 系统中进行 C 代码1234567891011121314151617// add.cint sub(int a, int b) &#123; return a-b;&#125;int add(int a, int b) &#123; a = sub(a, b); return a+b;&#125;int main(int argc, char *argv[]) &#123; int a, b, res; a = 3; b = 2; res = add(a,b); return 0;&#125; 汇编代码通过调用 gcc -S add.c -o add.s 可获得相应的汇编代码如下所示。 去除无关对齐代码后！！！自己编译后的代码有很多的对齐代码。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253.file "add.c".text.globl sub.type sub, @functionsub: pushq %rbp movq %rsp, %rbp movl %edi, -4(%rbp) movl %esi, -8(%rbp) movl -4(%rbp), %eax subl -8(%rbp), %eax popq %rbp ret.globl add.type add, @functionadd: pushq %rbp movq %rsp, %rbp subq $8, %rsp movl %edi, -4(%rbp) movl %esi, -8(%rbp) movl -8(%rbp), %edx movl -4(%rbp), %eax movl %edx, %esi movl %eax, %edi call sub movl %eax, -4(%rbp) movl -4(%rbp), %edx movl -8(%rbp), %eax addl %edx, %eax leave ret.globl main.type main, @functionmain: pushq %rbp movq %rsp, %rbp subq $32, %rsp movl %edi, -20(%rbp) movq $rsi, -32(%rbp) movl $3, -12(%rbp) movl $2, -8(%rbp) movl -8(%rbp), %edx movl -12(%rbp), %eax movl %edx, %esi movl %eax, %edi call add movl %eax, -4(%rbp) movl $0, %eax leav ret C 与汇编联合编译 实验所需要的三个文件在下面提供下载，可直接点击下载，并在同一个文件夹中。以便实验室用。 main.c 点击下载 add.s 点击下载 makefile 点击下载 实验开始 下载 main.c, add.s, makefile 到同一文件夹中。 执行 make 命令完成代码编译链接，生产 test 执行文件。ps: make clean 用于清理。 运行到 add 函数调用前 gdb --tui ./test b main r b 16 c disas 查看当前运行到的汇编位置，并用 ni 执行一条汇编，一点点执行，直到运行到 0x000000000040054d &lt;+39&gt;: callq 0x400582 &lt;add&gt; 此时查看堆栈寄存器 rbp, rsp 数据以及 程序计数器(pc)寄存器 rip 数据。 查看 rbp, i r rbp 查看 rsp, i r rsp 查看 rip, i r rip 查看当前上下文汇编代码, disas 1234567891011121314Dump of assembler code for function main: 0x0000000000400526 &lt;+0&gt;: push %rbp 0x0000000000400527 &lt;+1&gt;: mov %rsp,%rbp 0x000000000040052a &lt;+4&gt;: sub $0x20,%rsp 0x000000000040052e &lt;+8&gt;: mov %edi,-0x14(%rbp) 0x0000000000400531 &lt;+11&gt;: mov %rsi,-0x20(%rbp) 0x0000000000400535 &lt;+15&gt;: movl $0x3,-0xc(%rbp) 0x000000000040053c &lt;+22&gt;: movl $0x2,-0x8(%rbp) 0x0000000000400543 &lt;+29&gt;: mov -0x8(%rbp),%edx 0x0000000000400546 &lt;+32&gt;: mov -0xc(%rbp),%eax 0x0000000000400549 &lt;+35&gt;: mov %edx,%esi 0x000000000040054b &lt;+37&gt;: mov %eax,%edi =&gt; 0x000000000040054d &lt;+39&gt;: callq 0x400582 &lt;add&gt; 0x0000000000400552 &lt;+44&gt;: mov %eax,-0x4(%rbp) 从上面查到的几个信息中，我们可以看到 rbp=0x00007fffffffe460; rsp=0x00007fffffffe440; rip=0x000000000040054d这与我们认知的程序的代码分布一致。栈由高地址空间0x0000FFFFFFFFFFFF 向下扩展，代码段由低地址空间向上排列。在这里为何 rsp 比 rbp 小0x20，这与 sub $0x20, %rsp 这一指令相关，该指令通过计算当前栈帧所需栈大小从而预留使用空间，当然此时 0x20 是要大于实际所需要空间大小的。通过计算实际 main 函数中，只有 a(int)、b(int)、res(int)、argc(int)、argv(char )，int 使用 4 bytes，char 使用 8 bytes，实际只需要 24 bytes即可，但此时却分配 32 bytes，这与内存对齐有关，如果对 Cache 相关知识有一定理解的话，应该能够明白是怎么回事。 此时有必要将当前整个栈结构画出来，可以通过 x/20x 0x00007fffffffe440 查看相应的内存数据。 画成栈图如下所示： 当前 rip: 400543, 下一条pc是 400552。要注意调用 call 函数后是如何压栈的！！！！！记住400552这条数据！！！！ 各个参数对应的内存位置在图中已标出，很明显能够发现有部分空余空间没事使用。验证之前提到的，为了内存对齐存在内存浪费。 调用 callq add，进入 add 函数 s 此时，我们重新看一下栈内存的数据，栈指针信息，以及 pc 信息。 查看 rbp, i r rbp 查看 rsp, i r rsp 查看 rip, i r rip 查看栈内存数据, x/20x 0x00007fffffffe430 画成栈图如下所示： callq add, 相当于执行 pushq nextpc; jmp add callq add, 将下一条需要执行的 PC 压入栈中！！ add 栈帧初始化所谓 add 栈帧初始化，就是将 rbp、rsp 进行整理，为 add 函数执行做准备。主要指令如下： 123pushq %rbp // 将rbp压栈movq %rsp, %rbp // rsp 与 rbp 指向同一地址subq $8, %rsp // 计算 add 所需内存空间 执行完以上三条指令后，新的栈图如下： 运行到 sub 函数运行前 b 26 c 新的栈图如下： 调用 callq sub, 进入 sub 函数 s 新的栈图如下： sub 栈帧初始化12pushq %rbp // 将rbp压栈movq %rsp, %rbp // rsp 与 rbp 指向同一地址 这里有一点要特别注意，在 add 中，我们会使用 subq $8, %rsp 初始化一个栈空间，但在这里没有进行这一操作，原因在于 add 函数中需要调用 sub 函数，而 sub 函数中不需要调用任何函数。 运行到 sub popq %rbp 之前 (*) popq %rbp由于当前 rsp, rbp 都指向 0x00007fff~ffffe418，此时调用 popq %rbp 相当于执行 以下两条指令 12movq +8(%rsp), %rbpaddq $8, %rsp (*) ret 退出 sub 函数ret 实际做的就是恢复 rip 指向原函数需要执行的代码。相当于 123movq +8(%rsp), %ripaddq $8, %rspjmp %rip 此时 rsp, rbp 恢复到调用 sub 函数之前的状态。 (*) leaveqleaveq 相当于以下两条指令12movq %rbp, %rsppopq %rbp 与上面12pushq %rbpmovq %rsp, %rbp 对应。 (*) ret 退出 add 函数 实验整体过程 总结可能以上的过程写的比较繁杂，但是如果细细看，并且能够将最后整个栈过程完整画出来的话，应该能够对函数调用有一个十分深刻的印象。PS：如果需要的话，后期我可以录制一个视频，详细讲解下相关的知识，以做备份。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Computer CPU]]></title>
    <url>%2FLinux%2Flinux%2Fcomputer-cpu%2F</url>
    <content type="text"><![CDATA[本文主要介绍 CPU 如何一步一步进行优化，最终成为当前这样的体系架构。本文知识主要来源于 深入浅出计算机组成原理 、计算机体系结构（量化研究方法） 、计算机组成与设计硬件/软件接口。 CPU 性能与功耗性能 响应时间（执行时间），表示程序运行所需要的时间，体现了 CPU 运行速度。 吞吐率（带宽），表示程序运行能够处理的数据，体现了 CPU 处理数据的能力。 响应时间可以由以下公式决定： CPU时间 = 指令数 \times CPI \times 时钟频率 指令数：Instruction Num，由编译器以及程序员决定。 CPI(Cycles Per Instruction)：每个指令周期，通过 PipeLine、超流水线 等技术使得一条指令所需 CPU Cycle 尽可能地低。对 CPI 的优化是计算机体系结构中最重要的一环。 时钟频率：Clock Cycle Time，由计算机主频决定。 吞掉量通过超标量(Superscalar)、超线程、SIMD 等技术在不改变响应时间基础上，使得系统在相同时间内能够处理更多的数据。 功耗我们的 CPU，一般被被叫做超大规模集成电路(Very Large Scale Integration, VLSI)，CPU 实际是由大量的集成电路组成。功耗公式如下: 功耗 \approx 1/2 \times 负载电容 \times 电压^{2} \times 晶体管数量提高 CPU 主频则需要加大电压、增加晶体管，这将导致功耗不断上升，所有元器件均有熔点，因此我们不能不断地增大功耗。这就是功耗墙的来源。 既然，我们无法通过不断增加频率来提高 CPU 的“响应时间”，因此有人就提出了提高“吞吐率”使 CPU 在相同时间内能够处理更多的数据，即通过并行提升性能。 但是程序的并行度也不是能够一直提升的，这就涉及到 Amdahl’s Law，即优化后的执行时间也是有上限的，该部分主要受无法并行化的代码所花费时间的影响。 优化后的执行时间 = 受优化影响的执行时间/加倍速度 + 不受影响的执行时间 优化手段CPU 的优化单指令周期 CPU对于 CPU 而言，每次都需要处理一条指令，该部分指令由时钟频率驱动，一个 Cycle 即执行一条指令。由于不同指令所需实际执行时间不同，浮点数乘法所消耗的时间一定大于简单的移动指令。因此单指令周期的 CPU，都需要等待满一个时钟周期，浪费了等待时间，同时限制了时钟频率的提升。 PipeLine为了提高 CPU 的时钟频率，通过将指令按照不同的小步骤进行拆分，即形成了流水线工作。 此时不再需要确保最复杂的那条指令在一个时钟周期内完成，而只需要保证最复杂的流水线级的操作在一个时钟周期内完成即可。 通过 PipeLine 处理后，可以大大提升 CPU 主频，提升运行速度。现代的 ARM 或者 Intel 的 CPU，流水线级数可以到14级。 超长流水线瓶颈，增加流水线深度，其实是有性能成本，在读写流水线寄存器时，会有开销。同时还会带来结构冒险、数据冒险、控制冒险等其他依赖问题。为了应付各类冒险问题，采用乱序执行、分支预测等解决方案。流水线越长，这一类冒险问题就更难解决。 结构冒险所谓结构冒险，就是在访问内存和取指令的时候，都需要操作程序内存，但是我们只有一个地址译码器，此时便会产生资源冲突。 为了解决该冲突，可以将内存分为两部分，一个用于存放指令，另一个存放数据。 把内存拆成两部分的解决方案，被称为哈佛架构，我们使用的冯·诺依曼体系结构又叫普林斯顿架构，该体系结构没有拆分内存，而是在高速缓存部分分成指令缓存和数据缓存。 数据冒险 先写后读，数据依赖 1234567int main() &#123; int a = 1; int b = 2; // 先写1 再读1 a = a + 2; b = a + 3;&#125; 先读后写，反依赖 1234567int main() &#123; int a = 1; int b = 2; a = b + a; // 若下面早于上面完成 b = a + b;&#125; 写后再读，输出依赖 12345int main() &#123; int a = 1; // 先写1 再写1 a = 2;&#125; 使用 nop 保证有数据依赖的执行在上一条执行后继续。 操作数转发，将前一条指令执行结果直接作为后一条指令的输入。减少 nop。 由于不同指令不存在依赖关系，可以通过乱序执行，将没有依赖关系的指令提前，减少 nop 时间。乱序执行，有点类似于 线程池。 控制冒险以上假设都是基于代码都是顺序执行的，取指令 和 指令译码 不会遇到任何停顿，当遇到跳转指令时，为了保证争取，不得不等待造成延迟。 当有判断或跳转时，便于要分支预测的来提高运行效率，否则就需要通过 nop 直至此时判断完成。 分支预测，便是假装分支不发生，即静态预测成功率在50%，当发现错误时，就将不要的数据丢弃。 动态分支预测，即今天下雨，就猜明天也下雨。即用1比特保存当前预测结果，并用该结果作为下一次分支比较。还可以用2比特预测，叫双模态预测。 注意在不改变代码基础上，将循环少的放在外面。 Superscalar 和 VLIW使得 CPU 吞吐率大于1Superscalar超标量(Superscalar)，即多发射的结构，是在原有 PipeLine 基础上，通过增加多条流水线，达到一次取多条指令执行。如此并能使得 CPU 吞吐率高于 1。 此时需要增加的只有两个部分：修改取指令一次多个；增加多个译码器。译码完成的代码即可分发器被分发到各个执行 ALU 单元。 通过指令多发射设计，可以得到超标量，即并行执行多条指令。 VLIWVLIW(Very Long Instruction Word)，通过将指令打包，使得一次执行时能够执行多条指令，从而提升吞吐率。 但是该方法由于无法向前兼容，最后导致失败。 超线程超长流水线，意味着在流水线中指令越多，相互依赖关系越多，不得不面临冒险。2002 年底，Intel 在的 3.06GHz 主频的 Pentium 4 CPU上，第一次引入了超线程技术。超线程技术，就是在CPU中同时执行多个程序的指令。增加一份PC寄存器、指令寄存器、条件码寄存器。 SIMDSIMD，Single Instruction Multiple Data。SIMD 在获取数据和执行指令时，都做到了并行，一方面，再从内存读取数据的时候，SIMD是一次性读取多个数据，在SSE指令集中，CPU添加了8个16Bytes的寄存器，一个寄存器一次性可以加载4个整数。 Numpy，使用 SIMD 技术。 基于 SIMD 的向量计算指令，被称为 MMX（Matrix Math eXtensions） 指令集，即矩阵数学扩展。 总结 单指令周期 CPU 需要保证最耗时的指令在一个时钟周期内完成，限制主频发展。因此改进成 PipeLine。 PipeLine 可以分为 IF、ID、EX、MEM、WB， 只需要保证最复杂的流水线级的操作在一个时钟周期内完成即可，极大提升了主频发展。但是 PipeLine 会导致 结构冒险、数据冒险、控制冒险 等。结构冒险，通过 D-L1 Cache、I-L1 Cache 防止冲突。数据冒险，通过增加 nop、数据转发、乱序执行 等手段防止冒险出现。控制冒险，通过 分支预测 防止。 为了使 CPU 吞吐率高于 1，采用 Superscalar 和 VLIW 技术。Superscalar 利用 IF 一次取多条指令，并由多个 ID 译码，并分发给 ALU，从而达到多条流水线同时执行的效率。VLIW 将多个指令打包成一个指令，从而在执行时，同时执行多条，但是由于无法向前兼容最终淘汰。 Superscalar 导致多条流水线间的依赖关系过于复杂，不得不面临冒险。为减少冲突因此提出超线程手段，通过增加一份 PC寄存器、指令寄存器、条件码寄存器 使得一个 CPU 可以同时运行两个无关的线程代码。 SIMD，Single Instruction Multiple Data。通过一条指令操作多条数据，从而增加吞吐率，该技术也促使了 GPU 相关的发展。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>CPU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode The Skyline Problem 218]]></title>
    <url>%2FLeetcode%2Fleetcode%2Fleetcode-The-Skyline-Problem-218%2F</url>
    <content type="text"><![CDATA[Leetcode 218. The Skyline Problem题目描述城市的天际线是从远处观看该城市中所有建筑物形成的轮廓的外部轮廓。现在，假设您获得了城市风光照片（图A）上显示的所有建筑物的位置和高度，请编写一个程序以输出由这些建筑物形成的天际线 每个建筑物的几何信息用三元组 [Li，Ri，Hi] 表示，其中 Li 和 Ri 分别是第 i 座建筑物左右边缘的 x 坐标，Hi 是其高度。可以保证 0 ≤ Li, Ri ≤ INT_MAX, 0 &lt; Hi ≤ INT_MAX 和 Ri - Li &gt; 0。您可以假设所有建筑物都是在绝对平坦且高度为 0 的表面上的完美矩形。 例如，图A中所有建筑物的尺寸记录为：[ [2 9 10], [3 7 15], [5 12 12], [15 20 10], [19 24 8] ] 。 输出是以 [ [x1,y1], [x2, y2], [x3, y3], … ] 格式的“关键点”（图B中的红点）的列表，它们唯一地定义了天际线。关键点是水平线段的左端点。请注意，最右侧建筑物的最后一个关键点仅用于标记天际线的终点，并始终为零高度。此外，任何两个相邻建筑物之间的地面都应被视为天际线轮廓的一部分。 例如，图B中的天际线应该表示为：[ [2 10], [3 15], [7 12], [12 0], [15 10], [20 8], [24, 0] ]。 说明: 任何输入列表中的建筑物数量保证在 [0, 10000] 范围内。输入列表已经按左 x 坐标 Li 进行升序排列。输出列表必须按 x 位排序。输出天际线中不得有连续的相同高度的水平线。例如 […[2 3], [4 5], [7 5], [11 5], [12 7]…] 是不正确的答案；三条高度为 5 的线应该在最终输出中合并为一个：[…[2 3], [4 5], [12 7], …] 解题思路 将 [2,9,10] 转化为 [2,-10], [9,10] 从左往右排序 从左往右扫描线，每次从 PriorityQueue 中取出当前最高的点，与前一个点比较，如果是新的点则加入，否则下一次循环。 结果12345678910111213141516171819202122232425public List&lt;List&lt;Integer&gt;&gt; getSkyline(int[][] buildings) &#123; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); List&lt;int[]&gt; heights = new ArrayList&lt;&gt;(); for (int[] build: buildings) &#123; heights.add(new int[]&#123;build[0], -build[2]&#125;); heights.add(new int[]&#123;build[1], build[2]&#125;); &#125; Collections.sort(heights, (a,b)-&gt; a[0] == b[0] ? a[1] - b[1] : a[0] - b[0]); PriorityQueue&lt;Integer&gt; pq = new PriorityQueue&lt;&gt;((a,b)-&gt;b-a); pq.offer(0); int prev = 0; for (int[] h: heights) &#123; if (h[1] &lt; 0) &#123; pq.offer(-h[1]); &#125; else &#123; pq.remove(h[1]); &#125; int cur = pq.peek(); if (cur != prev) &#123; res.add(Arrays.asList(h[0], cur)); prev = cur; &#125; &#125; return res;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Heap</tag>
        <tag>Line Sweep</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode house rob 198, 213, 337]]></title>
    <url>%2FLeetcode%2Fleetcode%2Fleetcode-house-rob-198-213-337%2F</url>
    <content type="text"><![CDATA[Leetcode 198. House Rob I题目描述你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。 给定一个代表每个房屋存放金额的非负整数数组，计算你在不触动警报装置的情况下，能够偷窃到的最高金额。 输入: [2,7,9,3,1]输出: 12解释: 偷窃 1 号房屋 (金额 = 2), 偷窃 3 号房屋 (金额 = 9)，接着偷窃 5 号房屋 (金额 = 1)。 偷窃到的最高金额 = 2 + 9 + 1 = 12 。 解题思路 结果12345678910public int rob(int[] nums) &#123; if (nums == null || nums.length == 0) return 0; int notRob = 0, curRob = 0; for (int num : nums) &#123; int preRob = Math.max(notRob, curRob); curRob = notRob + num; notRob = preRob; &#125; return Math.max(curRob, notRob);&#125; Leetcode 213. House Rob II题目描述你是一个专业的小偷，计划偷窃沿街的房屋，每间房内都藏有一定的现金。这个地方所有的房屋都围成一圈，这意味着第一个房屋和最后一个房屋是紧挨着的。同时，相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。 给定一个代表每个房屋存放金额的非负整数数组，计算你在不触动警报装置的情况下，能够偷窃到的最高金额。 输入: [1,2,3,1]输出: 4解释: 你可以先偷窃 1 号房屋（金额 = 1），然后偷窃 3 号房屋（金额 = 3）。 偷窃到的最高金额 = 1 + 3 = 4 。 解题思路 结果123456789101112131415public int rob(int[] nums) &#123; if (nums == null || nums.length == 0) return 0; if (nums.length == 1) return nums[0]; return Math.max(rob(nums, 0, nums.length-2), rob(nums, 1, nums.length-1));&#125; private int rob(int[] nums, int start, int end) &#123; int notRob = 0, curRob = 0; for (int i = start; i &lt;= end; i++) &#123; int preRob = Math.max(notRob, curRob); curRob = notRob + nums[i]; notRob = preRob; &#125; return Math.max(curRob, notRob);&#125; Leetcode 337. House Rob III题目描述在上次打劫完一条街道之后和一圈房屋后，小偷又发现了一个新的可行窃的地区。这个地区只有一个入口，我们称之为“根”。 除了“根”之外，每栋房子有且只有一个“父“房子与之相连。一番侦察之后，聪明的小偷意识到“这个地方的所有房屋的排列类似于一棵二叉树”。 如果两个直接相连的房子在同一天晚上被打劫，房屋将自动报警。 计算在不触动警报的情况下，小偷一晚能够盗取的最高金额。 输入: [3,2,3,null,3,null,1] 12345678 3 / \ 2 3 \ \ 3 1输出: 7 解释: 小偷一晚能够盗取的最高金额 = 3 + 3 + 1 = 7. 解题思路 结果1234567891011121314public int rob(TreeNode root) &#123; int[] res = helper(root); return Math.max(res[0], res[1]);&#125; private int[] helper(TreeNode root) &#123; if (root == null) return new int[2]; int[] left = helper(root.left); int[] right = helper(root.right); int notRob = Math.max(left[0], left[1]) + Math.max(right[0], right[1]); int curRob = left[0] + right[0] + root.val; return new int[] &#123;notRob, curRob&#125;;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Dynamic Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode best time to buy and sell stock 121,122,123,188,714]]></title>
    <url>%2FLeetcode%2Fleetcode%2Fleetcode-best-time-to-buy-and-sell-stock-121-122-123-188%2F</url>
    <content type="text"><![CDATA[股票相关解答说明 Leetcode 121. Best Time to Buy and Sell Stock题目描述给定一个数组，它的第 i 个元素是一支给定股票第 i 天的价格。 如果你最多只允许完成一笔交易（即买入和卖出一支股票），设计一个算法来计算你所能获取的最大利润。 注意你不能在买入股票前卖出股票。 [7,1,5,3,6,4]5 解题思路由于只能买卖一次，因此每次都减去至今为止最小的买入价格就行。[7, 1, 5, 3, 6, 4] - [7, 1, 1, 1, 1, 1]至今最小可以使用一个变量 curMin 进行记录。 结果123456789public int maxProfit(int[] prices) &#123; if (prices == null || prices.length &lt; 2) return 0; int curMin = prices[0], res = 0; for (int i = 1; i &lt; prices.length; i++) &#123; res = Math.max(res, prices[i]-curMin); curMin = Math.min(curMin, prices[i]); &#125; return res;&#125; Leetcode 122. Best Time to Buy and Sell Stock II题目描述给定一个数组，它的第 i 个元素是一支给定股票第 i 天的价格。 设计一个算法来计算你所能获取的最大利润。你可以尽可能地完成更多的交易（多次买卖一支股票）。 注意：你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 [7,1,5,3,6,4]7 解题思路由于没有买卖次数限制，因此只要后面比前面大就进行交易即可。 结果1234567public int maxProfit(int[] prices) &#123; int res = 0; for (int i = 1; i &lt; prices.length; i++) &#123; res += prices[i] &gt; prices[i-1] ? prices[i]-prices[i-1] : 0; &#125; return res;&#125; Leetcode 123. Best Time to Buy and Sell Stock III题目描述给定一个数组，它的第 i 个元素是一支给定的股票在第 i 天的价格。 设计一个算法来计算你所能获取的最大利润。你最多可以完成 两笔 交易。 注意: 你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 解题思路参考188 结果123456789101112131415// dp[k][i] = max(dp[k][i-1], dp[k-1][j] + prices[i] - prices[j]) j from 0 to i-1// dp[k][i] = max(dp[k][i-1], tempMax + prices[i]), tempMax = dp[k-1][j]-prices[j] public int maxProfit(int[] prices) &#123; if (prices == null || prices.length &lt; 2) return 0; int[][] dp = new int[3][prices.length]; for (int k = 1; k &lt;= 2; k++) &#123; int tempMax = dp[k-1][0] - prices[0]; for (int i = 1; i &lt; prices.length; i++) &#123; dp[k][i] = Math.max(dp[k][i-1], prices[i] + tempMax); tempMax = Math.max(tempMax, dp[k-1][i] - prices[i]); &#125; &#125; return dp[2][prices.length-1];&#125; Leetcode 188. Best Time to Buy and Sell Stock IV题目描述给定一个数组，它的第 i 个元素是一支给定的股票在第 i 天的价格。 设计一个算法来计算你所能获取的最大利润。你最多可以完成 k 笔交易。 注意: 你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 解题思路Buy/Sell Stock With K transactions To Maximize Profit Dynamic Programming dp[k][i], k 表示交易次数，i 表示第几天。 \\ dp[k][i] = max\begin{cases} dp[k][i-1] \qquad & 在第 i 天没有交易 \\ prices[i]-prices[j]+dp[k-1][j] \qquad & 在第 j 天进行交易， j := 0 to i-1 \end{cases} \\ 可以将上面的动态规划公式进行修改得到： dp[k][i] = max\begin{cases} dp[k][i-1] \qquad & 在第 i 天没有交易 \\ max(dp[k-1][j]-prices[j]) + prices[i] \qquad & 在第 j 天进行交易， j := 0 to i-1 \end{cases} \\结果123456789101112131415161718192021222324// dp[k][i] = j不买 dp[k][i-1]// = j买 max(dp[k-1][j] + p[i] - p[j]) j = 0..i-1// = j买 max(dp[k-1][j] - p[j]) + p[i] j = 0..i-1public int maxProfit(int k, int[] prices) &#123; if (k &lt;= 0 || prices.length &lt; 2) return 0; if (k &gt;= prices.length / 2) return quickSolve(prices); int[][] dp = new int[k+1][prices.length]; for (int kk = 1; kk &lt;= k; kk++) &#123; int temp = dp[kk-1][0] - prices[0]; for (int i = 1; i &lt; prices.length; i++) &#123; dp[kk][i] = Math.max(dp[kk][i-1], temp+prices[i]); temp = Math.max(temp, dp[kk-1][i] - prices[i]); &#125; &#125; return dp[k][prices.length-1];&#125; private int quickSolve(int[] prices) &#123; int len = prices.length, profit = 0; for (int i = 1; i &lt; len; i++) // as long as there is a price gap, we gain a profit. if (prices[i] &gt; prices[i - 1]) profit += prices[i] - prices[i - 1]; return profit;&#125; Leetcode 309. Best Time to Buy and Sell Stock with Cooldown题目描述给定一个整数数组，其中第 i 个元素代表了第 i 天的股票价格 。​ 设计一个算法计算出最大利润。在满足以下约束条件下，你可以尽可能地完成更多的交易（多次买卖一支股票）: 你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。卖出股票后，你无法在第二天买入股票 (即冷冻期为 1 天)。 解题思路该题是对 LeetCode 122 的扩展。 结果Leetcode 714. Best Time to Buy and Sell Stock with Transaction Fee题目描述给定一个整数数组 prices，其中第 i 个元素代表了第 i 天的股票价格 ；非负整数 fee 代表了交易股票的手续费用。 你可以无限次地完成交易，但是你每次交易都需要付手续费。如果你已经购买了一个股票，在卖出它之前你就不能再继续购买股票了。 返回获得利润的最大值。 解题思路该题是对 LeetCode 122 的扩展。 结果 cash 表示未持有股票的现金部分。hold 表示持有股票。 12345678public int maxProfit(int[] prices, int fee) &#123; int cash = 0, hold = -prices[0]; for (int i = 1; i &lt; prices.length; i++) &#123; cash = Math.max(cash, hold+prices[i]-fee); hold = Math.max(hold, cash-prices[i]); &#125; return cash;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Dynamic Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KMP讲解]]></title>
    <url>%2FAlgorithm%2Falgorithm%2Fkmp%2F</url>
    <content type="text"><![CDATA[KMP原理讲解正常的匹配方式正常的匹配，从左往右一个一个进行匹配，当匹配失败的时候，P向右移动一格。若T:aaaaaaaaaab, P:aaaaab 此时将花费大量的时间。 KMP准备(1) 获取前缀表 获取前缀表为三步： 找所有前缀 求最大公共前后缀 向下移动一位，因为最后一位对KMP而言并没有什么用处 (2) 匹配过程 在KMP匹配过程中，若匹配不成功，则找前缀所在索引进行移动，即紫色代表的索引位，若是-1，则向右移动一位。 KMP代码讲解Prefix Table 12345678910111213141516171819202122232425private int[] prefixTable(String p) &#123; int[] prefix = new int[p.length()]; prefix[0] = 0; int len = 0, i = 1; while (i &lt; p.length()) &#123; if (p.charAt(i) == p.charAt(len) &#123; prefix[i++] = len++; &#125; else &#123; // a b a b c a b a a // 0 0 1 2 0 1 2 3 ? // len = 3 // 此时不等 if (len &gt; 0) len = prefix[len-1]; // 防止最开始不走 else prefix[i++] = len; &#125; &#125; return prefix;&#125;private int[] movePrefixTable(int[] prefix) &#123; for (int i = prefix.length - 1; i &gt; 0; i--) prefix[i] = prefix[i-1]; prefix[0] = -1; return prefix;&#125; 以上就是求解 PrefixTable 的方法 KMP Search12345678910111213141516171819public int kmpSearch(String t, String p) &#123; int[] prefix = prefixTable(p); movePrefix(prefix); int i = 0, j = 0; while (i &lt; t.length()) &#123; if (j == p.length() - 1 &amp;&amp; s.charAt(i) == p.charAt(j)) return i; if (t.charAt(i) == p.charAt(j)) &#123; i++; j++; &#125; else &#123; j = prefix[j]; if (j == -1) &#123; i++; j++; &#125; &#125; &#125; return -1;&#125; 对看看多理解，不行先背下来，多回忆就能理解了。]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Kmp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-001-005]]></title>
    <url>%2FLeetcode%2Fleetcode%2Fleetcode-001-005%2F</url>
    <content type="text"><![CDATA[Leetcode 001 Two Sum题目描述给出数字数组，找出能使和为target的两个索引，例：nums = [2, 7, 11, 15], target = 9, 因为 nums[0] + nums[1] = 2 + 7 = 9所以 return [0, 1] 解答该题直接采用简单map即可解决。时间复杂度为O(n)，空间复杂度为O(1)。 123456789101112131415class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; if (nums == null || nums.length &lt; 2) return new int[]&#123;&#125;; Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); for (int i = 0; i &lt; nums.length; i++) &#123; int remain = target - nums[i]; if (map.containsKey(remain)) &#123; return new int[]&#123;map.get(remain), i&#125;; &#125; map.put(nums[i], i); &#125; return new int[] &#123;&#125;; &#125;&#125; Leetcode 002 Add Two Numbers题目描述给定两个List，对其相加，例：(2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)7 -&gt; 0 -&gt; 8 解答在做 LinkedList 题时，一般都会使用 dummy。 12345678910111213141516171819class Solution &#123; public ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123; ListNode dummy = new ListNode(-1), cur = dummy; int carry = 0; while (l1 != null || l2 != null) &#123; int v1 = l1 == null ? 0 : l1.val; int v2 = l2 == null ? 0 : l2.val; int sum = v1 + v2 + carry; cur.next = new ListNode(sum%10); carry = sum / 10; cur = cur.next; if (l1 != null) l1 = l1.next; if (l2 != null) l2 = l2.next; &#125; if (carry != 0) cur.next = new ListNode(carry); return dummy.next; &#125;&#125; Leetcode 003 Longest Substring Without Repeating Characters题目描述给定一个字符串 “abcabcbb”, 最长的子串长度。 解答1利用 Set 做 Sliding Window。 1234567891011121314151617class Solution &#123; public int lengthOfLongestSubstring(String s) &#123; if (s == null || s.length() == 0) return 0; int n = s.length(); Set&lt;Character&gt; set = new HashSet&lt;&gt;(); int ans = 0, i = 0, j = 0; while (i &lt; n &amp;&amp; j &lt; n) &#123; if (!set.contains(s.charAt(j))) &#123; set.add(s.charAt(j++)); ans = Math.max(ans, j - i); &#125; else &#123; set.remove(s.charAt(i++)); &#125; &#125; return ans; &#125;&#125; 解答2利用 Map 进行跳转。 12345678910111213141516class Solution &#123; public int lengthOfLongestSubstring2(String s) &#123; int n = s.length(); Map&lt;Character, Integer&gt; map = new HashMap&lt;&gt;(); int ans = 0; for (int i = 0, j = 0; j &lt; n; j++) &#123; if (map.containsKey(s.charAt(j))) &#123; i = Math.max(map.get(s.charAt(j)), i); &#125; ans = Math.max(ans, j - i + 1); map.put(s.charAt(j), j + 1); &#125; return ans; &#125;&#125; 解答3在方法2的基础上进行优化。 1234567891011121314class Solution &#123; public int lengthOfLongestSubstring3(String s) &#123; int n = s.length(); int[] map = new int[128]; int ans = 0; for (int i = 0, j = 0; j &lt; n; j++) &#123; i = Math.max(map[s.charAt(j)], i); ans = Math.max(ans, j - i + 1); map[s.charAt(j)] = j + 1; &#125; return ans; &#125;&#125; Leetcode 004 Median of Two Sorted Arrays题目描述给出已排序的 nums1, nums2 求这两个数组的中位数。 nums1 = [1, 4]nums2 = [2, 6]return 3.0 解答1234567891011121314151617181920212223242526272829303132public class Alg4_MedianofTwoSortedArrays &#123; public double findMedianSortedArrays(int[] nums1, int[] nums2) &#123; if (nums1 == null || nums2 == null) return 0; // 暂时不考虑只有一个为null的情况 if (nums1.length &gt; nums2.length) return findMedianSortedArrays(nums2, nums1); int len = nums1.length + nums2.length; int start = 0, end = nums1.length; int cut1 = 0, cut2 = 0; while (cut1 &lt;= nums1.length) &#123; cut1 = (end - start) / 2 + start; cut2 = len / 2 - cut1; double l1 = (cut1 == 0) ? Integer.MIN_VALUE : nums1[cut1 - 1]; double r1 = (cut1 == nums1.length) ? Integer.MAX_VALUE : nums1[cut1]; double l2 = (cut2 == 0) ? Integer.MIN_VALUE : nums2[cut2 - 1]; double r2 = (cut2 == nums2.length) ? Integer.MAX_VALUE : nums2[cut2]; if (l1 &gt; r2) end = cut1 - 1; else if (l2 &gt; r1) start = cut1 + 1; else &#123; if (len % 2 == 0) &#123; l1 = l1 &gt; l2 ? l1 : l2; r1 = r1 &lt; r2 ? r1 : r2; return (l1 + r1) / 2.0; &#125; else &#123; return r1 &lt; r2 ? r1 : r2; &#125; &#125; &#125; return -1; &#125;&#125; 详解 以偶数为例子:nums1 = [1, 4, 5, 8]nums2 = [2, 6]all = [1, 2, 4, 5, 6, 8]以上的中位数为 mid(4, 5)，那么此时如果有一个cut则在 4, 5之间，halfLen = (nums1.length + nums2.length) / 2现在我们只考虑 nums1 如何切分，当我们确定了i的位置，j的位置也就确定了，因为i+j = halfLen。那么在什么情况下会满足合适的切分呢？当l1 &lt; r2 &amp;&amp; l2 &lt; r1 的时候，满足切分，小的都分到了左边，大的都分到了右侧。 l1 &gt; r2 此时，i偏右，需要向左移动，end = i - 1 l2 &gt; r1 此时，i偏左，需要向右一定，start = i + 1 Leetcode 005 Longest Palindromic Substring题目描述给定字符串，求最大回文子串。 解答1dp求解, baaab，dp[0][4]满足回文的前提是，s[0]==s[4]且dp[1][3]也是回文。但需要注意，当 j-i &lt;= 2 时，只要s[i] == s[j]必定是回文aba举例，dp[0][2]=true公式如下： dp[i][j] = s[i]==s[j] && (j-i]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[堆(HEAP)讲解]]></title>
    <url>%2FAlgorithm%2Falgorithm%2Fheap%2F</url>
    <content type="text"><![CDATA[堆(HEAP)介绍 堆(HEAP)必须要是完全二叉树，同时满足父节点的内容大于(或小于)子节点。堆的底层数据结构为一维数组，此时若当前节点为$i$，则其父子节点索引可直接通过以下公式直接计算得到。 \begin{cases} parent = (i-1)/2 \\ left = 2i + 1 \\ right = 2i + 2 \end{cases} Heapify 操作若给定的数据中，root节点无法保证为堆结构，此时需要对其进行heapfiy操作，保持堆结构。 1234567891011121314151617void heapify(int[] tree, int n, int i) &#123; // recursive exit if (i &gt;= n) return; int left = 2 * i + 1; int right = 2 * i + 2; int max = i; if (left &lt; n &amp;&amp; tree[left] &gt; tree[max]) &#123; max = left; &#125; if (right &lt; n &amp;&amp; tree[right] &gt; tree[max]) &#123; max = right; &#125; if (max != i) &#123; swap(tree, max, i); heapify(tree, n, max); &#125;&#125; heapify Time: $O(lgn)$ Init Heapify 操作若给定完全二叉树，初始状态无序，此时需要对其进行初始化，使其成为Heap。其Heapify顺序如下所示。 123456void buildHeap(int[] tree) &#123; int lastParent = (tree.length-2) / 2; for (int i = lastParent; i &gt;= 0; i--) &#123; heapify(tree, tree.length, i); &#125;&#125; 每次调用heapify的Time: $O(lgn)$，需要$O(n)$调用，因此可估为$O(nlgn)$，但这样的计算并不严谨。 由于不同高度，调用heapify的时间不同，高度为h的堆最多包涵$\lceil n/2^{h+1}\rceil$，在一个高度为h的节点上运行heapify的Time: $O(h)$，所以可以将BuildHeap的代价表示如下： T(n) = \sum_{h=0}^{\lfloor lgn \rfloor} \lceil \frac{n}{2^{h+1}} \rceil O(h) = O(n \sum_{h=0}^{\lfloor lgn \rfloor} \frac{h}{2^h}) \\ \sum_{h=0}^{\infty}\frac{h}{2^h} = \frac{1/2}{(1-1/2)^2} = 2\\ T(n)=O(n \sum_{h=0}^{\infty}\frac{h}{2^h}) = O(n)HeapSort1234567void heapSort(int[] tree) &#123; buildHeap(tree); for (int i = tree.length - 1; i &gt;= 0; i--) &#123; swap(tree, i, 0); heapify(tree, i, 0); &#125;&#125; Priority Queue12345678// 初始大小private static final int DEFAULT_INITIAL_CAPACITY = 11;// 数组作为堆transient Object[] queue; // 元素数量private int size = 0;// 比较器private final Comparator&lt;? super E&gt; comparator; offer, add, siftUp12345678910111213141516171819202122232425262728293031323334353637public boolean add(E e) &#123; return offer(e); &#125;public boolean offer(E e) &#123; int i = size; // 大于数组长度则增长 if (i &gt;= queue.length) grow(i + 1); size = i + 1; if (i == 0) queue[0] = e; else // 自下而上 siftUp(i, e); return true;&#125;private void siftUp(int k, E x) &#123; if (comparator != null) // 利用自定义的比较器 siftUpUsingComparator(k, x); else siftUpComparable(k, x);&#125;private void siftUpUsingComparator(int k, E x) &#123; while (k &gt; 0) &#123; int parent = (k - 1) &gt;&gt;&gt; 1; Object e = queue[parent]; // x &gt;= e, 小顶堆，直到parent &lt; 当前 if (comparator.compare(x, (E) e) &gt;= 0) break; queue[k] = e; k = parent; &#125; queue[k] = x;&#125; poll, siftDown1234567891011121314151617181920212223242526272829303132333435363738public E poll() &#123; if (size == 0) return null; int s = --size; modCount++; E result = (E) queue[0]; E x = (E) queue[s]; queue[s] = null; if (s != 0) // 自上向下 siftDown(0, x); return result;&#125;private void siftDown(int k, E x) &#123; if (comparator != null) siftDownUsingComparator(k, x); else siftDownComparable(k, x);&#125;private void siftDownUsingComparator(int k, E x) &#123; int half = size &gt;&gt;&gt; 1; while (k &lt; half) &#123; int child = (k &lt;&lt; 1) + 1; Object c = queue[child]; int right = child + 1; // left &gt; right, 取右边, 取小 if (right &lt; size &amp;&amp; comparator.compare((E) c, (E) queue[right]) &gt; 0) c = queue[child = right]; // x 比任何一个都小, 则退出 if (comparator.compare(x, (E) c) &lt;= 0) break; queue[k] = c; k = child; &#125; queue[k] = x;&#125; 参考文献 堆排序(Heapsort) 2.6.3 Heap - Heap Sort - Heapify - Priority Queues Algorithms lecture 13— Build max heap algorithm and analysis]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Heap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BFS(Breadth First Search) 与 DFS (Depth First Search)]]></title>
    <url>%2FAlgorithm%2Falgorithm%2Fdfs-bfs%2F</url>
    <content type="text"><![CDATA[BFS(Breadth First Search) 与 DFS (Depth First Search) BFS与DFS的主要应用场景在图和树之中。 BFS与DFS 上面的图可以转化为以下的数据结构。 12345678graph = &#123; "A": ["B", "C"], "B": ["A", "C", "D"], "C": ["A", "B", "D", "E"], "D": ["B", "C", "E", "F"], "E": ["C", "D"], "F": ["D"]&#125; BFS算法，利用 Queue 进行遍历，顺序为ABCDEF DFS算法，利用 Stack 进行遍历，顺序为ABDFEC BFS1234567891011121314private void BFS(Map&lt;String, List&lt;String&gt; graph, String start) &#123; Queue&lt;String&gt; queue = new LinkedList&lt;&gt;(); queue.offer(start); Set&lt;String&gt; used = new HashSet&lt;&gt;(); while (!queue.isEmpty()) &#123; String cur = queue.pop(); List&lt;String&gt; neighbors = graph.get(cur); for (String w : neighbors) &#123; if (used.contains(w)) continue; queue.offer(w); &#125; System.out.println(cur); &#125;&#125; DFS1234567891011121314private void DFS(Map&lt;String, List&lt;String&gt; graph, String start) &#123; Deque&lt;String&gt; stack = new LinkedList&lt;&gt;(); stack.push(start); Set&lt;String&gt; used = new HashSet&lt;&gt;(); while (!stack.isEmpty()) &#123; String cur = queue.pop(); List&lt;String&gt; neighbors = graph.get(cur); for (String w : neighbors) &#123; if (used.contains(w)) continue; stack.push(w); &#125; System.out.println(cur); &#125;&#125; 二叉树DFS Traversal123456public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125;&#125; 144. Binary Tree Preorder Traversal (Easy)Recursive1234567891011121314class Solution &#123; public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); dfs(root, res); return res; &#125; private void dfs(TreeNode root, List&lt;Integer&gt; res) &#123; if (root == null) return; res.add(root.val); if (root.left != null) dfs(root.left, res); if (root.right != null) dfs(root.right, res); &#125;&#125; Iteration123456789101112public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); Deque&lt;TreeNode&gt; stack = new LinkedList&lt;&gt;(); if (root != null) stack.push(root); while (!stack.isEmpty()) &#123; TreeNode cur = stack.pop(); res.add(cur.val); if (cur.right != null) stack.push(cur.right); if (cur.left != null) stack.push(cur.left); &#125; return res;&#125; 589. N-ary Tree Preorder Traversal (Easy)Recursive12345678910111213141516class Solution &#123; public List&lt;Integer&gt; preorder(Node root) &#123; List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); dfs(root, res); return res; &#125; private void dfs(Node root, List&lt;Integer&gt; res) &#123; if (root == null) return; res.add(root.val); if (root.children == null) return; for (Node child : root.children) &#123; dfs(child, res); &#125; &#125;&#125; Iteration1234567891011121314151617class Solution &#123; public List&lt;Integer&gt; preorder(Node root) &#123; List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); Deque&lt;Node&gt; stack = new LinkedList&lt;&gt;(); Node cur = root; if (cur != null) stack.push(cur); while (!stack.isEmpty()) &#123; cur = stack.pop(); res.add(cur.val); if (cur.children == null) continue; for (int i = cur.children.size()-1; i &gt;= 0; i--) &#123; stack.push(cur.children.get(i)); &#125; &#125; return res; &#125; &#125; 94. Binary Tree Inorder Traversal (Medium)Recursive1234567891011121314class Solution &#123; public List&lt;Integer&gt; inorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); dfs(root, res); return res; &#125; private void dfs(TreeNode root, List&lt;Integer&gt; res) &#123; if (root == null) return; if (root.left != null) dfs(root.left, res); res.add(root.val); if (root.right != null) dfs(root.right, res); &#125;&#125; Iteration1234567891011121314151617class Solution &#123; public List&lt;Integer&gt; inorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); Deque&lt;TreeNode&gt; stack = new LinkedList&lt;&gt;(); TreeNode cur = root; while (cur != null || !stack.isEmpty()) &#123; while (cur != null) &#123; stack.push(cur); cur = cur.left; &#125; cur = stack.pop(); res.add(cur.val); cur = cur.right; &#125; return res; &#125; 145. Binary Tree Postorder Traversal (Hard)Recursive1234567891011121314class Solution &#123; public List&lt;Integer&gt; postorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); dfs(root, res); return res; &#125; private void dfs(TreeNode root, List&lt;Integer&gt; res) &#123; if (root == null) return; if (root.left != null) dfs(root.left, res); if (root.right != null) dfs(root.right, res); res.add(root.val); &#125;&#125; Iteration12345678910111213141516171819202122232425class Solution &#123; public List&lt;Integer&gt; postorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); Deque&lt;TreeNode&gt; stack = new LinkedList&lt;&gt;(); TreeNode cur = root; while (cur != null || !stack.isEmpty()) &#123; if (cur != null) &#123; // left stack.push(cur); cur = cur.left; &#125; else &#123; TreeNode temp = stack.peek().right; if (temp != null) cur = temp; // right have node else &#123; // right don't have node temp = stack.pop(); res.add(temp.val); while (!stack.isEmpty() &amp;&amp; temp == stack.peek().right) &#123; temp = stack.pop(); res.add(temp.val); &#125; &#125; &#125; &#125; return res; &#125;&#125; 590. N-ary Tree Postorder Traversal (Easy)Recursive1234567891011121314151617class Solution &#123; public List&lt;Integer&gt; postorder(Node root) &#123; List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); dfs(root, res); return res; &#125; private void dfs(Node root, List&lt;Integer&gt; res) &#123; if (root == null) return; if (root.children != null) &#123; for (Node child : root.children) &#123; dfs(child, res); &#125; &#125; res.add(root.val); &#125;&#125; Iteration123456789101112131415class Solution &#123; public List&lt;Integer&gt; postorder(Node root) &#123; List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); Deque&lt;Node&gt; stack = new LinkedList&lt;&gt;(); if (root != null) stack.push(root); while (!stack.isEmpty()) &#123; Node cur = stack.pop(); res.add(cur.val); for (Node child : cur.children) stack.push(child); &#125; Collections.reverse(res); return res; &#125;&#125; 二叉树BFS Traversal102. Binary Tree Level Order Traversal (Medium)1234567891011121314151617181920class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; levelOrder(TreeNode root) &#123; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); if (root != null) queue.offer(root); while (!queue.isEmpty()) &#123; List&lt;Integer&gt; curLevel = new ArrayList&lt;&gt;(); for(int i = queue.size(); i &gt; 0; i--) &#123; TreeNode cur = queue.poll(); curLevel.add(cur.val); if (cur.left != null) queue.offer(cur.left); if (cur.right != null) queue.offer(cur.right); &#125; res.add(curLevel); &#125; return res; &#125;&#125; 107. Binary Tree Level Order Traversal II (Easy)123456789101112131415161718class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; levelOrderBottom(TreeNode root) &#123; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); if (root != null) queue.offer(root); while (!queue.isEmpty()) &#123; List&lt;Integer&gt; curLevel = new ArrayList&lt;&gt;(); for (int i = queue.size(); i &gt; 0; i--) &#123; TreeNode cur = queue.poll(); curLevel.add(cur.val); if (cur.left != null) queue.offer(cur.left); if (cur.right != null) queue.offer(cur.right); &#125; res.add(0, curLevel); &#125; return res; &#125;&#125; 429. N-ary Tree Level Order Traversal (Easy)1234567891011121314151617181920class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; levelOrder(Node root) &#123; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); if (root == null) return res; Queue&lt;Node&gt; queue = new LinkedList&lt;&gt;(); queue.offer(root); while (!queue.isEmpty()) &#123; List&lt;Integer&gt; curLevel = new ArrayList&lt;&gt;(); for (int i = queue.size(); i &gt; 0; i--) &#123; Node cur = queue.poll(); curLevel.add(cur.val); for (Node child : cur.children) &#123; queue.offer(child); &#125; &#125; res.add(curLevel); &#125; return res; &#125;&#125; 二叉树 Depth111. Minimum Depth of Binary TreeDFS12345678class Solution &#123; public int minDepth(TreeNode root) &#123; if (root == null) return 0; int left = minDepth(root.left); int right = minDepth(root.right); return (left == 0 || right == 0) ? left + right + 1 : Math.min(left, right)+1; &#125;&#125; BFS12345678910111213141516171819class Solution &#123; public int minDepth(TreeNode root) &#123; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); if (root == null) return 0; TreeNode cur = null; queue.offer(root); int depth = 0; while (!queue.isEmpty()) &#123; depth++; for (int i = queue.size(); i &gt; 0; i--) &#123; cur = queue.poll(); if (cur.left == null &amp;&amp; cur.right == null) return depth; if (cur.left != null) queue.offer(cur.left); if (cur.right != null) queue.offer(cur.right); &#125; &#125; return depth; &#125;&#125; 104. Maximum Depth of Binary TreeDFS12345class Solution &#123; public int maxDepth(TreeNode root) &#123; return root == null ? 0 : Math.max(maxDepth(root.left)+1, maxDepth(root.right)+1); &#125;&#125; BFS123456789101112131415161718class Solution &#123; public int maxDepth(TreeNode root) &#123; if (root == null) return 0; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.offer(root); TreeNode cur = null; int depth = 0; while (!queue.isEmpty()) &#123; depth++; for (int i = queue.size(); i &gt; 0; i--) &#123; cur = queue.poll(); if (cur.left != null) queue.offer(cur.left); if (cur.right != null) queue.offer(cur.right); &#125; &#125; return depth; &#125;&#125; 559. Maximum Depth of N-ary TreeDFS12345678910class Solution &#123; public int maxDepth(Node root) &#123; if (root == null) return 0; int depth = 0; for (Node child : root.children) &#123; depth = Math.max(depth, maxDepth(child)); &#125; return depth+1; &#125;&#125; DFS12345678910111213141516171819class Solution &#123; public int maxDepth(Node root) &#123; if (root == null) return 0; Queue&lt;Node&gt; queue = new LinkedList&lt;&gt;(); queue.offer(root); Node cur = null; int depth = 0; while (!queue.isEmpty()) &#123; depth++; for (int i = queue.size(); i &gt; 0; i--) &#123; cur = queue.poll(); for (Node child : cur.children) &#123; queue.offer(child); &#125; &#125; &#125; return depth; &#125;&#125; 参考文献 BFS和DFS算法(第1讲) BFS和DFS算法(第2讲) BFS和DFS算法(第3讲)]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>DFS</tag>
        <tag>BFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Computer Cache]]></title>
    <url>%2FLinux%2Flinux%2Fcomputer-cache%2F</url>
    <content type="text"><![CDATA[计算机体系结构中，内存资源相对于 CPU 而言是十分稀有的，存储技术的发展相对 CPU 而言也是非常缓慢的，当 CPU 在 Moore 定律基础上以指数级增长时，存储技术还在线性缓慢增长，因此为使存储技术能够适应 CPU 如此快速的发展，在现代计算机中都会存在一级、二级、三级缓存，使得CPU能够快速的获取指令与数据。本人在学习内存管理、并发与同步过程中，一直在思考，何为原子操作、内存屏障解决了何问题？以及缓存一致性、内存一致性都是什么？因此，不才经过搜索网上大量资料，学习后整理以下文章，以作分享。 问题来源 首先，希望通过几个例子，与大家分享一下有哪些地方出现了与计算机缓存相关的的知识点。 lock1234asm volatile(lock "cmpxchgb %2,%1" : "=a" (__ret), "+m" (*__ptr) // output : "q" (__new), "0" (__old) // input : "memory"); // clobbered 如果大家看过内核有关lock代码的话就会看到上面这一条汇编指令。在这条简单的汇编代码中，包含了关键词volatile, lock, cmpxchgb。其中cmpxchgb很简单就是用__old与__ptr内存中的数据做比较，如果相同则将__new赋给__ptr，返回__new；否则保持__ptr值不变，返回__ptr指向内容。让我想不通的一点是为何需要volatile, lock以及这两个关键词所起到的作用是什么？ volatile为了防止gcc优化代码。lock添加内存屏障，早期也采用锁总线技术。这边的话先给出回答，后面会详细解释。 spinlock自旋锁Spinlock是内核中保证原子操作的常用手段，高性能的Spinlock能够极大提高内核效率，因此有大量对于Spinlock的研究，相关知识大家可以通过搜索SpinLock实现等关键词获得。所有对于Spinlock的优化都是为了解决CPU cacheline bouncing这一问题。同时，为了更高效的使用Spinlock都会采用Cache line 对齐，使得Spinlock能够独占Cache line，如内存管理中Zone添加ZONE_PADDING以做对齐使用。 123456struct zone &#123; ZONE_PADDING(_pad1_) spinlock_t lock; ZONE_PADDING(_pad2_) spinlock_t lru_lock;&#125; Cache line是缓存最小的单位。CPU cacheline bouncing产生原因是不同Core访问同一内存地址数据时，实际都是从自身L1 Cache获取，此时需要由cache coherence protocol 来保证每一Core从各自Cache看到正确的数据，当竞争严重时，将产生大量的cache coherence message，从而产生大量的cache traffic，降低锁的伸缩性。 行遍历与列遍历12345678910111213// 行遍历for (int i = 0; i &lt; m; i++) &#123; for (int j = 0; j &lt; n; j++) &#123; sum += num[i][j] &#125;&#125;// 列遍历for (int j = 0; j &lt; n; j++) &#123; for (int i = 0; i &lt; m; i++) &#123; sum += num[i][j] &#125;&#125; 在我们学习计算机原理，或者其他相关课程时，老师一再强调行遍历要比列遍历高效，在没必要的情况下请使用行遍历。这是由于，数据在内存中，需要以页的形式缓存到L3，再到L2，在缓存到L1的 Cache Line，如果二维数组一行数据特别大，进行遍历的时候，需要频繁的从缓存中换出当前行数据，再读取下一行。如果采用行遍历会处理掉读入的整行数据；而采用列遍历时只会处理读入的整行数据中的某一个数据后缓存则无效。 多线程执行时的低效1234567891011121314151617int result[P];for (int p = 0; p &lt; P; p++) &#123; pool.run([&amp;,p]) &#123; result[p] = 0; int chunkSize = DIM/P + 1; int start = p * chunkSize; int end = min(start+chunkSize, DIM); for (int i = start; i &lt; end; i++) &#123; ++result[p]; &#125; &#125;&#125;pool.join(); // wait all taskint sum = 0;for (int i = 0; i &lt; P; i++) sum+= result[i]; 上面是一个典型的低效使用多线程的写法，result数组被多个线程所持有，在每个Core对result[i]进行操作的时候，都需要与其他所有Core同步这一变化，从而导致CPU cacheline bouncing，解决方式而很简单，每个线程持有自己的私有变量，等对当前私有变量处理完成后再赋给result[i]，此时缓存同步便极大降低。 相关知识可以看：Scott Meyers: Cpu Caches and Why You Care 内存屏障 内存屏障主要分三类：编译器优化 / 缓存优化 / CPU乱序执行 C/C++ — 编程中的内存屏障(Memory Barriers) 优化屏障和内存屏障 cache 对齐多核并发编程中的cache line对齐问题 Cpu cache 与内存对齐 数据跨越两个 cache line，就意味着两次 load 或者两次 store。如果数据结构是 cache line 对齐的，就有可能减少一次读写。数据结构的首地址cache line对齐，意味着可能有内存浪费（特别是数组这样连续分配的数据结构），所以需要在空间和时间两方面权衡。 比如现在有个变量 int x; 占用4个字节,它的起始地址是 0x1234567F，那么它占用的内存范围就在 0x1234567F-0x12345682 之间。如果现在Cache Line长度为32个字节，那么每次内存同 Cache 进行数据交换时，都必须取起始地址时32(0x20)倍数的内存位置开始的一段长度为32的内存同 Cache Line 进行交换.比如 0x1234567F 落在范围 0x12345660~0x1234567F 上，但是 0x12345680~0x12345682 落在范围 0x12345680~0x1234569F上，也就是说，为了将4个字节的整数变量 0x1234567F~0x12345682 装入 Cache ,我们必须调入两条 Cache Line 的数据。但是如果 int x 的起始地址按4的倍数对齐，比如是 0x1234567C~0x1234567F ,那么必然会落在一条Cache Line 上，所以每次访问变量x就最多只需要装入一条 Cache Line 的数据了。比如现在一般的 malloc() 函数，返回的内存地址会已经是8字节对齐的，这个就是为了能够让大部分程序有更好的性能。 CPU架构图x86微处理器经典架构 附: 首发：Meltdown漏洞分析与实践 经典处理器架构的流水线是5级流水线：取值(IF),译码(ID),执行(EX),数据内存访问(MEM),写会(WB) 现在处理器在设计上都采用了超标量体系结构(Superscalar Architecture)和乱序执行(Out-of-Order, OOO)技术，极大提高了处理器计算能力。 超标量技术能够在一个时钟周期内执行多个指令。 ALU: Arithmetic-logic unit, 算法逻辑单元 AGU: Address generation unit, 地址生成单元 CPU执行过程，采用顺序提交指令, 乱序执行, 最后顺序提交结果的过程。 不进行乱序优化时，处理器的指令执行过程如下： 指令获取。 如果输入的运算对象是可以获取的（比如已经存在于寄存器中），这条指令会被发送到合适的功能单元。如果一个或者更多的运算对象在当前的时钟周期中是不可获取的（通常需要从主内存获取），处理器会开始等待直到它们是可以获取的。 指令在合适的功能单元中被执行。 功能单元将运算结果写回寄存器。 乱序优化下的执行过程如下： 指令获取。 指令被发送到一个指令序列（也称执行缓冲区或者保留站）中。 指令将在序列中等待，直到它的数据运算对象是可以获取的。然后，指令被允许在先进入的、旧的指令之前离开序列缓冲区。（此处表现为乱序） 指令被分配给一个合适的功能单元并由之执行。 结果被放到一个序列中。 仅当所有在该指令之前的指令都将他们的结果写入寄存器后，这条指令的结果才会被写入寄存器中。（重整乱序结果） 当然，为了实现乱序优化，还需要很多技术的支持，如寄存器重命名、分枝预测等，但大致了解到这里就足够。后文的注释中会据此给出内存屏障的实现方案 CPU存储架构 附:How L1 and L2 CPU Caches Work, and Why They’re an Essential Part of Modern Chips CPU速度与内存速度差距越来越大，若CPU需要等待内存响应极大地降低了CPU效率，因此引入了缓存机制。 附:从CPU缓存看volatile为什么不能保证原子性 有了CPU高速缓存虽然解决了效率问题，但是它会带来一个新的问题：数据一致性。一般方案有两种：缓存一致性协议 or 总线锁机制由于总线锁在一个CPU锁住总线后，其他CPU会阻塞等待，效率较低，因此现在的体系结构中一般采用缓存一致性协议保证数据一致性。 现有的CPU主要有3级缓存，以i7为例，包含16个寄存器，32KB一级缓存（L1指令缓存，L1数据缓存），256KB二级缓存，2-20MB三级缓存 Linux查看缓存命令：cat /sys/devices/system/cpu/cpu0/cache/index0/size以线上机器为例，32K，256K，20ML1: 8way, 64sets, 64bytes cachelineL2: 8way, 512sets, 64bytes cachelineL3: 20way, 16384sets, 64bytes cacheline附: linux查看CPU高速缓存(cache)信息 附:CPU Cache 缓存学习笔记 不同的缓存执行效率不一样，L1大概在4ns，L2大概在11ns，L3大概在14ns Cache line 相关以上知道了Cache的存在，那么Cache是如何进行分布的呢？ 冬瓜哥彪悍图解Cache组关联Cache直接映射、组相连映射以及全相连映射Arm核920T性能优化之Cache关于CPU Cache — 程序猿需要知道的那些事 假设：主存容量1MB，每块512Byte，分成2048块。Cache容量8KB，每块512B，分成16块。 全关联(full-associative) 全关联的架构，主存中每一块都可以映射到Cache中的任意一块。Cache利用率高，但是需要一大堆的比较器，硬件设计、实现困难，因此只存在于小容量Cache中。 直接映射(direct-mapping) 直接映射的架构，N%16 即为对应的Cache块。这种架构硬件设计简单，成本低转化快，但是不够灵活，每个主存块只能对应一个固有的位置，容易产生冲突。例如一个程序需要重复使用内存0块和16块数据，即使其他缓存1-15块空闲，也只能使用0块缓存进行切换。降低命中率。 多路组相联(set-associative) 直接映射相当于一路组相联。主存中的各块与Cache的组号之间有固定的映射关系，但可自由映射到对应组中的任意一块。多路组关联，缓存控制器每收到一个请求就会并行去所有Way中，将同一行号Tag都读出来然后与请求比较，2路组关联会浪费50%能耗。因为需要将所有Way 中对应行数据读出。为减少功耗，将Tag与Data 分离。 实例 8way, 64set, 64byte = 32KB由于该处理器处理64G内存，分页为4k，总共有$64G/4k=2^{24}$个页面，需要24位进行标记(12-35位进行4KB对齐)4k/64Byte=64(6-11位表示选中set) 以物理地址0x800010a0为例，1000 0000 0000 0000 0001 | 0000 10|10 00000x800010a0必定在第二组中，至于在哪一路，则需要通过tag标签进行查找。 由于我们只需要去查看某一组中的8路，所以查找匹配标记是非常迅速的；事实上，从电学角度讲，所有的标记是同时进行比对的，我用箭头来表示这一点。如果此时正好有一条具有匹配标签的有效缓存线，我们就获得一次缓存命中（cache hit） 为什么Set段设置在低位 关于CPU Cache — 程序猿需要知道的那些事 Cache coherenceMIT 6.004 L21: Cache Coherence 缓存/内存Coherence模型 以上介绍的主要是在单个CPU中的Cache结构，那么在多CPU体系下，Cache是如何分布的呢？ 在I7中，每个CPU拥有自己独立的32KB一级缓存（L1指令缓存，L1数据缓存），256KB二级缓存，2-20MB三级缓存。 那么在多CPU中是如何保证数据一致性的？即cache coherence 协议是什么。 snoopy cache &amp; snooping based coherenceWiki-Snoopy cache In computing a snoopy cache is a type of memory cache that performs bus sniffing. The technique was introduced by Ravishankar and Goodman in 1983.[1] Such caches are used in systems where many processors or computers share the same memory and each has its own cache. In such systems processor ‘A’ may read a value from memory, then processor ‘B’ does the same. If either of the processors now change the value by writing back to memory they will invalidate the other processor’s cached value. In order to prevent this and maintain cache coherence, snoopy caches monitor (‘snoop on’) the memory bus to detect any writes to values that they are holding, including changes coming from other processors or distributed computers. However, this approach can only work in computer architectures like SGI Challenge and SGI Onyx where a single memory bus is shared between all processors 翻译一下大概意思是：snoopy cache 能够监控总线，这类缓存用于多处理器在自己的cache中分享相同的内存，如果A从内存中读取数据，然后B也读取了相同的数据，如果此时任何一个处理器修改了当前数据，并回写到内存，将会无效化其他的处理器。为保证缓存一致性，snoopy cache会监控所有自己已有数据的回写信号，包括其他处理器甚至分布式系统。然而这种设计只有在所有处理器共享同一总线下才能使用。 相关的硬件设计无法找到... Ravishankar, Chinya; Goodman, James (February 28, 1983) A Simple Protocol: Valid/InValid(VI) Every Write updates main memoryEvery Write requires broadcast &amp; snoop MSI(Modified-Shared-Invalid) 顺序写内存。优势：使用回写的方式，每次Write不需要立即写到main memoryMSI协议问题：1. 当多个私有缓存中都含有同一个共享数据块时，采用写无效策略的MSI 协议会进行多次的写作废操作，增加流量负载;2.其次，总线作为互斥资源其带宽有限，即总线的扩展性差；3.MSI 协议是通过广播的方式发送一致性消息，存在于总线上的消息对于一些处理器核来说是不需要的，即总线的有效利用率受到影响. MESI(Modified-Exclusive-Shared-Invalid)MESI protocol MOESI, MESIFCache一致性：MOESI/MESIF协议 MESI一致性协议中，一个CPU核先在L1 Cache中写入一个数据，然后另一个CPU请求读写此数据。则共享数据需要先写回L2 Cache，然后再发送给另一个CPU，需要两次操作。并且，在另一个CPU中更改后，最终还需要再次写回L2 Cache。这段操作中，完全可以减少一次数据传输和L2 Cache的写入。为此，需要加入一个Owner状态。不对这两个协议进行具体分析。 Cache line 回写与替换 首先需要明确一点，由于cache的最小单位是cache line(cache block)，因此无论是读取回写还是替换都是以cache line 作为单位的。 多个CPU中如何保证数据一致？是由cache1回写到cache2在回写到cache3再到主存？还是cache1直接回写到主存？什么时候回写？ cache line 什么时候进行替换，按什么替换. cache line 回写Cache Write Policies and Performance Cache hit1.1 直写策略 (VI, MSI)1.2 回写 (MESI, cache line 换出回写到内存) Cache miss2.1 write allocate policy(先调入chache)2.2 no write allocate policy(直接写到main memory) 直写策略，设计简单，但需要频繁的占用总线。回写，设计困难，极大减少总线占用。 cache line 替换Cache 替换策略 最不经常使用（LFU）算法LFU（Least Frequently Used，最不经常使用）算法将一段时间内被访问次数最少的那个块替换出去。每块设置一个计数器，从0开始计数，每访问一次，被访块的计数器就增1。当需要替换时，将计数值最小的块换出，同时将所有块的计数器都清零。 近期最少使用（LRU）算法LRU（Least Recently Used，近期最少使用）算法是把CPU近期最少使用的块替换出去。这种替换方法需要随时记录Cache中各块的使用情况，以便确定哪个块是近期最少使用的块。每块也设置一个计数器，Cache每命中一次，命中块计数器清零，其他各块计数器增1。当需要替换时，将计数值最大的块换出。需要复杂算法维护，Cache的命中率高。 随机替换效率高，不需要复杂算法，Cache命中率低。 memory consistencyMemory Barriers: a Hardware View for Software Hackers Introducing Caches: The MESI Protocol Memory Barriers（内存屏障）: a Hardware View for Software Hackers 阅读笔记 一致性杂谈 Memory Consistency的概念，与Cache Coherence不同的是，Memory Consistency关注的是多个变量，而非单个变量；Memory Model是多处理器和编译器优化导致存储器操作被多个处理器观察到的顺序不一致的问题，而Cache Coherence对程序员来说是透明的。 内存一致性模型的概念内存一致性模型(Memory Consistency Models) Shared MemoryConsistency Models:A Tutorial 多处理器编程：从缓存一致性到内存模型 Consistency model 严格一致性内存模型 / 内存严格一致性模型 (Strict Consistency)在严格的内存一致性模型下，任何对内存的读取，都会返回最近一次对该内存的修改，在没有缓存的情况下，所有操作通过总线直接与内存相连，这种形式的内存模型便是严格一致性内存模型 该模型只存在于理论中，实际中不使用。 顺序一致性 / 内存顺序一致性模型 / 顺序一致性内存模型（Sequential Consistency）对于Memory Order来说，最重要的是Sequential Consistency。它的意思是，每个线程按照程序次序执行，多个线程的执行结果，和将所有操作顺序执行的结果一样。换句话说，将所有线程的操作按照某一个顺序依次执行，结果和原来一样，但这个顺序未必是时间顺序。 缓存一致性（Cache Coherence）许多研究者几乎将缓存一致性(Cache Coherence)看作是顺序一致性的同义词；但是它们不是的，这也许让人感到惊讶。顺序一致性要求一个从全局（也就是所以内存）一致性的角度看待内存操作，缓存一致性仅仅要求一个局部的（也就是单个cache）一致性。这里有一个例子，它给出的场景符合缓存一致性，但是不符合顺序一致性： 缓存一致性的优化上面介绍了Cache, Cache line, Cache coherence, 这些都由硬件工程师为我们实现了，如果按上面模型的话，我们在开发过程中最需要关注的其实只有Cache line频繁miss这一问题。 0.2 spinlock，0.3 行遍历比列遍历高效，0.4 多线程执行时的低效这三个问题都可以找到对应的答案以及解决方案。内存屏障，至此只解决两个问题1.编译器优化，2.CPU优化 #define barrier() __asm__ __volatile__(&quot;&quot; ::: &quot;memory&quot;)以上内存屏障就是为了解决1.编译器优化，2.CPU优化问题 但实际硬件工程师在设计CPU的时候不仅仅局限于以上的设计，以上模型有一个问题同步执行。CPU将数据写入到cache line同时需要发送Invalidate给其他CPU，并且等待其他CPU置位返回acknowledgement消息后，本次写才完成。 store bufferstore buffer引入 在这种模型中，CPU0将数据写入store buffer后立即发送一个Invalidate给CPU1,等CPU1无效化cache line返回ack后即完成操作。 1234CPU0 a = 1b = a+1assert b==2 cpu0 执行 a = 1 cpu0 发现 a 不在 cache line, 因此发送Read Invalidate , 同时将a=1写入Store Buffer cpu1 接收 Read Invalidate, 并响应 cpu0 执行 b = a+1 cpu0 收到 cpu1返回数据a=0，放入cache line cpu0 load a from cache line，为0 cpu0 执行 b = 0+1 assert fails store buffer 改进 之前主要问题是cpu0只从cache line 中获取数据，未感知store buffer，因此添加一种从Store Buffer获取数据的机制就能解决上述问题。 sotre buffer 在多cpu中的问题123456789101112void cpu0(void) &#123; a = 1; b = 1;&#125;void cpu1(void) &#123; while (b == 0) contine; assert(a == 1);&#125;// cpu0 包含 b的cache line// cpu1 包含 a的cache line 没有编译器优化和cpu优化，即均按顺序执行 cpu0 执行 a = 1, a不在cache line, 存入store buffer cpu0 执行 b = 1, b在cache line, 直接替换 cpu1 执行 while (b == 0), b不在 cache line 发送 Read Invalidate cpu0 接收到 cpu1发送的Read b Invalidate，将b=1发送给cpu1 cpu1 接收到 b=1 存入cache line cpu1 结束while死循环 cpu1 执行a==1 fail cpu0 将store buffer内的a=1写入cache line,并发送Read Invalidate给cpu1, 此时已晚啦。 写屏障 smp_wmb12345678910111213void cpu0(void) &#123; a = 1; smp_wmb() b = 1;&#125;void cpu1(void) &#123; while (b == 0) contine; assert(a == 1);&#125;// cpu0 包含 b的cache line// cpu1 包含 a的cache line smp_mb()将导致cpu flush its store buffer cpu0 执行 a = 1, 在缓存中，置于store buffer cpu0 执行 smp_mp(), 将stpre buffer数据刷到 cache line，同时发送给cpu1 Read InValidate cpu1 接收到Read InValidate并返回ack cpu0 执行 b = 1, b在cache line, 直接替换 cpu1 执行b==1，读取cpu0的b存入cache line cpu1 结束while死循环 cpu1 执行a==1, 此时a无效，读取cpu0的a缓存 现在还有一个问题，无效状态确认必须等到其他cpu都完成后才能返回，16个CPU的话相当于每次都需要等到其他15个CPU将相应位置位无效后才返回继续执行。有没有方法减少这样的同步？？加入Invalidate Queue Invalidate QueueInvalidate Queue引入 引入无效队列，可以把收到的无效通知置于此，立即返回。 Invalidate Queue 在多cpu中的问题12345678910111213void cpu0(void) &#123; a = 1; smp_wmb() b = 1;&#125;void cpu1(void) &#123; while (b == 0) contine; assert(a == 1);&#125;// cpu0 包含 b的cache line// cpu1 包含 a的cache line 读屏障 smp_rmb1234567891011121314void cpu0(void) &#123; a = 1; smp_wmb(); b = 1;&#125;void cpu1(void) &#123; while (b == 0) contine; smp_rmb(); assert(a == 1);&#125;// cpu0 包含 b的cache line// cpu1 包含 a的cache line lock与屏障相关 为什么我们需要内存屏障？ 一文解决内存屏障 至此，从cache-&gt;cache line-&gt;cache coherency-&gt;memory consistency-&gt;memory barrier lock到底起到什么作用，在cache line中有数据的时候，并非锁总线, 而是通过添加内存屏障保证内存一致性，有些解释认为lock起到锁定缓存。 在x86架构中，lock前缀作为一个特殊的信号，执行了如下几个过程: 对总线和cache line上锁 强制lock信号之前的指令，都在之前被执行，并invalidate queue。 执行lock后的指令。 释放对总线和cache line上的锁 强制所有lock信号后的指令，都在之后执行，并同步store buffer。 因此lock信号虽然不是内存屏障，但具有mfence(全屏障)的语义，与内存屏障相比，lock信号要额外对总线和cache line上锁，成本更高 Java voliate相关 voliate 汇编代码1234567891011121314pac kage design;public class VolatileTest &#123; private static volatile int a = 10; public static void main(String[] args) &#123; a++; // 0x00000001176dff3b: mov 0x68(%r10),%r11d // 0x00000001176dff3f: inc %r11d // 0x00000001176dff42: mov %r11d,0x68(%r10) // 0x00000001176dff46: lock addl $0x0,(%rsp) System.out.println("Hello world!"); &#125;&#125; 编译javac design/VolatileTest 字节码反汇编javap -c -l design.VolatileTest 机器码反编译java -XX:+UnlockDiagnosticVMOptions -XX:+PrintAssembly -Xcomp design.VolatileTest &gt; test.asm voliate 原理 如果不保证，仍以x86架构为例，JVM对volatile变量的处理如下： 在写volatile变量v之后，插入一个sfence。这样，sfence之前的所有store（包括写v）不会被重排序到sfence之后，sfence之后的所有store不会被重排序到sfence之前，禁用跨sfence的store重排序；且sfence之前修改的值都会被写回缓存，并标记其他CPU中的缓存失效。 在读volatile变量v之前，插入一个lfence。这样，lfence之后的load（包括读v）不会被重排序到lfence之前，lfence之前的load不会被重排序到lfence之后，禁用跨lfence的load重排序；且lfence之后，会首先刷新无效缓存，从而得到最新的修改值，与sfence配合保证内存可见性。 内存屏障解决了多个问题：1.编译器优化，2.cpu优化，3.缓存相关优化，以此保证内存一致性。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Memory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode 516]]></title>
    <url>%2FLeetcode%2Fleetcode%2Fleetcode-516%2F</url>
    <content type="text"><![CDATA[本页主要更新与Palindromic相关的动态规划问题 Leetcode 516 Longest Palindromic Subsequence题目描述给定字符串s, 寻找该字符串中最长回文的子序列。假设最长长度为1000. 例: s = “bbbab” res = 4 解题思路DP$首先介绍一种Time: O(n^2), Space: O(n^2) \dp[i][j] 表示 s[i..j]中最长回文子串长度。 \dp[i][j] = \begin{cases}1 \qquad &amp; i = j \dp[i+1][j-1]+2 \qquad &amp; s[i]==s[j]\max(dp[i+1][j], dp[i][j-1]) \qquad &amp; s[i]!=s[j]\\end{cases} \$至此整个算法核心部分已经解决。 求解结果DP12345678910111213141516171819202122232425public int longestPalindromeSubseq(String s) &#123; if (s == null || s.length() == 0) return 0; int n = s.length(); // 长度 int[][] dp = new int[n][n]; // 错误的思路: // for (int i = 0; i &lt; n; i++) &#123; // dp[i][i] = 1; // for (int j = i-1; j &gt;= 0; j--) &#123; // &#125; // &#125; // 若使用以上形式遍历, 由于 dp[i][j] = dp[i+1][j-1] // 当i=n-1，或j=0时，都会越界。 for (int i = n-1; i &gt;= 0; i--) &#123; dp[i][i] = 1; for (int j = i+1; j &lt; n; j++) &#123; if (s.charAt(i) == s.charAt(j)) &#123; dp[i][j] = dp[i+1][j-1] + 2; &#125; else &#123; dp[i][j] = Math.max(dp[i+1][j], dp[i][j-1]); &#125; &#125; &#125; return dp[0][n-1];&#125; 拿s = “bbbab” 举例： \begin{matrix} 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 1 \\ \end{matrix} => \begin{matrix} 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 0 & 1 \\ \end{matrix} => \begin{matrix} 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 1 & 1 & 3 \\ 0 & 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 0 & 1 \\ \end{matrix} => \begin{matrix} 0 & 0 & 0 & 0 & 0 \\ 0 & 1 & 2 & 2 & 3 \\ 0 & 0 & 1 & 1 & 3 \\ 0 & 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 0 & 1 \\ \end{matrix} => \begin{matrix} 1 & 2 & 3 & 3 & 4 \\ 0 & 1 & 2 & 2 & 3 \\ 0 & 0 & 1 & 1 & 3 \\ 0 & 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 0 & 1 \\ \end{matrix} 由以上可知，当前状态只与前一状态相关，因此只需要int[] pre, int[] cur用于空间优化，从$O(n^2)$优化到$O(n)$ DP优化1234567891011121314151617181920public int longestPalindromeSubseq(String s) &#123; if (s == null || s.length() == 0) return 0; int n = s.length(); // 长度 int[] pre = new int[n]; int[] cur = new int[n]; for (int i = n-1; i &gt;= 0; i--) &#123; cur[i] = 1; for (int j = i+1; j &lt; n; j++) &#123; if (s.charAt(i) == s.charAt(j)) &#123; cur[j] = pre[j-1] + 2; &#125; else &#123; cur[j] = Math.max(pre[j], cur[j-1]); &#125; &#125; pre = cur; cur = new int[n]; &#125; return pre[n-1];&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Dynamic Programming</tag>
        <tag>Palindromic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分治(Divid And Conquer)]]></title>
    <url>%2FAlgorithm%2Falgorithm%2Fdivid-and-conquer%2F</url>
    <content type="text"><![CDATA[分治(Divide and Conquer)Divide and Conquer Algorithm | Introduction 一个典型的分治分为三步： Divide: 将原问题分解为相同类型的子问题 Conquer: 递归地解决这些子问题 Combine: 组合子问题的解作为原问题的解 最常见的分治算法包括： Binary Search QuickSort MergeSort Closest Pair of Points 1. Binary Search1234567891011121314151617class BinarySearch &#123; public int search(int[] nums, int target) &#123; int left = 0; int right = nums.length - 1; while (left &lt;= right) &#123; int mid = left + ((right - left) &gt;&gt; 1); if (nums[mid] == target) return mid; else if (nums[mid] &gt; target) &#123; right = mid - 1; &#125; else &#123; left = mid + 1; &#125; &#125; return -1; &#125; &#125; 我们的循环条件中包含了 left == right的情况，则我们必须在每次循环中改变 left 和 right的指向，以防止进入死循环 2. QuickSort123456789101112131415161718192021222324252627282930313233343536void quickSort(int arr[], int low, int high) &#123; if (low &lt; high) &#123; int pi = partition(arr, low, high); quickSort(arr, low, pi-1); quickSort(arr, pi+1, high); &#125; &#125; // 顺序遍历法 int partition(int arr[], int low, int high) &#123; int pivot = arr[high]; int i = low - 1; for (int j = low; j &lt; high; j++) &#123; if (arr[j] &lt;= pivot) &#123; i++; swap(arr, i, j); &#125; &#125; swap(arr, i+1, highg); return i+1; &#125; // 填坑法 int partition(int arr[], int low, int high) &#123; int pivot = arr[low]; while (low &lt; high) &#123; while (low &lt; high &amp;&amp; arr[high] &gt;= pivot) high--; arr[low] = arr[high]; while (low &lt; high &amp;&amp; arr[low] &lt;= pivot) low++; arr[high] = arr[low]; &#125; arr[low] = pivot; return low; &#125; 3. MergeSort123456789101112131415161718192021222324252627void mergeSort(int arr[], int low, int high) &#123; if (low &lt; high) &#123; int mid = low + (high - low) / 2; mergeSort(arr, low, mid); mergeSort(arr, mid+1, high); merge(arr, low, mid, high); &#125; &#125; void merge(int arr[], int low, int mid, int high) &#123; int len1 = mid - low + 1; int len2 = high - mid; int[] arr1 = copy2Temp(arr, low, len1); int[] arr2 = copy2Temp(arr, mid+1, len2); // merge the temp arrays int i = 0, j = 0; while (i &lt; len1 &amp;&amp; j &lt; len2) &#123; if (arr1[i] &lt;= arr2[j]) &#123; arr[low++] = arr1[i++]; &#125; else &#123; arr[low++] = arr2[j++]; &#125; &#125; while (i &lt; len1) arr[low++] = arr1[i++]; while (j &lt; len2) arr[low++] = arr2[j++]; &#125; 4. Closest Pair of Points123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657private double dist(Point point1, Point point2) &#123; int xdist = point1.x - point2.x; int ydist = point1.y - point2.y; return Math.sqrt(xdist * xdist + ydist * ydist); &#125; private double bruteForce(Point[] points, int low, int high) &#123; double min = Double.MAX_VALUE; for (int i = low; i &lt; high; i++) &#123; for (int j = i + 1; j &lt; high; j++) &#123; min = Math.min(min, dist(points[i], points[j])); &#125; &#125; return min; &#125; private double stripClosest(Point[] strip, int len, double d) &#123; double min = d; for (int i = 0; i &lt; len; i++) for (int j = i + 1; j &lt; len &amp;&amp; Math.abs(strip[j].y - strip[i].y) &lt; d; j++) min = Math.min(min, dist(strip[i], strip[j])); return min; &#125; public double closest(Point[] points) &#123; // 排序根据x轴排序 Arrays.sort(points, new Comparator&lt;Point&gt;() &#123; @Override public int compare(Point o1, Point o2) &#123; return o1.x - o2.x; &#125; &#125;); return closest(points, 0, points.length); &#125; private double closest(Point[] points, int low, int high) &#123; if (high - low &lt;= 3) &#123; return bruteForce(points, low, high); &#125; // 按x轴一分为二 int mid = low + (high - low) / 2; double d1 = closest(points, low, mid); double d2 = closest(points, mid + 1, high); double d = Math.min(d1, d2); // 中间地带谁最小 Point[] strip = new Point[high - low]; int j = 0; for (int i = low; i &lt; high; i++) if (Math.abs(points[i].x - points[mid].x) &lt; d) strip[j++] = points[i]; return Math.min(d, stripClosest(strip, j, d)); &#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Divid and conquer</tag>
      </tags>
  </entry>
</search>
